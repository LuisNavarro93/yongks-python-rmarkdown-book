[
["index.html", "Python Bookdown Prerequisites", " Python Bookdown Yong Keh Soon 2020-03-14 Prerequisites "],
["fundamentals.html", "1 Fundamentals 1.1 Library Management 1.2 Everything Is Object 1.3 Assignment", " 1 Fundamentals 1.1 Library Management 1.1.1 Built-In Libraries import string import datetime as dt 1.1.2 Common External Libraries import numpy as np import pandas as pd import datetime as dt import matplotlib import matplotlib.pyplot as plt from plydata import define, query, select, group_by, summarize, arrange, head, rename import plotnine from plotnine import * 1.1.2.1 numpy large multi-dimensional array and matrices High level mathematical funcitons to operate on them Efficient array computation, modeled after matlab Support vectorized array math functions (built on C, hence faster than python for loop and list) 1.1.2.2 scipy Collection of mathematical algorithms and convenience functions built on the numpy extension Built uponi numpy 1.1.2.3 Pandas Data manipulation and analysis Offer data structures and operations for manipulating numerical tables and time series Good for analyzing tabular data Use for exploratory data analysis, data pre-processing, statistics and visualization Built upon numpy 1.1.2.4 scikit-learn Machine learning functions Built on top of scipy 1.1.2.5 matplotlib Data Visualization 1.1.3 Package Management 1.1.4 Conda 1.1.4.1 Conda Environment system(&quot;conda info&quot;) 1.1.4.2 Package Version system(&quot;conda list&quot;) 1.1.4.3 Package Installation Conda is recommended distribution. To install from official conda channel: conda install &lt;package_name&gt; # always install latest conda install &lt;package_name=version_number&gt; ## Example: Install From conda official channel conda install numpy conda install scipy conda install pandas conda install matpotlib conda install scikit-learn conda install seaborn conda install pip To install from conda-forge community channel: conda install -c conda-forge &lt;package_name&gt; conda install -c conda-forge &lt;package_name=version_number&gt; ## Example: Install From conda community: conda install -c conda-forge plotnine 1.1.5 PIP PIP is python open repository (not part of conda). Use pip if the package is not available in conda. 1.1.5.1 Package Version system(&quot;pip list&quot;) 1.1.5.2 Package Installation pip install &lt;package_name&gt; ## Example: pip install plydata 1.2 Everything Is Object Every varibales in python are objects Every variable assginment is reference based, that is, each object value is the reference to memory block of data In the below exmaple, a, b and c refer to the same memory location: - Notice when an object assigned to another object, they refer to the same memory location - When two variable refers to the same value, they refer to the same memory location a = 123 b = 123 c = a print (&#39;Data of a =&#39;, a, &#39;\\nData of b =&#39;,b, &#39;\\nData of c =&#39;,c, &#39;\\nID of a = &#39;, id(a), &#39;\\nID of b = &#39;, id(b), &#39;\\nID of c = &#39;, id(c) ) ## Data of a = 123 ## Data of b = 123 ## Data of c = 123 ## ID of a = 140730136505696 ## ID of b = 140730136505696 ## ID of c = 140730136505696 Changing data value (using assignment) changes the reference a = 123 b = a a = 456 # reassignemnt changed a memory reference # b memory reference not changed print (&#39;Data of a =&#39;,a, &#39;\\nData of b =&#39;,b, &#39;\\nID of a = &#39;, id(a), &#39;\\nID of b = &#39;, id(b) ) ## Data of a = 456 ## Data of b = 123 ## ID of a = 578639504 ## ID of b = 140730136505696 1.3 Assignment 1.3.1 Multiple Assignment Assign multiple variable at the same time with same value. Note that all object created using this method refer to the same memory location. x = y = &#39;same mem loc&#39; print (&#39;x = &#39;, x, &#39;\\ny = &#39;, y, &#39;\\nid(x) = &#39;, id(x), &#39;\\nid(y) = &#39;, id(y) ) ## x = same mem loc ## y = same mem loc ## id(x) = 578790640 ## id(y) = 578790640 1.3.2 Augmented Assignment x = 1 y = x + 1 y += 1 print (&#39;y = &#39;, y) ## y = 3 1.3.3 Unpacking Assingment Assign multiple value to multiple variabels at the same time. x,y = 1,3 print (x,y) ## 1 3 "],
["built-in-data-types.html", "2 Built-in Data Types 2.1 Numbers 2.2 String 2.3 Boolean 2.4 None", " 2 Built-in Data Types 2.1 Numbers Two types of built-in number type, integer and float. 2.1.1 Integer n = 123 type (n) ## &lt;class &#39;int&#39;&gt; 2.1.2 Float f = 123.4 type (f) ## &lt;class &#39;float&#39;&gt; 2.1.3 Number Operators In general, when the operation potentially return float, the result is float type. Otherwise it return integer. Division always return float print(4/2) # return float ## 2.0 type(4/2) ## &lt;class &#39;float&#39;&gt; Integer Division by integer return inter. Integer division by float return float. print (8//3,&#39;\\n&#39;, # return int 8//3.2) # return float ## 2 ## 2.0 Remainder by integer return integer. Remainder by float return float print (8%3, &#39;\\n&#39;, # return int 8%3.2) # return float ## 2 ## 1.5999999999999996 Power return int or float print (2**3) # return int ## 8 print (2.1**3) # return float ## 9.261000000000001 print (2**3.1) # return float ## 8.574187700290345 2.2 String String is an object class ‘str’. It is an ordered collection of letters, an array of object type str import string s = &#39;abcde&#39; print( &#39;\\nvar type = &#39;, type(s), &#39;\\nelems = &#39;,s[0], s[1], s[2], &#39;\\nlen = &#39;, len(s), &#39;\\nelem type = &#39;,type(s[1])) ## ## var type = &lt;class &#39;str&#39;&gt; ## elems = a b c ## len = 5 ## elem type = &lt;class &#39;str&#39;&gt; 2.2.1 Constructor 2.2.1.1 Classical Method class str(object='') my_string = str() ## empty string class str(object=b'', encoding='utf-8', errors='strict') my_string = str(&#39;abc&#39;) 2.2.1.2 Shortcut Method my_string = &#39;abc&#39; 2.2.1.3 Multiline Method my_string = &#39;&#39;&#39; This is me. Yong Keh Soon &#39;&#39;&#39; print(my_string) ## ## This is me. ## Yong Keh Soon Note that the variable contain \\n front and end of the string. my_string ## &#39;\\nThis is me.\\nYong Keh Soon\\n&#39; 2.2.1.4 Immutability String is immuatable. Changing its content will result in error s = &#39;abcde&#39; print (&#39;s : &#39;, id(s)) #s[1] = &#39;z&#39; # immutable, result in error ## s : 578835320 Changing the variable completley change the reference (for new object) s = &#39;efgh&#39; print (&#39;s : &#39;, id(s)) ## s : 578761760 2.2.2 Class Constants 2.2.2.1 Letters print( &#39;letters = &#39;, string.ascii_letters, &#39;\\nlowercase = &#39;,string.ascii_lowercase, &#39;\\nuppercase = &#39;,string.ascii_uppercase ) ## letters = abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ## lowercase = abcdefghijklmnopqrstuvwxyz ## uppercase = ABCDEFGHIJKLMNOPQRSTUVWXYZ 2.2.2.2 Digits string.digits ## &#39;0123456789&#39; 2.2.2.3 White Spaces string.whitespace ## &#39; \\t\\n\\r\\x0b\\x0c&#39; 2.2.3 Instance Methods 2.2.3.1 Substitution : format() By Positional print( &#39;{} + {} = {}&#39;.format(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;), # auto sequence &#39;\\n{0} + {1} = {2}&#39;.format(&#39;aa&#39;, &#39;bb&#39;, &#39;cc&#39;)) # manual sequence ## a + b = c ## aa + bb = cc By Name &#39;Coordinates: {latitude}, {longitude}&#39;.format(latitude=&#39;37.24N&#39;, longitude=&#39;-115.81W&#39;) ## constant ## &#39;Coordinates: 37.24N, -115.81W&#39; By Dictionary Name coord = {&#39;latitude&#39;: &#39;37.24N&#39;, &#39;longitude&#39;: &#39;-115.81W&#39;} ## dictionary key/value &#39;Coordinates: {latitude}, {longitude}&#39;.format(**coord) ## &#39;Coordinates: 37.24N, -115.81W&#39; Formatting Number Float &#39;{:+f}; {:+f}&#39;.format(3.14, -3.14) # show it always ## &#39;+3.140000; -3.140000&#39; &#39;{: f}; {: f}&#39;.format(3.14, -3.14) # show a space for positive numbers ## &#39; 3.140000; -3.140000&#39; &#39;Correct answers: {:.2f}&#39;.format(55676.345345) ## &#39;Correct answers: 55676.35&#39; Integer, Percentage &#39;{0:,} {0:.2%} {0:,.2%}&#39;.format(1234567890.4455) ## &#39;1,234,567,890.4455 123456789044.55% 123,456,789,044.55%&#39; Alignment &#39;{0:&lt;20} {0:&lt;&lt;20}&#39;.format(&#39;left aligned&#39;) ## &#39;left aligned left aligned&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&#39; &#39;{0:&gt;20} {0:$&gt;20}&#39;.format(&#39;right aligned&#39;) ## &#39; right aligned $$$$$$$right aligned&#39; &#39;{:^30}&#39;.format(&#39;centered&#39;) # use &#39;*&#39; as a fill char ## &#39; centered &#39; 2.2.3.2 Substitution : f-string my_name = &#39;Yong Keh Soon&#39; salary = 11123.346 f&#39;Hello, {my_name}, your salary is {salary:,.2f} !&#39; ## &#39;Hello, Yong Keh Soon, your salary is 11,123.35 !&#39; 2.2.3.3 Conversion: upper() lower() &#39;myEXEel.xls&#39;.upper() ## &#39;MYEXEEL.XLS&#39; &#39;myEXEel.xls&#39;.lower() ## &#39;myexeel.xls&#39; 2.2.3.4 find() pattern position string.find() return position of first occurance. -1 if not found s=&#39;I love karaoke, I know you love it oo&#39; print (s.find(&#39;lov&#39;)) ## 2 print (s.find(&#39;kemuning&#39;)) ## -1 2.2.3.5 strip() off blank spaces filename = &#39; myexce l. xls &#39; filename.strip() ## &#39;myexce l. xls&#39; 2.2.3.6 List Related: split() Splitting delimeter is specified. Observe the empty spaces were conserved in result array animals = &#39;a1,a2 ,a3, a4&#39; animals.split(&#39;,&#39;) ## [&#39;a1&#39;, &#39;a2 &#39;, &#39;a3&#39;, &#39; a4&#39;] 2.2.3.7 List Related: join() &#39;-&#39;.join([&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;]) ## &#39;1-2-3-4&#39; 2.2.3.8 Replacement: .replace() string = &quot;geeks for geeks geeks geeks geeks&quot; # Prints the string by replacing geeks by Geeks print(string.replace(&quot;geeks&quot;, &quot;Geeks&quot;)) # Prints the string by replacing only 3 occurrence of Geeks ## Geeks for Geeks Geeks Geeks Geeks print(string.replace(&quot;geeks&quot;, &quot;GeeksforGeeks&quot;, 3)) ## GeeksforGeeks for GeeksforGeeks GeeksforGeeks geeks geeks 2.2.4 Operator 2.2.4.1 % Old Style Substitution https://docs.python.org/3/library/stdtypes.html#old-string-formatting my_name = &#39;Yong Keh Soon&#39; salary = 11123.346 &#39;Hello, %s, your salary is %.2f !&#39; %(my_name, salary) ## &#39;Hello, Yong Keh Soon, your salary is 11123.35 !&#39; 2.2.4.2 + Concatenation &#39;this is &#39; + &#39;awesome&#39; ## &#39;this is awesome&#39; 2.2.4.3 in matching For single string, partial match print( &#39;abc&#39; in &#39;123abcdefg&#39; ) ## True For list of strings, exact match (even though only one element in list). For partial match, workaround is to convert list to single string print( &#39;abc&#39; in [&#39;abcdefg&#39;], # false &#39;abc&#39; in [&#39;abcdefg&#39;,&#39;123&#39;], # fakse &#39;abc&#39; in [&#39;123&#39;,&#39;abc&#39;,&#39;def&#39;], # true &#39;abc&#39; in str([&#39;123&#39;,&#39;abcdefg&#39;])) # true ## False False True True 2.2.4.4 Comparitor Comparitor compares the memory address. a=&#39;abc&#39; b=&#39;abc&#39; print(&#39;id(a) = &#39;, id(a), &#39;\\nid(b) = &#39;, id(b), &#39;\\na == b &#39;, a==b) ## id(a) = 371652904 ## id(b) = 371652904 ## a == b True 2.2.5 Iterations string[start:end:step] # default start:0, end:last, step:1 If step is negative (reverse), end value must be lower than start value s = &#39;abcdefghijk&#39; print (s[0]) # first later ## a print (s[:3]) # first 3 letters ## abc print (s[2:8 :2]) # stepping ## ceg print (s[-1]) # last letter ## k print (s[-3:]) # last three letters ## ijk print (s[: :-1]) # reverse everything ## kjihgfedcba print (s[8:2 :-1]) ## ihgfed print (s[8:2]) # return NOTHING 2.3 Boolean b = False if (b): print (&#39;It is true&#39;) else: print (&#39;It is fake&#39;) ## It is fake 2.3.1 What is Considered False ? Everything below are false, anything else are true print ( bool(0), # zero bool(None), # none bool(&#39;&#39;), # empty string bool([]), # empty list bool(()), # empty tupple bool(False), # False bool(2-2)) # expression that return any value above ## False False False False False False False 2.3.2 and operator BEWARE ! and can return different data types If evaluated result is True, the last True Value is returned (because python need to evaluate up to the last value) If evaluated result is False, the first False Value will be returned (because python return it immediately when detecting False value) print (123 and 2 and 1, 123 and [] and 2) ## 1 [] 2.3.3 not operator not (True) ## False not (True or False) ## False not (False) ## True not (True and False) ## True ~(False) ## -1 2.3.4 or operator or can return different data type If evaluated result is True, first True Value will be returned (right hand side value need not be evaluated) If evaluated result is False, last Fasle Value will be returned (need to evalute all items before concluding False) print (1 or 2) ## 1 print (0 or 1 or 1) ## 1 print (0 or () or []) ## [] 2.4 None 2.4.1 None is an Object None is a Python object NonType Any operation to None object will result in error For array data with None elements, verification is required to check through iteration to determine if the item is not None. It is very computaionaly heavy type(None) ## &lt;class &#39;NoneType&#39;&gt; t1 = np.array([1, 2, 3, 4, 5]) t2= np.array([1, 2, 3, None, 4, 5]) print( t1.dtype , &#39;\\n\\n&#39;, # it&#39;s an object t2.dtype) ## int32 ## ## object 2.4.2 Comparing None Not Prefered Method null_variable = None print( null_variable == None ) ## True Prefered print( null_variable is None ) ## True print( null_variable is not None ) ## False 2.4.3 Operation on None Any operator (except is) on None results in error. None &amp; None ## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: unsupported operand type(s) for &amp;: &#39;NoneType&#39; and &#39;NoneType&#39; ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; "],
["built-in-data-structure.html", "3 Built-In Data Structure 3.1 Tuple 3.2 List 3.3 Dictionaries 3.4 Sets 3.5 range", " 3 Built-In Data Structure 3.1 Tuple Tuple is an immutable list. Any attempt to change/update tuple will return error. It can contain different types of object. Benefits of tuple against List are: - Faster than list - Protects your data against accidental change - Can be used as key in dictionaries, list can’t 3.1.1 Creating 3.1.1.1 Constructor mylist = [1,2,3] tuple(mylist) ## (1, 2, 3) 3.1.1.2 Assignment With or Without () This is a formal syntax for defining tuple, items inside ( ) notation. Assignment although works without (), it is not recommended. t1 = (1,2,3,&#39;o&#39;,&#39;apple&#39;) t2 = 1,2,3,&#39;o&#39;,&#39;apple&#39; print(type(t1), type(t2)) ## &lt;class &#39;tuple&#39;&gt; &lt;class &#39;tuple&#39;&gt; 3.1.2 Accessing print( t[1], t[1:3] ) ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;t&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; 3.1.3 Duplicating Use normal assignment = to duplicate. Reference of the memory address is copied. Data is actually not duplicated in memory. original = (1,2,3,4,5) copy_test = original print(original) ## (1, 2, 3, 4, 5) print(copy_test) ## (1, 2, 3, 4, 5) The copy and original has the same memory location. print(&#39;Original ID: &#39;, id(original)) ## Original ID: 578682640 print(&#39;Copy ID: &#39;, id(copy_test)) ## Copy ID: 578682640 3.2 List List is a collection of ordered items, where the items can be different data types You can pack list of items by placing them into [] List is mutable 3.2.1 Creating List 3.2.1.1 Empty List empty = [] # literal assignment method empty = list() # constructor method print (empty) ## [] 3.2.1.2 Literal Assignment Multiple data types is allowed in a list [123,&#39;abc&#39;,456, None] ## [123, &#39;abc&#39;, 456, None] Constructor Note that list(string) will split the string into letters list(&#39;hello&#39;) ## [&#39;h&#39;, &#39;e&#39;, &#39;l&#39;, &#39;l&#39;, &#39;o&#39;] 3.2.2 Accessing Items Access specific index number food = [&#39;bread&#39;, &#39;noodle&#39;, &#39;rice&#39;, &#39;biscuit&#39;,&#39;jelly&#39;,&#39;cake&#39;] print (food[2]) # 3rd item ## rice print (food[-1]) # last item ## cake Access range of indexes print (food[:4]) # first 3 items ## [&#39;bread&#39;, &#39;noodle&#39;, &#39;rice&#39;, &#39;biscuit&#39;] print (food[-3:]) # last 3 items ## [&#39;biscuit&#39;, &#39;jelly&#39;, &#39;cake&#39;] print (food[1:5]) # item 1 to 4 ## [&#39;noodle&#39;, &#39;rice&#39;, &#39;biscuit&#39;, &#39;jelly&#39;] print (food[5:2:-1]) # item 3 to 5, reverse order ## [&#39;cake&#39;, &#39;jelly&#39;, &#39;biscuit&#39;] print (food[::-1]) # reverse order ## [&#39;cake&#39;, &#39;jelly&#39;, &#39;biscuit&#39;, &#39;rice&#39;, &#39;noodle&#39;, &#39;bread&#39;] 3.2.3 Methods 3.2.3.1 Remove Item(s) Removal of non-existance item will result in error Search and remove first matching item food = list([&#39;bread&#39;, &#39;noodle&#39;, &#39;rice&#39;, &#39;biscuit&#39;,&#39;jelly&#39;,&#39;cake&#39;,&#39;noodle&#39;]) food.remove(&#39;noodle&#39;) print (food) ## [&#39;bread&#39;, &#39;rice&#39;, &#39;biscuit&#39;, &#39;jelly&#39;, &#39;cake&#39;, &#39;noodle&#39;] Remove last item food.pop() ## &#39;noodle&#39; print (food) ## [&#39;bread&#39;, &#39;rice&#39;, &#39;biscuit&#39;, &#39;jelly&#39;, &#39;cake&#39;] Remove item at specific position food.pop(1) # counter start from 0 ## &#39;rice&#39; print(food) ## [&#39;bread&#39;, &#39;biscuit&#39;, &#39;jelly&#39;, &#39;cake&#39;] food.remove(&#39;jelly&#39;) print(food) ## [&#39;bread&#39;, &#39;biscuit&#39;, &#39;cake&#39;] 3.2.3.2 Appending Item (s) Append One Item food.append(&#39;jelly&#39;) print (food) ## [&#39;bread&#39;, &#39;biscuit&#39;, &#39;cake&#39;, &#39;jelly&#39;] Append Multiple Items extend() will expand the list/tupple argument and append as multiple items food.extend([&#39;nand&#39;,&#39;puff&#39;]) print (food) ## [&#39;bread&#39;, &#39;biscuit&#39;, &#39;cake&#39;, &#39;jelly&#39;, &#39;nand&#39;, &#39;puff&#39;] 3.2.3.3 Other Methods Reversing the order of the items food.reverse() food ## [&#39;puff&#39;, &#39;nand&#39;, &#39;jelly&#39;, &#39;cake&#39;, &#39;biscuit&#39;, &#39;bread&#39;] Locating the Index Number of An Item food.index(&#39;biscuit&#39;) ## 4 Count occurance test = [&#39;a&#39;,&#39;a&#39;,&#39;a&#39;,&#39;b&#39;,&#39;c&#39;] test.count(&#39;a&#39;) ## 3 Sorting The Order of Items food.sort() print (food) ## [&#39;biscuit&#39;, &#39;bread&#39;, &#39;cake&#39;, &#39;jelly&#39;, &#39;nand&#39;, &#39;puff&#39;] 3.2.4 Operator 3.2.4.1 Concatenation Concatenating Lists Two lists can be concatenanted using ‘+’ operator. [&#39;dog&#39;,&#39;cat&#39;,&#39;horse&#39;] + [&#39;elephant&#39;,&#39;tiger&#39;] + [&#39;sheep&#39;] ## [&#39;dog&#39;, &#39;cat&#39;, &#39;horse&#39;, &#39;elephant&#39;, &#39;tiger&#39;, &#39;sheep&#39;] 3.2.5 List is Mutable The reference of list variable won’t change after adding/removing its item food = [&#39;cake&#39;,&#39;jelly&#39;,&#39;roti&#39;,&#39;noodle&#39;] print (&#39;food : &#39;,id(food)) ## food : 482437768 food += [&#39;salad&#39;,&#39;chicken&#39;] print (&#39;food : &#39;,id(food)) ## food : 482437768 A function is actually an object, which reference never change, hence mutable def spam (elem, some_list=[&#39;a&#39;,&#39;b&#39;]): some_list.append(elem) return some_list print (spam(1,[&#39;x&#39;])) ## [&#39;x&#39;, 1] print (spam(2)) ## second parameter is not passed ## [&#39;a&#39;, &#39;b&#39;, 2] print (spam(3)) ## notice the default was remembered ## [&#39;a&#39;, &#39;b&#39;, 2, 3] 3.2.6 Duplicate or Reference Use = : It just copy the refernce. IDs are similar original = [1,2,3,4,5] copy_test = original print(&#39;Original ID: &#39;, id(original)) ## Original ID: 578751432 print(&#39;Copy ID: &#39;, id(copy_test)) ## Copy ID: 578751432 original[0]=999 ## change original print(original) ## [999, 2, 3, 4, 5] print(copy_test) ## copy affected ## [999, 2, 3, 4, 5] Duplicate A List Object with copy(). Resulting IDs are different original = [1,2,3,4,5] copy_test = original.copy() print(original) ## [1, 2, 3, 4, 5] print(copy_test) ## [1, 2, 3, 4, 5] print(&#39;Original ID: &#39;, id(original)) ## Original ID: 579003464 print(&#39;Copy ID: &#39;, id(copy_test)) ## Copy ID: 578908872 original[0] = 999 ## change original print(original) ## [999, 2, 3, 4, 5] print(copy_test) ## copy not affected ## [1, 2, 3, 4, 5] Passing To Function As Reference def func(x): print (x) print(&#39;ID in Function: &#39;, id(x)) x.append(6) ## modify the refrence my_list = [1,2,3,4,5] print(&#39;ID outside Function: &#39;, id(my_list)) ## ID outside Function: 578752072 func(my_list) ## call the function, pass the reference ## [1, 2, 3, 4, 5] ## ID in Function: 578752072 print(my_list) ## content was altered ## [1, 2, 3, 4, 5, 6] 3.2.7 List Is Iterable 3.2.7.1 For Loop s = [&#39;abc&#39;,&#39;abcd&#39;,&#39;bcde&#39;,&#39;bcdee&#39;,&#39;cdefg&#39;] for x in s: if &#39;abc&#39; in x: print (x) ## abc ## abcd 3.2.7.2 List Comprehension This code below is a shorform method of for loop and if. old_list = [&#39;abc&#39;,&#39;abcd&#39;,&#39;bcde&#39;,&#39;bcdee&#39;,&#39;cdefg&#39;] [x for x in old_list if &#39;abc&#39; in x] ## [&#39;abc&#39;, &#39;abcd&#39;] Compare to traditional version of code below: new_list = [] old_list = [&#39;abc&#39;,&#39;abcd&#39;,&#39;bcde&#39;,&#39;bcdee&#39;,&#39;cdefg&#39;] for x in old_list: if &#39;abc&#39; in x: new_list.append(x) print( new_list ) ## [&#39;abc&#39;, &#39;abcd&#39;] 3.2.8 Conversion Convert mutable list to immutable tuple with tuple() original = [1,2,3] original_tuple = tuple(original) print( id(original), id(original_tuple)) ## 579056904 578864184 3.2.9 Built-In Functions Applicable To List Number of Elements len(food) ## 6 Max Value test = [1,2,3,5,5,3,2,1] m = max(test) test.index(m) ## only first occurance is found ## 3 3.3 Dictionaries Dictionary is a list of index-value items. 3.3.1 Creating dict 3.3.1.1 From Literals Simple Dictionary animal_counts = { &#39;cats&#39; : 2, &#39;dogs&#39; : 5, &#39;horses&#39;:4} print (animal_counts) ## {&#39;cats&#39;: 2, &#39;dogs&#39;: 5, &#39;horses&#39;: 4} print( type(animal_counts) ) ## &lt;class &#39;dict&#39;&gt; Dictionary with list animal_names = {&#39;cats&#39;: [&#39;Walter&#39;,&#39;Ra&#39;], &#39;dogs&#39;: [&#39;Jim&#39;,&#39;Roy&#39;,&#39;John&#39;,&#39;Lucky&#39;,&#39;Row&#39;], &#39;horses&#39;: [&#39;Sax&#39;,&#39;Jack&#39;,&#39;Ann&#39;,&#39;Jeep&#39;] } animal_names ## {&#39;cats&#39;: [&#39;Walter&#39;, &#39;Ra&#39;], &#39;dogs&#39;: [&#39;Jim&#39;, &#39;Roy&#39;, &#39;John&#39;, &#39;Lucky&#39;, &#39;Row&#39;], &#39;horses&#39;: [&#39;Sax&#39;, &#39;Jack&#39;, &#39;Ann&#39;, &#39;Jeep&#39;]} 3.3.1.2 From Variables cat_names = [&#39;Walter&#39;,&#39;Ra&#39;,&#39;Jim&#39;] dog_names = [&#39;Jim&#39;,&#39;Roy&#39;,&#39;John&#39;,&#39;Lucky&#39;,&#39;Row&#39;] horse_names= [&#39;Sax&#39;,&#39;Jack&#39;,&#39;Ann&#39;,&#39;Jeep&#39;] animal_names = {&#39;cats&#39;: cat_names, &#39;dogs&#39;: dog_names, &#39;horses&#39;: horse_names} animal_names ## {&#39;cats&#39;: [&#39;Walter&#39;, &#39;Ra&#39;, &#39;Jim&#39;], &#39;dogs&#39;: [&#39;Jim&#39;, &#39;Roy&#39;, &#39;John&#39;, &#39;Lucky&#39;, &#39;Row&#39;], &#39;horses&#39;: [&#39;Sax&#39;, &#39;Jack&#39;, &#39;Ann&#39;, &#39;Jeep&#39;]} 3.3.2 Accessing dict 3.3.2.1 Get All Keys print (animal_names.keys()) ## dict_keys([&#39;cats&#39;, &#39;dogs&#39;, &#39;horses&#39;]) print (sorted(animal_names.keys())) ## [&#39;cats&#39;, &#39;dogs&#39;, &#39;horses&#39;] 3.3.2.2 Get All Values print (animal_names.values()) ## dict_values([[&#39;Walter&#39;, &#39;Ra&#39;, &#39;Jim&#39;], [&#39;Jim&#39;, &#39;Roy&#39;, &#39;John&#39;, &#39;Lucky&#39;, &#39;Row&#39;], [&#39;Sax&#39;, &#39;Jack&#39;, &#39;Ann&#39;, &#39;Jeep&#39;]]) print (sorted(animal_names.values())) ## [[&#39;Jim&#39;, &#39;Roy&#39;, &#39;John&#39;, &#39;Lucky&#39;, &#39;Row&#39;], [&#39;Sax&#39;, &#39;Jack&#39;, &#39;Ann&#39;, &#39;Jeep&#39;], [&#39;Walter&#39;, &#39;Ra&#39;, &#39;Jim&#39;]] 3.3.2.3 Access value with Specific Key Use [ key ] notation. However, this will return Error if key does not exist animal_names[&#39;dogs&#39;] ## [&#39;Jim&#39;, &#39;Roy&#39;, &#39;John&#39;, &#39;Lucky&#39;, &#39;Row&#39;] Use get( key ) notation. will return None if key does not exist print (animal_counts.get(&#39;cow&#39;)) ## None 3.3.3 Dict Is Mutable 3.3.3.1 Update/Append Use [key] notation to update o append the content of element. animal_names[&#39;dogs&#39;] = [&#39;Ali&#39;,&#39;Abu&#39;,&#39;Bakar&#39;] animal_names ## {&#39;cats&#39;: [&#39;Walter&#39;, &#39;Ra&#39;, &#39;Jim&#39;], &#39;dogs&#39;: [&#39;Ali&#39;, &#39;Abu&#39;, &#39;Bakar&#39;], &#39;horses&#39;: [&#39;Sax&#39;, &#39;Jack&#39;, &#39;Ann&#39;, &#39;Jeep&#39;]} Use clear() to erase all elements animal_names.clear() 3.3.4 Iterating Elements Loop through .items() animal_dict = { &#39;cats&#39; : 2, &#39;dogs&#39; : 5, &#39;horses&#39;:4} for key,val in animal_dict.items(): print( key, val ) ## cats 2 ## dogs 5 ## horses 4 3.4 Sets Set is unordered collection of unique items. Set is mutable 3.4.1 Creation Set can be declared with {}, unlike list creation uses ‘[]’. myset = {&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;a&#39;,&#39;b&#39;,&#39;e&#39;,&#39;f&#39;,&#39;g&#39;} print (myset) # notice no repetition values ## {&#39;a&#39;, &#39;g&#39;, &#39;f&#39;, &#39;e&#39;, &#39;c&#39;, &#39;b&#39;, &#39;d&#39;} Set can be created from list, and then converted back to list mylist = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;a&#39;,&#39;b&#39;,&#39;e&#39;,&#39;f&#39;,&#39;g&#39;] myset = set(mylist) my_unique_list = list(myset) print ( &#39;Original List : &#39;, mylist, &#39;\\nConvert to set : &#39;, myset, &#39;\\nConvert back to list: &#39;, my_unique_list) # notice no repetition values ## Original List : [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;a&#39;, &#39;b&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;] ## Convert to set : {&#39;a&#39;, &#39;g&#39;, &#39;f&#39;, &#39;e&#39;, &#39;c&#39;, &#39;b&#39;, &#39;d&#39;} ## Convert back to list: [&#39;a&#39;, &#39;g&#39;, &#39;f&#39;, &#39;e&#39;, &#39;c&#39;, &#39;b&#39;, &#39;d&#39;] 3.4.2 Membership Test print (&#39;a&#39; in myset) # is member ? ## True print (&#39;f&#39; not in myset) # is not member ? ## False 3.4.3 Subset Test Subset Test : &lt;= Proper Subset Test : &lt; mysubset = {&#39;d&#39;,&#39;g&#39;} mysubset &lt;= myset ## True Proper Subset test that the master set contain at least one element which is not in the subset mysubset = {&#39;b&#39;,&#39;a&#39;,&#39;d&#39;,&#39;c&#39;,&#39;e&#39;,&#39;f&#39;,&#39;g&#39;} print (&#39;Is Subset : &#39;, mysubset &lt;= myset) ## Is Subset : True print (&#39;Is Proper Subet : &#39;, mysubset &lt; myset) ## Is Proper Subet : False 3.4.4 Union using | {&#39;a&#39;,&#39;b&#39;,&#39;c&#39;} | {&#39;a&#39;,&#39;e&#39;,&#39;f&#39;} ## {&#39;a&#39;, &#39;f&#39;, &#39;e&#39;, &#39;c&#39;, &#39;b&#39;} 3.4.5 Intersection using &amp; Any elments that exist in both left and right set {&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;} &amp; {&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;} ## {&#39;c&#39;, &#39;d&#39;} 3.4.6 Difference using - Remove right from left {&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;} - {&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;} ## {&#39;a&#39;, &#39;b&#39;} 3.5 range range(X) generates sequence of integer object range (lower_bound, upper_bound, step_size) # lower bound is optional, default = 0 # upper bound is not included in result # step is optional, default = 1 Use list() to convert in order to view actual sequence of data r = range(10) # default lower bound =0, step =1 print (type (r)) ## &lt;class &#39;range&#39;&gt; print (r) ## range(0, 10) print (list(r)) ## [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] More Examples print (list(range(2,8))) # step not specified, default 1 ## [2, 3, 4, 5, 6, 7] print (&#39;Odds Number : &#39; , list(range(1,10,2))) # generate odds number ## Odds Number : [1, 3, 5, 7, 9] "],
["control-and-loops.html", "4 Control and Loops 4.1 If Statement 4.2 For Loops 4.3 Generators", " 4 Control and Loops 4.1 If Statement 4.1.1 Multiline If.. Statements price = 102 if price &lt;100: print (&#39;buy&#39;) elif price &lt; 110: print (&#39;hold&#39;) elif price &lt; 120: print (&#39;think about it&#39;) else: print (&#39;sell&#39;) ## hold print(&#39;end of programming&#39;) ## end of programming 4.1.2 Single Line If .. Statement 4.1.2.1 if … In One Statement price = 70 if price&lt;80: print(&#39;buy&#39;) ## buy 4.1.2.2 Ternary Statemnt This statement return a value with simple condition price = 85 &#39;buy&#39; if (price&lt;80) else &#39;dont buy&#39; ## &#39;dont buy&#39; 4.2 For Loops 4.2.1 For .. Else Construct else is only executed when the for loop completed all cycles mylist = [1,2,3,4,5] for i in mylist: print (i) else: print(&#39;Hooray, the loop is completed successfully&#39;) ## 1 ## 2 ## 3 ## 4 ## 5 ## Hooray, the loop is completed successfully In below exmaple, for loop encountered break, hence the else section is not executed. for i in mylist: if i &lt; 4: print (i) else: print(&#39;Oops, I am breaking out half way in the loop&#39;) break else: print(&#39;Hooray, the loop is completed successfully&#39;) ## 1 ## 2 ## 3 ## Oops, I am breaking out half way in the loop 4.2.2 Loop thorugh ‘range’ for i in range (1,10,2): print (&#39;Odds Number : &#39;,i) ## Odds Number : 1 ## Odds Number : 3 ## Odds Number : 5 ## Odds Number : 7 ## Odds Number : 9 4.2.3 Loop through ‘list’ 4.2.3.1 Standard For Loop letters = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;] for e in letters: print (&#39;Letter : &#39;,e) ## Letter : a ## Letter : b ## Letter : c ## Letter : d 4.2.3.2 List Comprehension Iterate through existing list, and build new list based on condition new_list = [expression(i) for i in old_list] s = [&#39;abc&#39;,&#39;abcd&#39;,&#39;bcde&#39;,&#39;bcdee&#39;,&#39;cdefg&#39;] [x.upper() for x in s] ## [&#39;ABC&#39;, &#39;ABCD&#39;, &#39;BCDE&#39;, &#39;BCDEE&#39;, &#39;CDEFG&#39;] Extend list comprehension can be extended with if condition** new_list = [expression(i) for i in old_list if filter(i)] old_list = [&#39;abc&#39;,&#39;abcd&#39;,&#39;bcde&#39;,&#39;bcdee&#39;,&#39;cdefg&#39;] matching = [ x.upper() for x in old_list if &#39;bcd&#39; in x ] print( matching ) ## [&#39;ABCD&#39;, &#39;BCDE&#39;, &#39;BCDEE&#39;] 4.2.4 Loop Through ‘Dictionary’ Looping through dict will picup key d = {&quot;x&quot;: 1, &quot;y&quot;: 2} for key in d: print (key, d[key]) ## x 1 ## y 2 4.3 Generators Generator is lazy, produce items only if asked for, hence more memory efficient Generator is function with ‘yield’ instead of ‘return’ Generator contains one or more yields statement When called, it returns an object (iterator) but does not start execution immediately Methods like iter() and next() are implemented automatically. So we can iterate through the items using next() Once the function yields, the function is paused and the control is transferred to the caller Local variables and their states are remembered between successive calls Finally, when the function terminates, StopIteration is raised automatically on further calls 4.3.1 Basic Generator Function Below example give clear understanding of how generator works def my_gen(): n = 1 print(&#39;This is printed first&#39;) # Generator function contains yield statements yield n n += 1 print(&#39;This is printed second&#39;) yield n n += 1 print(&#39;This is printed at last&#39;) yield n a = my_gen() type(a) ## &lt;class &#39;generator&#39;&gt; next(a) ## This is printed first ## 1 next(a) ## This is printed second ## 2 4.3.2 Useful Generator Fuction Generator is only useful when it uses for-loop - for-loop within generator - for-loop to iterate through a generator def rev_str(my_str): length = len(my_str) for i in range(length - 1,-1,-1): yield my_str[i] for c in rev_str(&quot;hello&quot;): print(c) ## o ## l ## l ## e ## h 4.3.3 Generator Expression Use () to create an annonymous generator function my_list = [1, 3, 6, 10] a = (x**2 for x in my_list) next(a) ## 1 next(a) ## 9 sum(a) # sum the power of 6,10 ## 136 4.3.4 Compare to Iterator Class class PowTwo: def __init__(self, max = 0): self.max = max def __iter__(self): self.n = 0 return self def __next__(self): if self.n &gt; self.max: raise StopIteration result = 2 ** self.n self.n += 1 return result Obviously, Generator is more concise and cleaner def PowTwoGen(max = 0): n = 0 while n &lt; max: yield 2 ** n n += 1 "],
["library-and-functions.html", "5 Library and Functions 5.1 Package Source 5.2 Importing Library 5.3 Define Function", " 5 Library and Functions Library are group of functions 5.1 Package Source 5.1.1 Conda Package manager for any language Install binaries 5.1.2 PIP Package manager python only Compile from source Stands for Pip Installs Packages Python’s officially-sanctioned package manager, and is most commonly used to install packages published on the Python Package Index (PyPI) Both pip and PyPI are governed and supported by the Python Packaging Authority (PyPA). 5.2 Importing Library There are two methods to import library functions: Standalone Namespace - import &lt;libName&gt; # access function through: libName.functionName - import &lt;libName&gt; as &lt;shortName&gt; # access function through: shortName.functionName Global Namespace - from &lt;libName&gt; import * # all functions available at global namespace - from &lt;libName&gt; import &lt;functionName&gt; # access function through: functionName - from &lt;libName&gt; import &lt;functionName&gt; as &lt;shortFunctionName&gt; # access function through shortFunctionName 5.2.1 Import Entire Library 5.2.1.1 Import Into Standalone Namespace import math math.sqrt(9) ## 3.0 Use as for aliasing library name. This is useful if you have conflicting library name import math as m m.sqrt(9) ## 3.0 5.2.1.2 Import Into Global Name Space All functions in the library accessible through global namespace from &lt;libName&gt; import * 5.2.2 Import Specific Function from math import sqrt print (sqrt(9)) ## 3.0 Use as for aliasing function name from math import sqrt as sq print (sq(9)) ## 3.0 5.2.3 Machine Learning Packages alt text 5.3 Define Function 5.3.1 Function Arguments By default, arguments are assigned to function left to right def myfun(x,y): print (&#39;x:&#39;,x) print (&#39;y:&#39;,y) myfun(5,8) ## x: 5 ## y: 8 However, you can also specify the argument assigment during function call myfun (y=8,x=5) ## x: 5 ## y: 8 Function can have default argement value def myfun(x=1,y=1): # default argument value is 1 print (&#39;x:&#39;,x) print (&#39;y:&#39;,y) myfun(5) # pass only one argument ## x: 5 ## y: 1 5.3.2 List Within Function Consider a function is an object, its variable (some_list) is immutable and hence its reference won’t change, even data changes def spam (elem, some_list=[]): some_list.append(elem) return some_list print (spam(1)) ## [1] print (spam(2)) ## [1, 2] print (spam(3)) ## [1, 2, 3] 5.3.3 Return Statement def bigger(x,y): if (x&gt;y): return x else: return y print (bigger(5,8)) ## 8 5.3.4 No Return Statement if no return statement, python return None def dummy(): print (&#39;This is a dummy function, return no value&#39;) dummy() ## This is a dummy function, return no value 5.3.5 Return Multiple Value Multiple value is returned as tuple. Use multiple assignment to assign to multiple variable def minmax(x,y,z): return min(x,y,z), max(x,y,z) a,b = minmax(7,8,9) # multiple assignment c = minmax(7,8,9) # tuple print (a,b) ## 7 9 print (c) ## (7, 9) 5.3.6 Passing Function as Argument You can pass a function name as an argument to a function def myfun(x,y,f): f(x,y) myfun(&#39;hello&#39;,54,print) ## hello 54 5.3.7 Arguments args is a tuple 5.3.7.1 Example 1 Error example, too many parameters passed over to function 5.3.7.2 Example 2 First argument goes to x, remaining goes to args as tuple def myfun(x,*args): print (x) print (args) #tuple myfun(1,2,3,4,5,&#39;abc&#39;) ## 1 ## (2, 3, 4, 5, &#39;abc&#39;) 5.3.7.3 Example 3 First argument goes to x, second argument goest to y, remaining goes to args def myfun(x,y,*args): print (x) print (y) print (args) #tuple myfun(1,2,3) ## 1 ## 2 ## (3,) 5.3.7.4 Example 4 def myfun(x,*args, y=9): print (x) print (y) print (args) #tuple myfun(1,2,3,4,5) ## 1 ## 9 ## (2, 3, 4, 5) 5.3.7.5 Example 5 All goes to args def myfun(*args): print (args) #tuple myfun(1,2,3,4,5) ## (1, 2, 3, 4, 5) 5.3.7.6 Example 6 Empty args def myfun(x,y,*args): print (x) print (y) print (args) myfun(1,2) ## 1 ## 2 ## () 5.3.8 keyword arguments kwargs is a dictionary 5.3.8.1 Example 1 def foo(**kwargs): print(kwargs) foo(a=1,b=2,c=3) ## {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3} 5.3.8.2 Example 2 def foo(x,**kwargs): print(x) print(kwargs) foo(9,a=1,b=2,c=3) ## 9 ## {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3} foo(9) #empty dictionary ## 9 ## {} 5.3.8.3 Example 3 def foo(a,b,c,d=1): print(a) print(b) print(c) print(d) foo(**{&quot;a&quot;:2,&quot;b&quot;:3,&quot;c&quot;:4}) ## 2 ## 3 ## 4 ## 1 5.3.9 Mixing *args, **kwargs Always put args before kwargs 5.3.9.1 Example 1 def foo(x,y=1,**kwargs): print (x) print (y) print (kwargs) foo(1,2,c=3,d=4) ## 1 ## 2 ## {&#39;c&#39;: 3, &#39;d&#39;: 4} 5.3.9.2 Example 2 def foo(x,y=2,*args,**kwargs): print (x) print (y) print (args) print (kwargs) foo(1,2,3,4,5,c=6,d=7) ## 1 ## 2 ## (3, 4, 5) ## {&#39;c&#39;: 6, &#39;d&#39;: 7} "],
["exception-handling.html", "6 Exception Handling 6.1 Catching Error 6.2 Custom Exception", " 6 Exception Handling The try statement works as follows: - First, the try clause (the statement(s) between the try and except keywords) is executed - If no exception occurs, the except clause is skipped and execution of the try statement is finished - If an exception occurs during execution of the try clause, the rest of the clause is skipped. Then if its type matches the exception named after the except keyword, the except clause is executed, and then execution continues after the try statement - If an exception occurs which does not match the exception named in the except clause, it is passed on to outer try statements; if no handler is found, it is an unhandled exception and execution stops with a message as shown above A try statement may have more than one except clause, to specify handlers for different exceptions. 6.1 Catching Error Different exception object has different attributes. try: a = 1 + &#39;a&#39; ## known error except TypeError as err: print(&#39;I know this error !!!!&#39;, &#39;\\n Error: &#39;, err, &#39;\\n Args: &#39;, err.args, &#39;\\n Type: &#39;, type(err)) ## all other error except Exception as err: print( &#39;Error: &#39;, err, &#39;\\nArgs: &#39;, err.args, &#39;\\nType: &#39;, type(err)) ## I know this error !!!! ## Error: unsupported operand type(s) for +: &#39;int&#39; and &#39;str&#39; ## Args: (&quot;unsupported operand type(s) for +: &#39;int&#39; and &#39;str&#39;&quot;,) ## Type: &lt;class &#39;TypeError&#39;&gt; 6.2 Custom Exception try: raise Exception(&#39;bloody&#39;, &#39;hell&#39;) #simulate exception except Exception as err: print( &#39;Error: &#39;, err, &#39;\\nArgs: &#39;, err.args, &#39;\\nType: &#39;, type(err)) ## Error: (&#39;bloody&#39;, &#39;hell&#39;) ## Args: (&#39;bloody&#39;, &#39;hell&#39;) ## Type: &lt;class &#39;Exception&#39;&gt; "],
["object-oriented-programming.html", "7 Object Oriented Programming 7.1 Defining Class 7.2 Constructor 7.3 Calling Method 7.4 Getting Property 7.5 Setting Property", " 7 Object Oriented Programming 7.1 Defining Class Every function within a class must have at least one parameter - self Use init as the constructor function. init is optional class Person: wallet = 0 # def __init__(self, myname,money=0): # constructor self.name = myname self.wallet=money print(&#39;I\\&#39;m in Person Constructor: {}&#39;.format(myname)) def say_hi(self): print(&#39;Hello, my name is : &#39;, self.name) def say_bye(self): print(&#39;Goodbye&#39;, Person.ID) def take(self,amount): self.wallet+=amount def balance(self): print(&#39;Wallet Balance:&#39;,self.wallet) def MakeCry(self): self.Cry() class Kid(Person): def __init__(self, myname, money=0): print(&#39;I\\&#39;m in Kid Constructor: {}&#39;.format(myname)) super().__init__(myname=myname, money=money) def Cry(self): print(&#39;Kid is crying&#39;) 7.2 Constructor p1 = Person(&#39;Yong&#39;) ## I&#39;m in Person Constructor: Yong p2 = Person(&#39;Gan&#39;,200) ## I&#39;m in Person Constructor: Gan p3 = Kid(&#39;Jolin&#39;,50) ## I&#39;m in Kid Constructor: Jolin ## I&#39;m in Person Constructor: Jolin 7.3 Calling Method p1.say_hi() ## Hello, my name is : Yong p1.balance() ## Wallet Balance: 0 p3.Cry() ## Kid is crying p3.MakeCry() ## Kid is crying p2.say_hi() ## Hello, my name is : Gan p2.balance() ## Wallet Balance: 200 7.4 Getting Property p1.wallet ## 0 p2.wallet ## 200 7.5 Setting Property p1.wallet = 900 p1.wallet ## 900 "],
["decorator.html", "8 Decorator 8.1 Definition 8.2 Examples", " 8 Decorator 8.1 Definition Decorator is a function that accept callable as the only argument The main purpose of decarator is to enhance the program of the decorated function It returns a callable 8.2 Examples 8.2.1 Example 1 - Plain decorator function Many times, it is useful to register a function elsewhere - for example, registering a task in a task runner, or a functin with signal handler register is a decarator, it accept decorated as the only argument foo() and bar() are the decorated function of register registry = [] def register(decorated): registry.append(decorated) return decorated @register def foo(): return 3 @register def bar(): return 5 registry ## [&lt;function foo at 0x00000000045D7620&gt;, &lt;function bar at 0x00000000045D0AE8&gt;] registry[0]() ## 3 registry[1]() ## 5 8.2.2 Example 2 - Decorator with Class Extending the use case above register is the decarator, it has only one argument class Registry(object): def __init__(self): self._functions = [] def register(self,decorated): self._functions.append(decorated) return decorated def run_all(self,*args,**kwargs): return_values = [] for func in self._functions: return_values.append(func(*args,**kwargs)) return return_values The decorator will decorate two functions, for both object a and b a = Registry() b = Registry() @a.register def foo(x=3): return x @b.register def bar(x=5): return x @a.register @b.register def bax(x=7): return x Observe the result print (a._functions) ## [&lt;function foo at 0x00000000045E5378&gt;, &lt;function bax at 0x00000000045E5488&gt;] print (b._functions) ## [&lt;function bar at 0x00000000045E5400&gt;, &lt;function bax at 0x00000000045E5488&gt;] print (a.run_all()) ## [3, 7] print (b.run_all()) ## [5, 7] print ( a.run_all(x=9) ) ## [9, 9] print ( b.run_all(x=9) ) ## [9, 9] "],
["datetime-standard-library.html", "9 datetime Standard Library 9.1 ISO8601 9.2 Module Import 9.3 Class 9.4 date 9.5 date and datetime 9.6 time 9.7 timedelta", " 9 datetime Standard Library This is a built-in library by Python. There is no need to install this library. 9.1 ISO8601 https://en.wikipedia.org/wiki/ISO_8601#Time_zone_designators 9.1.1 Date Time UTC: \"2007-04-05T14:30Z\" #notice Z GMT+8: \"2007-04-05T12:30+08:00 #notice +08:00 GMT+8: \"2007-04-05T12:30+0800 #notice +0800 GMT+8: \"2007-04-05T12:30+08 #notice +08 9.1.2 Date 2019-02-04 #notice no timezone available 9.2 Module Import from datetime import date # module for date object from datetime import time # module for time object from datetime import datetime # module for datetime object from datetime import timedelta 9.3 Class datetime library contain three class of objects: - date (year,month,day) - time (hour,minute,second) - datetime (year,month,day,hour,minute,second) - timedelta: duration between two datetime or date object 9.4 date 9.4.1 Constructor print( date(2000,1,1) ) ## 2000-01-01 print( date(year=2000,month=1,day=1) ) ## 2000-01-01 print( type(date(year=2000,month=1,day=1))) ## &lt;class &#39;datetime.date&#39;&gt; 9.4.2 Class Method 9.4.2.1 today This is local date (not UTC) date.today() ## datetime.date(2020, 3, 14) print( date.today() ) ## 2020-03-14 9.4.2.2 Convert From ISO fromisoformat strptime is not available for date conversion. It is only for datetime conversion date.fromisoformat(&#39;2011-11-11&#39;) ## datetime.date(2011, 11, 11) To convert non-iso format date string to date object, convert to datetime first, then to date 9.4.3 Instance Method 9.4.3.1 replace() Replace year/month/day with specified parameter, non specified params will remain unchange. Example below change only month. You can change year or day in combination print( date.today() ) ## 2020-03-14 print( date.today().replace(month=8) ) ## 2020-08-14 9.4.3.2 weekday(), isoweekday() For weekday(), Zero being Monday For isoweekday(), Zero being Sunday print( date.today().weekday() ) ## 5 print( date.today().isoweekday() ) ## 6 weekdays = [&#39;Mon&#39;,&#39;Tue&#39;,&#39;Wed&#39;,&#39;Thu&#39;,&#39;Fri&#39;,&#39;Sat&#39;,&#39;Sun&#39;] wd = date.today().weekday() print( date.today(), &quot;is day&quot;, wd ,&quot;which is&quot;, weekdays[wd] ) ## 2020-03-14 is day 5 which is Sat 9.4.3.3 Formating with isoformat() isoformat() return ISO 8601 String (YYYY-MM-DD) date.today().isoformat() # return string ## &#39;2020-03-14&#39; 9.4.3.4 Formating with strftime For complete directive, see below: https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior date.today().strftime(&quot;%m/%d&quot;) ## &#39;03/14&#39; 9.4.3.5 isocalendar() isocalendar return a 3-tuple, (ISO year, ISO week number, ISO weekday). date.today().isocalendar() ## return tuple ## (2020, 11, 6) 9.4.4 Attributes print( date.today().year ) ## 2020 print( date.today().month ) ## 3 print( date.today().day ) ## 14 9.5 date and datetime 9.5.1 Constructor import datetime as dt print( dt.date(2000,1,1,), &#39;\\n&#39;, dt.datetime(2000,1,1,0,0,0), &#39;\\n&#39;, dt.datetime(year=2000,month=1,day=1,hour=23,minute=15,second=55),&#39;\\n&#39;, type(dt.date(2000,1,1)),&#39;\\n&#39;, type(dt.datetime(2000,1,1,0,0,0))) ## 2000-01-01 ## 2000-01-01 00:00:00 ## 2000-01-01 23:15:55 ## &lt;class &#39;datetime.date&#39;&gt; ## &lt;class &#39;datetime.datetime&#39;&gt; 9.5.2 Class Method 9.5.2.1 now and today Both now() and today() return current system local datetime, no timezone print( dt.datetime.now(), &#39;\\n&#39;, dt.datetime.now().date()) ## 2020-03-14 11:04:02.547278 ## 2020-03-14 dt.datetime.today() ## datetime.datetime(2020, 3, 14, 11, 4, 2, 581282) 9.5.2.2 utcnow dt.datetime.utcnow() ## datetime.datetime(2020, 3, 14, 3, 4, 2, 616276) 9.5.2.3 combine() date and time Apply datetime.combine() module method on both date and time object to get datetime now = dt.datetime.now() dt.datetime.combine(now.date(), now.time()) ## datetime.datetime(2020, 3, 14, 11, 4, 2, 648275) 9.5.2.4 Convert from String strptime() Use strptime to convert string into datetime object %I : 12-hour %H : 24-hour %M : Minute %p : AM/PM %y : 18 %Y : 2018 %b : Mar %m : month (1 to 12) %d : day datetime.strptime(&#39;2011-02-25&#39;,&#39;%Y-%m-%d&#39;) ## datetime.datetime(2011, 2, 25, 0, 0) datetime.strptime(&#39;9-01-18&#39;,&#39;%d-%m-%y&#39;) ## datetime.datetime(2018, 1, 9, 0, 0) datetime.strptime(&#39;09-Mar-2018&#39;,&#39;%d-%b-%Y&#39;) ## datetime.datetime(2018, 3, 9, 0, 0) datetime.strptime(&#39;2/5/2018 4:49 PM&#39;, &#39;%m/%d/%Y %I:%M %p&#39;) ## datetime.datetime(2018, 2, 5, 16, 49) 9.5.2.5 Convert from ISO fromisoformat fromisoformat() is intend to be reverse of isoformat() It actually not ISO compliance: when Z or +8 is included at the end of the string, error occur #s = dt.datetime.now().isoformat() dt.datetime.fromisoformat(&quot;2019-02-05T10:22:33&quot;) ## datetime.datetime(2019, 2, 5, 10, 22, 33) 9.5.3 Instance Method 9.5.3.1 weekday datetime.now().weekday() ## 5 9.5.3.2 replace datetime.now().replace(year=1999) ## datetime.datetime(1999, 3, 14, 11, 4, 2, 959293) 9.5.3.3 convert to .time() datetime.now().time() ## datetime.time(11, 4, 3, 2278) 9.5.3.4 Convert to .date() datetime.now().date() ## datetime.date(2020, 3, 14) 9.5.3.5 Convert to String str str( datetime.now() ) ## &#39;2020-03-14 11:04:03.083276&#39; Use strftime() dt.datetime.now().strftime(&#39;%d-%b-%Y&#39;) ## &#39;14-Mar-2020&#39; dt.datetime.utcnow().strftime(&#39;%Y-%m-%dT%H:%M:%S.%fZ&#39;) ## ISO 8601 UTC ## &#39;2020-03-14T03:04:03.182275Z&#39; Use isoformat() dt.datetime.utcnow().isoformat() ## &#39;2020-03-14T03:04:03.222275&#39; 9.5.4 Attributes print( datetime.now().year ) ## 2020 print( datetime.now().month ) ## 3 print( datetime.now().day ) ## 14 print( datetime.now().hour ) ## 11 print( datetime.now().minute ) ## 4 9.6 time 9.6.1 Constructor print( time(2) ) #default single arugement, hour ## 02:00:00 print( time(2,15) ) #default two arguments, hour, minute ## 02:15:00 print( time(hour=2,minute=15,second=30) ) ## 02:15:30 9.6.2 Class Method 9.6.2.1 now() There is unfortunately no single function to extract the current time. Use time() function of an datetime object datetime.now().time() ## datetime.time(11, 4, 3, 362274) 9.6.3 Attributes print( datetime.now().time().hour ) ## 11 print( datetime.now().time().minute ) ## 4 print( datetime.now().time().second ) ## 3 9.7 timedelta years argument is not supported Apply timedelta on datetime object timedelta cannot be applied on time object , because timedelta potentially go beyond single day (24H) delt = timedelta(days=365,minutes=33,seconds=15) now = datetime.now() print (&#39;delt+now : &#39;, now+delt) ## delt+now : 2021-03-14 11:37:18.533274 "],
["getting-external-data.html", "10 Getting External Data", " 10 Getting External Data "],
["plydata-dplyr-for-python.html", "11 Plydata (dplyr for Python) 11.1 Sample Data 11.2 Column Manipulation 11.3 Sorting (arrange) 11.4 Grouping 11.5 Summarization", " 11 Plydata (dplyr for Python) 11.1 Sample Data n = 200 comp = [&#39;C&#39; + i for i in np.random.randint( 1,4, size = n).astype(str)] # 3x Company dept = [&#39;D&#39; + i for i in np.random.randint( 1,6, size = n).astype(str)] # 5x Department grp = [&#39;G&#39; + i for i in np.random.randint( 1,3, size = n).astype(str)] # 2x Groups value1 = np.random.normal( loc=50 , scale=5 , size = n) value2 = np.random.normal( loc=20 , scale=3 , size = n) #value3 = np.random.normal( loc=5 , scale=30 , size = n) mydf = pd.DataFrame({ &#39;comp&#39;:comp, &#39;dept&#39;:dept, &#39;grp&#39;: grp, &#39;value1&#39;:value1, &#39;value2&#39;:value2 #&#39;value3&#39;:value3 }) mydf.head() ## comp dept grp value1 value2 ## 0 C2 D4 G2 43.954278 22.105300 ## 1 C1 D1 G2 56.168341 19.222227 ## 2 C1 D2 G2 50.201138 20.928504 ## 3 C1 D4 G1 52.368635 23.013335 ## 4 C1 D2 G1 49.821394 22.916938 11.2 Column Manipulation 11.2.1 Copy Column mydf &gt;&gt; define(newcol = &#39;value1&#39;) # simple method for one column ## comp dept grp value1 value2 newcol ## 0 C2 D4 G2 43.954278 22.105300 43.954278 ## 1 C1 D1 G2 56.168341 19.222227 56.168341 ## 2 C1 D2 G2 50.201138 20.928504 50.201138 ## 3 C1 D4 G1 52.368635 23.013335 52.368635 ## 4 C1 D2 G1 49.821394 22.916938 49.821394 ## .. ... ... .. ... ... ... ## 195 C2 D1 G1 48.110583 24.628444 48.110583 ## 196 C3 D3 G2 49.991787 21.601550 49.991787 ## 197 C1 D2 G1 45.252883 19.904932 45.252883 ## 198 C2 D2 G2 44.437240 20.856602 44.437240 ## 199 C3 D4 G1 46.616023 18.391714 46.616023 ## ## [200 rows x 6 columns] mydf &gt;&gt; define ((&#39;newcol1&#39;, &#39;value1&#39;), newcol2=&#39;value2&#39;) # method for muiltiple new columns ## comp dept grp value1 value2 newcol1 newcol2 ## 0 C2 D4 G2 43.954278 22.105300 43.954278 22.105300 ## 1 C1 D1 G2 56.168341 19.222227 56.168341 19.222227 ## 2 C1 D2 G2 50.201138 20.928504 50.201138 20.928504 ## 3 C1 D4 G1 52.368635 23.013335 52.368635 23.013335 ## 4 C1 D2 G1 49.821394 22.916938 49.821394 22.916938 ## .. ... ... .. ... ... ... ... ## 195 C2 D1 G1 48.110583 24.628444 48.110583 24.628444 ## 196 C3 D3 G2 49.991787 21.601550 49.991787 21.601550 ## 197 C1 D2 G1 45.252883 19.904932 45.252883 19.904932 ## 198 C2 D2 G2 44.437240 20.856602 44.437240 20.856602 ## 199 C3 D4 G1 46.616023 18.391714 46.616023 18.391714 ## ## [200 rows x 7 columns] 11.2.2 New Column from existing Column Without specify the new column name, it will be derived from expression mydf &gt;&gt; define (&#39;value1*2&#39;) ## comp dept grp value1 value2 value1*2 ## 0 C2 D4 G2 43.954278 22.105300 87.908557 ## 1 C1 D1 G2 56.168341 19.222227 112.336682 ## 2 C1 D2 G2 50.201138 20.928504 100.402275 ## 3 C1 D4 G1 52.368635 23.013335 104.737270 ## 4 C1 D2 G1 49.821394 22.916938 99.642789 ## .. ... ... .. ... ... ... ## 195 C2 D1 G1 48.110583 24.628444 96.221167 ## 196 C3 D3 G2 49.991787 21.601550 99.983573 ## 197 C1 D2 G1 45.252883 19.904932 90.505767 ## 198 C2 D2 G2 44.437240 20.856602 88.874481 ## 199 C3 D4 G1 46.616023 18.391714 93.232045 ## ## [200 rows x 6 columns] Specify the new column name mydf &gt;&gt; define(value3 = &#39;value1*2&#39;) ## comp dept grp value1 value2 value3 ## 0 C2 D4 G2 43.954278 22.105300 87.908557 ## 1 C1 D1 G2 56.168341 19.222227 112.336682 ## 2 C1 D2 G2 50.201138 20.928504 100.402275 ## 3 C1 D4 G1 52.368635 23.013335 104.737270 ## 4 C1 D2 G1 49.821394 22.916938 99.642789 ## .. ... ... .. ... ... ... ## 195 C2 D1 G1 48.110583 24.628444 96.221167 ## 196 C3 D3 G2 49.991787 21.601550 99.983573 ## 197 C1 D2 G1 45.252883 19.904932 90.505767 ## 198 C2 D2 G2 44.437240 20.856602 88.874481 ## 199 C3 D4 G1 46.616023 18.391714 93.232045 ## ## [200 rows x 6 columns] Define multiple new columns in one go. Observe there are three ways to specify the new columns mydf &gt;&gt; define(&#39;value1*2&#39;,(&#39;newcol2&#39;,&#39;value2*2&#39;),newcol3=&#39;value2*3&#39;) ## comp dept grp value1 value2 value1*2 newcol2 newcol3 ## 0 C2 D4 G2 43.954278 22.105300 87.908557 44.210599 66.315899 ## 1 C1 D1 G2 56.168341 19.222227 112.336682 38.444454 57.666681 ## 2 C1 D2 G2 50.201138 20.928504 100.402275 41.857008 62.785512 ## 3 C1 D4 G1 52.368635 23.013335 104.737270 46.026671 69.040006 ## 4 C1 D2 G1 49.821394 22.916938 99.642789 45.833877 68.750815 ## .. ... ... .. ... ... ... ... ... ## 195 C2 D1 G1 48.110583 24.628444 96.221167 49.256888 73.885332 ## 196 C3 D3 G2 49.991787 21.601550 99.983573 43.203100 64.804650 ## 197 C1 D2 G1 45.252883 19.904932 90.505767 39.809863 59.714795 ## 198 C2 D2 G2 44.437240 20.856602 88.874481 41.713205 62.569807 ## 199 C3 D4 G1 46.616023 18.391714 93.232045 36.783428 55.175143 ## ## [200 rows x 8 columns] 11.2.3 Select Column(s) mydf2 = mydf &gt;&gt; define(newcol1=&#39;value1&#39;,newcol2=&#39;value2&#39;) mydf2.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 200 entries, 0 to 199 ## Data columns (total 7 columns): ## comp 200 non-null object ## dept 200 non-null object ## grp 200 non-null object ## value1 200 non-null float64 ## value2 200 non-null float64 ## newcol1 200 non-null float64 ## newcol2 200 non-null float64 ## dtypes: float64(4), object(3) ## memory usage: 11.1+ KB 11.2.3.1 By Column Names Exact Coumn Name mydf2 &gt;&gt; select (&#39;comp&#39;,&#39;dept&#39;,&#39;value1&#39;) ## comp dept value1 ## 0 C2 D4 43.954278 ## 1 C1 D1 56.168341 ## 2 C1 D2 50.201138 ## 3 C1 D4 52.368635 ## 4 C1 D2 49.821394 ## .. ... ... ... ## 195 C2 D1 48.110583 ## 196 C3 D3 49.991787 ## 197 C1 D2 45.252883 ## 198 C2 D2 44.437240 ## 199 C3 D4 46.616023 ## ## [200 rows x 3 columns] Column Name Starts With … mydf2 &gt;&gt; select (&#39;comp&#39;, startswith=&#39;val&#39;) ## comp value1 value2 ## 0 C2 43.954278 22.105300 ## 1 C1 56.168341 19.222227 ## 2 C1 50.201138 20.928504 ## 3 C1 52.368635 23.013335 ## 4 C1 49.821394 22.916938 ## .. ... ... ... ## 195 C2 48.110583 24.628444 ## 196 C3 49.991787 21.601550 ## 197 C1 45.252883 19.904932 ## 198 C2 44.437240 20.856602 ## 199 C3 46.616023 18.391714 ## ## [200 rows x 3 columns] Column Name Ends With … mydf2 &gt;&gt; select (&#39;comp&#39;,endswith=(&#39;1&#39;,&#39;2&#39;,&#39;3&#39;)) ## comp value1 value2 newcol1 newcol2 ## 0 C2 43.954278 22.105300 43.954278 22.105300 ## 1 C1 56.168341 19.222227 56.168341 19.222227 ## 2 C1 50.201138 20.928504 50.201138 20.928504 ## 3 C1 52.368635 23.013335 52.368635 23.013335 ## 4 C1 49.821394 22.916938 49.821394 22.916938 ## .. ... ... ... ... ... ## 195 C2 48.110583 24.628444 48.110583 24.628444 ## 196 C3 49.991787 21.601550 49.991787 21.601550 ## 197 C1 45.252883 19.904932 45.252883 19.904932 ## 198 C2 44.437240 20.856602 44.437240 20.856602 ## 199 C3 46.616023 18.391714 46.616023 18.391714 ## ## [200 rows x 5 columns] Column Name Contains … mydf2 &gt;&gt; select(&#39;comp&#39;, contains=(&#39;col&#39;,&#39;val&#39;)) ## comp value1 value2 newcol1 newcol2 ## 0 C2 43.954278 22.105300 43.954278 22.105300 ## 1 C1 56.168341 19.222227 56.168341 19.222227 ## 2 C1 50.201138 20.928504 50.201138 20.928504 ## 3 C1 52.368635 23.013335 52.368635 23.013335 ## 4 C1 49.821394 22.916938 49.821394 22.916938 ## .. ... ... ... ... ... ## 195 C2 48.110583 24.628444 48.110583 24.628444 ## 196 C3 49.991787 21.601550 49.991787 21.601550 ## 197 C1 45.252883 19.904932 45.252883 19.904932 ## 198 C2 44.437240 20.856602 44.437240 20.856602 ## 199 C3 46.616023 18.391714 46.616023 18.391714 ## ## [200 rows x 5 columns] 11.2.3.2 Specify Column Range mydf2 &gt;&gt; select (&#39;comp&#39;, slice(&#39;value1&#39;,&#39;newcol2&#39;)) ## comp value1 value2 newcol1 newcol2 ## 0 C2 43.954278 22.105300 43.954278 22.105300 ## 1 C1 56.168341 19.222227 56.168341 19.222227 ## 2 C1 50.201138 20.928504 50.201138 20.928504 ## 3 C1 52.368635 23.013335 52.368635 23.013335 ## 4 C1 49.821394 22.916938 49.821394 22.916938 ## .. ... ... ... ... ... ## 195 C2 48.110583 24.628444 48.110583 24.628444 ## 196 C3 49.991787 21.601550 49.991787 21.601550 ## 197 C1 45.252883 19.904932 45.252883 19.904932 ## 198 C2 44.437240 20.856602 44.437240 20.856602 ## 199 C3 46.616023 18.391714 46.616023 18.391714 ## ## [200 rows x 5 columns] 11.2.4 Drop Column(s) mydf2 &gt;&gt; select(&#39;newcol1&#39;,&#39;newcol2&#39;,drop=True) ## comp dept grp value1 value2 ## 0 C2 D4 G2 43.954278 22.105300 ## 1 C1 D1 G2 56.168341 19.222227 ## 2 C1 D2 G2 50.201138 20.928504 ## 3 C1 D4 G1 52.368635 23.013335 ## 4 C1 D2 G1 49.821394 22.916938 ## .. ... ... .. ... ... ## 195 C2 D1 G1 48.110583 24.628444 ## 196 C3 D3 G2 49.991787 21.601550 ## 197 C1 D2 G1 45.252883 19.904932 ## 198 C2 D2 G2 44.437240 20.856602 ## 199 C3 D4 G1 46.616023 18.391714 ## ## [200 rows x 5 columns] mydf &gt;&gt; rename( {&#39;val.1&#39; : &#39;value1&#39;, &#39;val.2&#39; : &#39;value2&#39; }) ## comp dept grp val.1 val.2 ## 0 C2 D4 G2 43.954278 22.105300 ## 1 C1 D1 G2 56.168341 19.222227 ## 2 C1 D2 G2 50.201138 20.928504 ## 3 C1 D4 G1 52.368635 23.013335 ## 4 C1 D2 G1 49.821394 22.916938 ## .. ... ... .. ... ... ## 195 C2 D1 G1 48.110583 24.628444 ## 196 C3 D3 G2 49.991787 21.601550 ## 197 C1 D2 G1 45.252883 19.904932 ## 198 C2 D2 G2 44.437240 20.856602 ## 199 C3 D4 G1 46.616023 18.391714 ## ## [200 rows x 5 columns] Combined Method Combine both assignment and dictionary method mydf &gt;&gt; rename( {&#39;val.1&#39; : &#39;value1&#39;, &#39;val.2&#39; : &#39;value2&#39; }, group = &#39;grp&#39; ) ## comp dept group val.1 val.2 ## 0 C2 D4 G2 43.954278 22.105300 ## 1 C1 D1 G2 56.168341 19.222227 ## 2 C1 D2 G2 50.201138 20.928504 ## 3 C1 D4 G1 52.368635 23.013335 ## 4 C1 D2 G1 49.821394 22.916938 ## .. ... ... ... ... ... ## 195 C2 D1 G1 48.110583 24.628444 ## 196 C3 D3 G2 49.991787 21.601550 ## 197 C1 D2 G1 45.252883 19.904932 ## 198 C2 D2 G2 44.437240 20.856602 ## 199 C3 D4 G1 46.616023 18.391714 ## ## [200 rows x 5 columns] 11.3 Sorting (arrange) Use ‘-colName’ for decending mydf &gt;&gt; arrange(&#39;comp&#39;, &#39;-value1&#39;) ## comp dept grp value1 value2 ## 120 C1 D2 G2 62.580226 19.605437 ## 33 C1 D5 G2 61.826041 24.082190 ## 159 C1 D2 G2 59.948538 17.315291 ## 117 C1 D1 G1 57.631347 21.586851 ## 124 C1 D1 G2 57.158583 26.812822 ## .. ... ... .. ... ... ## 141 C3 D1 G2 43.422948 21.032885 ## 34 C3 D1 G1 43.367125 21.856450 ## 14 C3 D2 G2 43.234333 15.461173 ## 127 C3 D4 G1 41.940449 18.349287 ## 144 C3 D2 G1 41.116459 19.534499 ## ## [200 rows x 5 columns] 11.4 Grouping mydf.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 200 entries, 0 to 199 ## Data columns (total 5 columns): ## comp 200 non-null object ## dept 200 non-null object ## grp 200 non-null object ## value1 200 non-null float64 ## value2 200 non-null float64 ## dtypes: float64(2), object(3) ## memory usage: 7.9+ KB gdf = mydf &gt;&gt; group_by(&#39;comp&#39;,&#39;dept&#39;) type(gdf) ## &lt;class &#39;plydata.types.GroupedDataFrame&#39;&gt; 11.5 Summarization 11.5.1 Simple Method Passing Multiple Expressions gdf &gt;&gt; summarize(&#39;n()&#39;,&#39;sum(value1)&#39;,&#39;mean(value2)&#39;) ## comp dept n() sum(value1) mean(value2) ## 0 C2 D4 12 601.079383 19.814068 ## 1 C1 D1 13 681.501862 22.175850 ## 2 C1 D2 20 999.795884 20.227498 ## 3 C1 D4 10 504.006995 21.018179 ## 4 C3 D4 22 1103.732461 20.110725 ## .. ... ... ... ... ... ## 10 C2 D3 16 804.891563 19.903550 ## 11 C1 D5 9 457.927773 20.623450 ## 12 C3 D5 10 510.961591 19.237443 ## 13 C2 D1 12 626.821672 18.361628 ## 14 C3 D1 12 606.251377 19.331650 ## ## [15 rows x 5 columns] 11.5.2 Specify Summarized Column Name Assignment Method - Passing colName=‘expression’** - Column name cannot contain special character gdf &gt;&gt; summarize(count=&#39;n()&#39;,v1sum=&#39;sum(value1)&#39;,v2_mean=&#39;mean(value2)&#39;) ## comp dept count v1sum v2_mean ## 0 C2 D4 12 601.079383 19.814068 ## 1 C1 D1 13 681.501862 22.175850 ## 2 C1 D2 20 999.795884 20.227498 ## 3 C1 D4 10 504.006995 21.018179 ## 4 C3 D4 22 1103.732461 20.110725 ## .. ... ... ... ... ... ## 10 C2 D3 16 804.891563 19.903550 ## 11 C1 D5 9 457.927773 20.623450 ## 12 C3 D5 10 510.961591 19.237443 ## 13 C2 D1 12 626.821672 18.361628 ## 14 C3 D1 12 606.251377 19.331650 ## ## [15 rows x 5 columns] Tuple Method (‘colName’,‘expression’) Use when the column name contain special character gdf &gt;&gt; summarize((&#39;count&#39;,&#39;n()&#39;),(&#39;v1.sum&#39;,&#39;sum(value1)&#39;),(&#39;s2.sum&#39;,&#39;sum(value2)&#39;),v2mean=np.mean(value2)) ## comp dept count v1.sum s2.sum v2mean ## 0 C2 D4 12 601.079383 237.768816 20.148306 ## 1 C1 D1 13 681.501862 288.286046 20.148306 ## 2 C1 D2 20 999.795884 404.549956 20.148306 ## 3 C1 D4 10 504.006995 210.181789 20.148306 ## 4 C3 D4 22 1103.732461 442.435943 20.148306 ## .. ... ... ... ... ... ... ## 10 C2 D3 16 804.891563 318.456800 20.148306 ## 11 C1 D5 9 457.927773 185.611048 20.148306 ## 12 C3 D5 10 510.961591 192.374427 20.148306 ## 13 C2 D1 12 626.821672 220.339537 20.148306 ## 14 C3 D1 12 606.251377 231.979805 20.148306 ## ## [15 rows x 6 columns] 11.5.3 Number of Rows in Group n() : total rows in group n_unique() : total of rows with unique value gdf &gt;&gt; summarize(count=&#39;n()&#39;, va11_unique=&#39;n_unique(value1)&#39;) ## comp dept count va11_unique ## 0 C2 D4 12 12 ## 1 C1 D1 13 13 ## 2 C1 D2 20 20 ## 3 C1 D4 10 10 ## 4 C3 D4 22 22 ## .. ... ... ... ... ## 10 C2 D3 16 16 ## 11 C1 D5 9 9 ## 12 C3 D5 10 10 ## 13 C2 D1 12 12 ## 14 C3 D1 12 12 ## ## [15 rows x 4 columns] "],
["numpy-1.html", "12 numpy 12.1 Environment Setup 12.2 Module Import 12.3 Data Types 12.4 Numpy Array 12.5 Random Numbers 12.6 Sampling (Integer) 12.7 NaN : Missing Numerical Data", " 12 numpy Best array data manipulation, fast numpy array allows only single data type, unlike list Support matrix operation 12.1 Environment Setup from IPython.core.display import display, HTML display(HTML(&quot;&lt;style&gt;.container { width:75% !important; margin-left:350px; }&lt;/style&gt;&quot;)) #%matplotlib inline ## &lt;IPython.core.display.HTML object&gt; import pandas as pd import matplotlib.pyplot as plt import math pd.set_option( &#39;display.notebook_repr_html&#39;, False) # render Series and DataFrame as text, not HTML pd.set_option( &#39;display.max_column&#39;, 10) # number of columns pd.set_option( &#39;display.max_rows&#39;, 10) # number of rows pd.set_option( &#39;display.width&#39;, 90) # number of characters per row 12.2 Module Import import numpy as np np.__version__ ## other modules ## &#39;1.17.3&#39; from datetime import datetime from datetime import date from datetime import time 12.3 Data Types 12.3.1 NumPy Data Types NumPy supports a much greater variety of numerical types than Python does. This makes numpy much more powerful https://www.numpy.org/devdocs/user/basics.types.html Integer: np.int8, np.int16, np.int32, np.uint8, np.uint16, np.uint32 Float: np.float32, np.float64 12.3.2 int32/64 np.int is actually python standard int x = np.int(13) y = int(13) print( type(x) ) ## &lt;class &#39;int&#39;&gt; print( type(y) ) ## &lt;class &#39;int&#39;&gt; np.int32/64 are NumPy specific x = np.int32(13) y = np.int64(13) print( type(x) ) ## &lt;class &#39;numpy.int32&#39;&gt; print( type(y) ) ## &lt;class &#39;numpy.int64&#39;&gt; 12.3.3 float32/64 x = np.float(13) y = float(13) print( type(x) ) ## &lt;class &#39;float&#39;&gt; print( type(y) ) ## &lt;class &#39;float&#39;&gt; x = np.float32(13) y = np.float64(13) print( type(x) ) ## &lt;class &#39;numpy.float32&#39;&gt; print( type(y) ) ## &lt;class &#39;numpy.float64&#39;&gt; 12.3.4 bool np.bool is actually python standard bool x = np.bool(True) print( type(x) ) ## &lt;class &#39;bool&#39;&gt; print( type(True) ) ## &lt;class &#39;bool&#39;&gt; 12.3.5 str np.str is actually python standard str x = np.str(&quot;ali&quot;) print( type(x) ) ## &lt;class &#39;str&#39;&gt; x = np.str_(&quot;ali&quot;) print( type(x) ) ## &lt;class &#39;numpy.str_&#39;&gt; 12.3.6 datetime64 Unlike python standard datetime library, there is no seperation of date, datetime and time. There is no time equivalent object NumPy only has one object: datetime64 object . 12.3.6.1 Constructor From String Note that the input string cannot be ISO8601 compliance, meaning any timezone related information at the end of the string (such as Z or +8) will result in error. np.datetime64(&#39;2005-02&#39;) ## numpy.datetime64(&#39;2005-02&#39;) np.datetime64(&#39;2005-02-25&#39;) ## numpy.datetime64(&#39;2005-02-25&#39;) np.datetime64(&#39;2005-02-25T03:30&#39;) ## numpy.datetime64(&#39;2005-02-25T03:30&#39;) From datetime np.datetime64( date.today() ) ## numpy.datetime64(&#39;2020-03-14&#39;) np.datetime64( datetime.now() ) ## numpy.datetime64(&#39;2020-03-14T11:04:06.064892&#39;) 12.3.6.2 Instance Method Convert to datetime using astype() dt64 = np.datetime64(&quot;2019-01-31&quot; ) dt64.astype(datetime) ## datetime.date(2019, 1, 31) 12.3.7 nan 12.3.7.1 Creating NaN NaN is NOT A BUILT-IN datatype. It means not a number, a numpy float object type. Can be created using two methods below. import numpy as np import pandas as pd import math kosong1 = float(&#39;NaN&#39;) kosong2 = np.nan print(&#39;Type: &#39;, type(kosong1), &#39;\\n&#39;, &#39;Value: &#39;, kosong1) ## Type: &lt;class &#39;float&#39;&gt; ## Value: nan print(&#39;Type: &#39;, type(kosong2), &#39;\\n&#39;, &#39;Value: &#39;, kosong2) ## Type: &lt;class &#39;float&#39;&gt; ## Value: nan 12.3.7.2 Detecting NaN Detect nan using various function from panda, numpy and math. print(pd.isna(kosong1), &#39;\\n&#39;, pd.isna(kosong2), &#39;\\n&#39;, np.isnan(kosong1),&#39;\\n&#39;, math.isnan(kosong2)) ## True ## True ## True ## True 12.3.7.3 Operation 12.3.7.3.1 Logical Operator print( True and kosong1, kosong1 and True) ## nan True print( True or kosong1, False or kosong1) ## True nan 12.3.7.3.2 Comparing Compare nan with anything results in False, including itself. print(kosong1 &gt; 0, kosong1==0, kosong1&lt;0, kosong1 ==1, kosong1==kosong1, kosong1==False, kosong1==True) ## False False False False False False False 12.3.7.3.3 Casting nan is numpy floating value. It is not a zero value, therefore casting to boolean returns True. bool(kosong1) ## True 12.4 Numpy Array 12.4.1 Concept Structure - NumPy provides an N-dimensional array type, the ndarray - ndarray is homogenous: every item takes up the same size block of memory, and all blocks - For each ndarray, there is a seperate dtype object, which describe ndarray data type - An item extracted from an array, e.g., by indexing, is represented by a Python object whose type is one of the array scalar types built in NumPy. The array scalars allow easy manipulation of also more complicated arrangements of data. 12.4.2 Constructor By default, numpy.array autodetect its data types based on most common denominator 12.4.2.1 dType: int, float Notice example below auto detected as int32 data type x = np.array( (1,2,3,4,5) ) print(x) ## [1 2 3 4 5] print(&#39;Type: &#39;, type(x)) ## Type: &lt;class &#39;numpy.ndarray&#39;&gt; print(&#39;dType:&#39;, x.dtype) ## dType: int32 Notice example below auto detected as float64 data type x = np.array( (1,2,3,4.5,5) ) print(x) ## [1. 2. 3. 4.5 5. ] print(&#39;Type: &#39;, type(x)) ## Type: &lt;class &#39;numpy.ndarray&#39;&gt; print(&#39;dType:&#39;, x.dtype) ## dType: float64 You can specify dtype to specify desired data types. NumPy will auto convert the data into specifeid types. Observe below that we convert float into integer x = np.array( (1,2,3,4.5,5), dtype=&#39;int&#39; ) print(x) ## [1 2 3 4 5] print(&#39;Type: &#39;, type(x)) ## Type: &lt;class &#39;numpy.ndarray&#39;&gt; print(&#39;dType:&#39;, x.dtype) ## dType: int32 12.4.2.2 dType: datetime64 Specify dtype is necessary to ensure output is datetime type. If not, output is generic object type. From str x = np.array([&#39;2007-07-13&#39;, &#39;2006-01-13&#39;, &#39;2010-08-13&#39;], dtype=&#39;datetime64&#39;) print(x) ## [&#39;2007-07-13&#39; &#39;2006-01-13&#39; &#39;2010-08-13&#39;] print(&#39;Type: &#39;, type(x)) ## Type: &lt;class &#39;numpy.ndarray&#39;&gt; print(&#39;dType:&#39;, x.dtype) ## dType: datetime64[D] From datetime x = np.array([datetime(2019,1,12), datetime(2019,1,14),datetime(2019,3,3)], dtype=&#39;datetime64&#39;) print(x) ## [&#39;2019-01-12T00:00:00.000000&#39; &#39;2019-01-14T00:00:00.000000&#39; ## &#39;2019-03-03T00:00:00.000000&#39;] print(&#39;Type: &#39;, type(x)) ## Type: &lt;class &#39;numpy.ndarray&#39;&gt; print(&#39;dType:&#39;, x.dtype) ## dType: datetime64[us] print(&#39;\\nElement Type:&#39;,type(x[1])) ## ## Element Type: &lt;class &#39;numpy.datetime64&#39;&gt; 12.4.2.3 2D Array x = np.array([range(10),np.arange(10)]) x ## array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], ## [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]) 12.4.3 Dimensions 12.4.3.1 Differentiating Dimensions 1-D array is array of single list 2-D array is array made of list containing lists (each row is a list) 2-D single row array is array with list containing just one list 12.4.3.2 1-D Array Observe that the shape of the array is (5,). It seems like an array with 5 rows, empty columns ! What it really means is 5 items single dimension. arr = np.array(range(5)) print (arr) ## [0 1 2 3 4] print (arr.shape) ## (5,) print (arr.ndim) ## 1 12.4.3.3 2-D Array arr = np.array([range(5),range(5,10),range(10,15)]) print (arr) ## [[ 0 1 2 3 4] ## [ 5 6 7 8 9] ## [10 11 12 13 14]] print (arr.shape) ## (3, 5) print (arr.ndim) ## 2 12.4.3.4 2-D Array - Single Row arr = np.array([range(5)]) print (arr) ## [[0 1 2 3 4]] print (arr.shape) ## (1, 5) print (arr.ndim) ## 2 12.4.3.5 2-D Array : Single Column Using array slicing method with newaxis at COLUMN, will turn 1D array into 2D of single column arr = np.arange(5)[:, np.newaxis] print (arr) ## [[0] ## [1] ## [2] ## [3] ## [4]] print (arr.shape) ## (5, 1) print (arr.ndim) ## 2 Using array slicing method with newaxis at ROW, will turn 1D array into 2D of single row arr = np.arange(5)[np.newaxis,:] print (arr) ## [[0 1 2 3 4]] print (arr.shape) ## (1, 5) print (arr.ndim) ## 2 12.4.4 Class Method 12.4.4.1 arange() Generate array with a sequence of numbers np.arange(10) ## array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 12.4.4.2 ones() np.ones(10) # One dimension, default is float ## array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) np.ones((2,5),&#39;int&#39;) #Two dimensions ## array([[1, 1, 1, 1, 1], ## [1, 1, 1, 1, 1]]) 12.4.4.3 zeros() np.zeros( 10 ) # One dimension, default is float ## array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) np.zeros((2,5),&#39;int&#39;) # 2 rows, 5 columns of ZERO ## array([[0, 0, 0, 0, 0], ## [0, 0, 0, 0, 0]]) 12.4.4.4 where() On 1D array numpy.where() returns the items matching the criteria ar1 = np.array(range(10)) print( ar1 ) ## [0 1 2 3 4 5 6 7 8 9] print( np.where(ar1&gt;5) ) ## (array([6, 7, 8, 9], dtype=int64),) On 2D array, where() return array of row index and col index for matching elements ar = np.array([(1,2,3,4,5),(11,12,13,14,15),(21,22,23,24,25)]) print (&#39;Data : \\n&#39;, ar) ## Data : ## [[ 1 2 3 4 5] ## [11 12 13 14 15] ## [21 22 23 24 25]] np.where(ar&gt;13) ## (array([1, 1, 2, 2, 2, 2, 2], dtype=int64), array([3, 4, 0, 1, 2, 3, 4], dtype=int64)) 12.4.4.5 Logical Methods numpy.logical_or Perform or operation on two boolean array, generate new resulting boolean arrays ar = np.arange(10) print( ar==3 ) # boolean array 1 ## [False False False True False False False False False False] print( ar==6 ) # boolean array 2 ## [False False False False False False True False False False] print( np.logical_or(ar==3,ar==6 ) ) # resulting boolean ## [False False False True False False True False False False] numpy.logical_and Perform and operation on two boolean array, generate new resulting boolean arrays ar = np.arange(10) print( ar==3 ) # boolean array 1 ## [False False False True False False False False False False] print( ar==6 ) # boolean array 2 ## [False False False False False False True False False False] print( np.logical_and(ar==3,ar==6 ) ) # resulting boolean ## [False False False False False False False False False False] 12.4.5 Instance Method 12.4.5.1 astype() conversion Convert to from datetime64 to datetime ar1 = np.array([&#39;2007-07-13&#39;, &#39;2006-01-13&#39;, &#39;2010-08-13&#39;], dtype=&#39;datetime64&#39;) print( type(ar1) ) ## a numpy array ## &lt;class &#39;numpy.ndarray&#39;&gt; print( ar1.dtype ) ## dtype is a numpy data type ## datetime64[D] After convert to datetime (non-numpy object, the dtype becomes generic ‘object’. ar2 = ar1.astype(datetime) print( type(ar2) ) ## still a numpy array ## &lt;class &#39;numpy.ndarray&#39;&gt; print( ar2.dtype ) ## dtype becomes generic &#39;object&#39; ## object 12.4.5.2 reshape() reshape ( row numbers, col numbers ) Sample Data a = np.array([range(5), range(10,15), range(20,25), range(30,35)]) a ## array([[ 0, 1, 2, 3, 4], ## [10, 11, 12, 13, 14], ## [20, 21, 22, 23, 24], ## [30, 31, 32, 33, 34]]) Resphepe 1-Dim to 2-Dim np.arange(12) # 1-D Array ## array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) np.arange(12).reshape(3,4) # 2-D Array ## array([[ 0, 1, 2, 3], ## [ 4, 5, 6, 7], ## [ 8, 9, 10, 11]]) Respahe 2-Dim to 2-Dim np.array([range(5), range(10,15)]) # 2-D Array ## array([[ 0, 1, 2, 3, 4], ## [10, 11, 12, 13, 14]]) np.array([range(5), range(10,15)]).reshape(5,2) # 2-D Array ## array([[ 0, 1], ## [ 2, 3], ## [ 4, 10], ## [11, 12], ## [13, 14]]) Reshape 2-Dimension to 2-Dim (of single row) - Change 2x10 to 1x10 - Observe [[ ]], and the number of dimension is stll 2, don’t be fooled np.array( [range(0,5), range(5,10)]) # 2-D Array ## array([[0, 1, 2, 3, 4], ## [5, 6, 7, 8, 9]]) np.array( [range(0,5), range(5,10)]).reshape(1,10) # 2-D Array ## array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]) Reshape 1-Dim Array to 2-Dim Array (single column) np.arange(8) ## array([0, 1, 2, 3, 4, 5, 6, 7]) np.arange(8).reshape(8,1) ## array([[0], ## [1], ## [2], ## [3], ## [4], ## [5], ## [6], ## [7]]) A better method, use newaxis, easier because no need to input row number as parameter np.arange(8)[:,np.newaxis] ## array([[0], ## [1], ## [2], ## [3], ## [4], ## [5], ## [6], ## [7]]) Reshape 1-Dim Array to 2-Dim Array (single row) np.arange(8) ## array([0, 1, 2, 3, 4, 5, 6, 7]) np.arange(8)[np.newaxis,:] ## array([[0, 1, 2, 3, 4, 5, 6, 7]]) 12.4.6 Element Selection 12.4.6.1 Sample Data x1 = np.array( (0,1,2,3,4,5,6,7,8)) x2 = np.array(( (1,2,3,4,5), (11,12,13,14,15), (21,22,23,24,25))) print(x1) ## [0 1 2 3 4 5 6 7 8] print(x2) ## [[ 1 2 3 4 5] ## [11 12 13 14 15] ## [21 22 23 24 25]] 12.4.6.2 1-Dimension All indexing starts from 0 (not 1) Choosing Single Element does not return array print( x1[0] ) ## first element ## 0 print( x1[-1] ) ## last element ## 8 print( x1[3] ) ## third element from start 3 ## 3 print( x1[-3] ) ## third element from end ## 6 Selecting multiple elments return ndarray print( x1[:3] ) ## first 3 elements ## [0 1 2] print( x1[-3:]) ## last 3 elements ## [6 7 8] print( x1[3:] ) ## all except first 3 elements ## [3 4 5 6 7 8] print( x1[:-3] ) ## all except last 3 elements ## [0 1 2 3 4 5] print( x1[1:4] ) ## elemnt 1 to 4 (not including 4) ## [1 2 3] 12.4.6.3 2-Dimension Indexing with [ row_positoins, row_positions ], index starts with 0 x[1:3, 1:4] # row 1 to 2 column 1 to 3 ## array([[1, 2, 3]]) 12.4.7 Attributes 12.4.7.1 dtype ndarray contain a property called dtype, whcih tell us the type of underlying items a = np.array( (1,2,3,4,5), dtype=&#39;float&#39; ) a.dtype ## dtype(&#39;float64&#39;) print(a.dtype) ## float64 print( type(a[1])) ## &lt;class &#39;numpy.float64&#39;&gt; 12.4.7.2 dim dim returns the number of dimensions of the NumPy array. Example below shows 2-D array x = np.array(( (1,2,3,4,5), (11,12,13,14,15), (21,22,23,24,25))) x.ndim ## 2 12.4.7.3 shape shape return a type of (rows, cols) x = np.array(( (1,2,3,4,5), (11,12,13,14,15), (21,22,23,24,25))) x.shape ## (3, 5) np.identity(5) ## array([[1., 0., 0., 0., 0.], ## [0., 1., 0., 0., 0.], ## [0., 0., 1., 0., 0.], ## [0., 0., 0., 1., 0.], ## [0., 0., 0., 0., 1.]]) 12.4.8 Operations 12.4.8.1 Arithmetic Sample Date ar = np.arange(10) print( ar ) ## [0 1 2 3 4 5 6 7 8 9] * ar = np.arange(10) print (ar) ## [0 1 2 3 4 5 6 7 8 9] print (ar*2) ## [ 0 2 4 6 8 10 12 14 16 18] **+ and -** ar = np.arange(10) print (ar+2) ## [ 2 3 4 5 6 7 8 9 10 11] print (ar-2) ## [-2 -1 0 1 2 3 4 5 6 7] 12.4.8.2 Comparison Sample Data ar = np.arange(10) print( ar ) ## [0 1 2 3 4 5 6 7 8 9] == print( ar==3 ) ## [False False False True False False False False False False] &gt;, &gt;=, &lt;, &lt;= print( ar&gt;3 ) ## [False False False False True True True True True True] print( ar&lt;=3 ) ## [ True True True True False False False False False False] 12.5 Random Numbers 12.5.1 Uniform Distribution 12.5.1.1 Random Integer (with Replacement) randint() Return random integers from low (inclusive) to high (exclusive) np.random.randint( low ) # generate an integer, i, which i &lt; low np.random.randint( low, high ) # generate an integer, i, which low &lt;= i &lt; high np.random.randint( low, high, size=1) # generate an ndarray of integer, single dimension np.random.randint( low, high, size=(r,c)) # generate an ndarray of integer, two dimensions np.random.randint( 10 ) ## 3 np.random.randint( 10, 20 ) ## 19 np.random.randint( 10, high=20, size=5) # single dimension ## array([19, 16, 10, 10, 10]) np.random.randint( 10, 20, (3,5) ) # two dimensions ## array([[11, 18, 14, 12, 14], ## [12, 10, 12, 12, 19], ## [17, 12, 18, 11, 16]]) 12.5.1.2 Random Integer (with or without replacement) numpy.random .choice( a, size, replace=True) # sampling from a, # if a is integer, then it is assumed sampling from arange(a) # if a is an 1-D array, then sampling from this array np.random.choice(10,5, replace=False) # take 5 samples from 0:19, without replacement ## array([8, 6, 2, 4, 9]) np.random.choice( np.arange(10,20), 5, replace=False) ## array([14, 17, 16, 13, 19]) 12.5.1.3 Random Float randf() Generate float numbers in between 0.0 and 1.0 np.random.ranf(size=None) np.random.ranf(4) ## array([0.98266534, 0.46924883, 0.88391757, 0.89123032]) uniform() Return random float from low (inclusive) to high (exclusive) np.random.uniform( low ) # generate an float, i, which f &lt; low np.random.uniform( low, high ) # generate an float, i, which low &lt;= f &lt; high np.random.uniform( low, high, size=1) # generate an array of float, single dimension np.random.uniform( low, high, size=(r,c)) # generate an array of float, two dimensions np.random.uniform( 2 ) ## 1.530933816456104 np.random.uniform( 2,5, size=(4,4) ) ## array([[2.69783345, 2.53942131, 4.10827472, 4.92201354], ## [2.75878786, 3.21861632, 2.14665348, 4.79638818], ## [3.62795852, 4.56519014, 2.22101306, 4.15983322], ## [4.3244035 , 3.19793701, 4.35573142, 3.91362716]]) 12.5.2 Normal Distribution numpy. random.randn (n_items) # 1-D standard normal (mean=0, stdev=1) numpy. random.randn (nrows, ncols) # 2-D standard normal (mean=0, stdev=1) numpy. random.standard_normal( size=None ) # default to mean = 0, stdev = 1, non-configurable numpy. random.normal ( loc=0, scale=1, size=None) # loc = mean, scale = stdev, size = dimension 12.5.2.1 Standard Normal Distribution Generate random normal numbers with gaussion distribution (mean=0, stdev=1) One Dimension np.random.standard_normal(3) ## array([-1.29943222, 0.57961638, -0.5190715 ]) np.random.randn(3) ## array([ 0.07804682, 1.85163805, -0.46432972]) Two Dimensions np.random.randn(2,4) ## array([[ 0.42562464, -0.89907701, 1.25026869, -1.70894375], ## [-2.70567344, 2.01463839, -0.01626053, 0.74143314]]) np.random.standard_normal((2,4)) ## array([[ 0.65459399, -1.34412728, -0.10232435, 0.77122548], ## [-0.15882864, -0.91464491, 1.44577544, 1.37888319]]) Observe: randn(), standard_normal() and normal() are able to generate standard normal numbers np.random.seed(15) print (np.random.randn(5)) ## [-0.31232848 0.33928471 -0.15590853 -0.50178967 0.23556889] np.random.seed(15) print (np.random.normal ( size = 5 )) # stdev and mean not specified, default to standard normal ## [-0.31232848 0.33928471 -0.15590853 -0.50178967 0.23556889] np.random.seed(15) print (np.random.standard_normal (size=5)) ## [-0.31232848 0.33928471 -0.15590853 -0.50178967 0.23556889] 12.5.2.2 Normal Distribution (Non-Standard) np.random.seed(125) np.random.normal( loc = 12, scale=1.25, size=(3,3)) ## array([[11.12645382, 12.01327885, 10.81651695], ## [12.41091248, 12.39383072, 11.49647195], ## [ 8.70837035, 12.25246312, 11.49084235]]) 12.5.2.3 Linear Spacing numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None) # endpoint: If True, stop is the last sample, otherwise it is not included Include Endpoint Step = Gap divide by (number of elements minus 1) (2/(10-1)) np.linspace(1,3,10) #default endpont=True ## array([1. , 1.22222222, 1.44444444, 1.66666667, 1.88888889, ## 2.11111111, 2.33333333, 2.55555556, 2.77777778, 3. ]) Does Not Include Endpoint Step = Gap divide by (number of elements minus 1) (2/(101)) np.linspace(1,3,10,endpoint=False) ## array([1. , 1.2, 1.4, 1.6, 1.8, 2. , 2.2, 2.4, 2.6, 2.8]) 12.6 Sampling (Integer) random.choice( a, size=None, replace=True, p=None) # a=integer, return &lt;size&gt; integers &lt; a random.choice( a, size=None, replace=True, p=None) # a=array-like, return &lt;size&gt; integers picked from list a np.random.choice (100, size=10) ## array([58, 0, 84, 50, 89, 32, 87, 30, 66, 92]) np.random.choice( [1,3,5,7,9,11,13,15,17,19,21,23], size=10, replace=False) ## array([ 5, 1, 23, 17, 3, 13, 15, 9, 21, 7]) 12.7 NaN : Missing Numerical Data You should be aware that NaN is a bit like a data virus?it infects any other object it touches t = np.array([1, np.nan, 3, 4]) t.dtype ## dtype(&#39;float64&#39;) Regardless of the operation, the result of arithmetic with NaN will be another NaN 1 + np.nan ## nan t.sum(), t.mean(), t.max() ## (nan, nan, nan) np.nansum(t), np.nanmean(t), np.nanmax(t) ## (8.0, 2.6666666666666665, 4.0) "],
["pandas-1.html", "13 pandas 13.1 Modules Import 13.2 Pandas Objects 13.3 Class Method 13.4 class: Timestamp 13.5 class: DateTimeIndex 13.6 class: Series 13.7 class: DataFrame 13.8 class: MultiIndex 13.9 class: Categories 13.10 Dummies 13.11 GroupBy 13.12 Fundamental Analysis 13.13 Missing Data", " 13 pandas 13.1 Modules Import import pandas as pd ## Other Libraries import numpy as np import datetime as dt from datetime import datetime from datetime import date 13.2 Pandas Objects 13.2.1 Pandas Data Types pandas.Timestamp pandas.Timedelta pandas.Period pandas.Interval pandas.DateTimeIndex 13.2.2 Pandas Data Structure Type Dimension Size Value Constructor Series 1 Immutable Mutable pandas.DataFrame( data, index, dtype, copy) DataFrame 2 Mutable Mutable pandas.DataFrame( data, index, columns, dtype, copy) Panel 3 Mutable Mutable data can be ndarray, list, constants index must be unique and same length as data. Can be integer or string dtype if none, it will be inferred copy copy data. Default false 13.3 Class Method 13.3.1 Creating Timestamp Objects Pandas to_datetime() can: - Convert list of dates to DateTimeIndex - Convert list of dates to Series of Timestamps - Convert single date into Timestamp Object . Source can be string, date, datetime object 13.3.1.1 From List to DateTimeIndex dti = pd.to_datetime([&#39;2011-01-03&#39;, # from string date(2018,4,13), # from date datetime(2018,3,1,7,30)] # from datetime ) print( dti, &#39;\\nObject Type: &#39;, type(dti), &#39;\\nObject dtype: &#39;, dti.dtype, &#39;\\nElement Type: &#39;, type(dti[1])) ## DatetimeIndex([&#39;2011-01-03 00:00:00&#39;, &#39;2018-04-13 00:00:00&#39;, &#39;2018-03-01 07:30:00&#39;], dtype=&#39;datetime64[ns]&#39;, freq=None) ## Object Type: &lt;class &#39;pandas.core.indexes.datetimes.DatetimeIndex&#39;&gt; ## Object dtype: datetime64[ns] ## Element Type: &lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; 13.3.1.2 From List to Series of Timestamps sdt = pd.to_datetime(pd.Series([&#39;2011-01-03&#39;, # from string date(2018,4,13), # from date datetime(2018,3,1,7,30)]# from datetime )) print(sdt, &#39;\\nObject Type: &#39;,type(sdt), &#39;\\nObject dtype: &#39;, sdt.dtype, &#39;\\nElement Type: &#39;,type(sdt[1])) ## 0 2011-01-03 00:00:00 ## 1 2018-04-13 00:00:00 ## 2 2018-03-01 07:30:00 ## dtype: datetime64[ns] ## Object Type: &lt;class &#39;pandas.core.series.Series&#39;&gt; ## Object dtype: datetime64[ns] ## Element Type: &lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; 13.3.1.3 From Scalar to Timestamp print( pd.to_datetime(&#39;2011-01-03&#39;), &#39;\\n&#39;, pd.to_datetime(date(2011,1,3)), &#39;\\n&#39;, pd.to_datetime(datetime(2011,1,3,5,30)), &#39;\\n&#39;, &#39;\\nElement Type: &#39;, type(pd.to_datetime(datetime(2011,1,3,5,30)))) ## 2011-01-03 00:00:00 ## 2011-01-03 00:00:00 ## 2011-01-03 05:30:00 ## ## Element Type: &lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; 13.3.2 Generate Timestamp Sequence The function date_range() return DateTimeIndex object. Use Series() to convert into Series if desired. 13.3.2.1 Hourly If start time not specified, default to 00:00:00. If start time specified, it will be honored on all subsequent Timestamp elements. Specify start and end, sequence will automatically distribute Timestamp according to frequency. print( pd.date_range(&#39;2018-01-01&#39;, periods=3, freq=&#39;H&#39;), pd.date_range(datetime(2018,1,1,12,30), periods=3, freq=&#39;H&#39;), pd.date_range(start=&#39;2018-01-03-1230&#39;, end=&#39;2018-01-03-18:30&#39;, freq=&#39;H&#39;)) ## DatetimeIndex([&#39;2018-01-01 00:00:00&#39;, &#39;2018-01-01 01:00:00&#39;, &#39;2018-01-01 02:00:00&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;H&#39;) DatetimeIndex([&#39;2018-01-01 12:30:00&#39;, &#39;2018-01-01 13:30:00&#39;, &#39;2018-01-01 14:30:00&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;H&#39;) DatetimeIndex([&#39;2018-01-03 12:30:00&#39;, &#39;2018-01-03 13:30:00&#39;, &#39;2018-01-03 14:30:00&#39;, ## &#39;2018-01-03 15:30:00&#39;, &#39;2018-01-03 16:30:00&#39;, &#39;2018-01-03 17:30:00&#39;, ## &#39;2018-01-03 18:30:00&#39;], ## dtype=&#39;datetime64[ns]&#39;, freq=&#39;H&#39;) 13.3.2.2 Daily When the frequency is Day and time is not specified, output is date distributed. When time is specified, output will honor the time. print( pd.date_range(date(2018,1,2), periods=3, freq=&#39;D&#39;), pd.date_range(&#39;2018-01-01-1230&#39;, periods=4, freq=&#39;D&#39;)) ## DatetimeIndex([&#39;2018-01-02&#39;, &#39;2018-01-03&#39;, &#39;2018-01-04&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) DatetimeIndex([&#39;2018-01-01 12:30:00&#39;, &#39;2018-01-02 12:30:00&#39;, &#39;2018-01-03 12:30:00&#39;, ## &#39;2018-01-04 12:30:00&#39;], ## dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) 13.3.2.3 First Day Of Month Use freq=MS, M stands for montly, S stand for Start. If the day specified, the sequence start from first day of following month. print( pd.date_range(&#39;2018-01&#39;, periods=4, freq=&#39;MS&#39;), pd.date_range(&#39;2018-01-09&#39;, periods=4, freq=&#39;MS&#39;), pd.date_range(&#39;2018-01-09 12:30:00&#39;, periods=4, freq=&#39;MS&#39;) ) ## DatetimeIndex([&#39;2018-01-01&#39;, &#39;2018-02-01&#39;, &#39;2018-03-01&#39;, &#39;2018-04-01&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;MS&#39;) DatetimeIndex([&#39;2018-02-01&#39;, &#39;2018-03-01&#39;, &#39;2018-04-01&#39;, &#39;2018-05-01&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;MS&#39;) DatetimeIndex([&#39;2018-02-01 12:30:00&#39;, &#39;2018-03-01 12:30:00&#39;, &#39;2018-04-01 12:30:00&#39;, ## &#39;2018-05-01 12:30:00&#39;], ## dtype=&#39;datetime64[ns]&#39;, freq=&#39;MS&#39;) 13.3.2.4 Last Day of Month Sequence always starts from the end of the specified month. print( pd.date_range(&#39;2018-01&#39;, periods=4, freq=&#39;M&#39;), pd.date_range(&#39;2018-01-09&#39;, periods=4, freq=&#39;M&#39;), pd.date_range(&#39;2018-01-09 12:30:00&#39;, periods=4, freq=&#39;M&#39;)) ## DatetimeIndex([&#39;2018-01-31&#39;, &#39;2018-02-28&#39;, &#39;2018-03-31&#39;, &#39;2018-04-30&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;M&#39;) DatetimeIndex([&#39;2018-01-31&#39;, &#39;2018-02-28&#39;, &#39;2018-03-31&#39;, &#39;2018-04-30&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;M&#39;) DatetimeIndex([&#39;2018-01-31 12:30:00&#39;, &#39;2018-02-28 12:30:00&#39;, &#39;2018-03-31 12:30:00&#39;, ## &#39;2018-04-30 12:30:00&#39;], ## dtype=&#39;datetime64[ns]&#39;, freq=&#39;M&#39;) 13.3.3 Frequency Table (crosstab) crosstab returns Dataframe Object crosstab( index = &lt;SeriesObj&gt;, columns = &lt;new_colName&gt; ) # one dimension table crosstab( index = &lt;SeriesObj&gt;, columns = &lt;SeriesObj&gt; ) # two dimension table crosstab( index = &lt;SeriesObj&gt;, columns = [&lt;SeriesObj1&gt;, &lt;SeriesObj2&gt;] ) # multi dimension table crosstab( index = &lt;SeriesObj&gt;, columns = &lt;SeriesObj&gt;, margines=True ) # add column and row margins 13.3.3.1 Sample Data n = 200 comp = [&#39;C&#39; + i for i in np.random.randint( 1,4, size = n).astype(str)] # 3x Company dept = [&#39;D&#39; + i for i in np.random.randint( 1,6, size = n).astype(str)] # 5x Department grp = [&#39;G&#39; + i for i in np.random.randint( 1,3, size = n).astype(str)] # 2x Groups value1 = np.random.normal( loc=50 , scale=5 , size = n) value2 = np.random.normal( loc=20 , scale=3 , size = n) value3 = np.random.normal( loc=5 , scale=30 , size = n) mydf = pd.DataFrame({ &#39;comp&#39;:comp, &#39;dept&#39;:dept, &#39;grp&#39;: grp, &#39;value1&#39;:value1, &#39;value2&#39;:value2, &#39;value3&#39;:value3 }) mydf.head() ## comp dept grp value1 value2 value3 ## 0 C1 D2 G2 63.434839 24.762085 14.037684 ## 1 C3 D2 G1 48.319613 18.965650 -30.215550 ## 2 C2 D1 G1 52.776620 21.003838 49.610150 ## 3 C3 D5 G1 44.648475 23.730746 20.963278 ## 4 C2 D5 G2 45.309311 16.353753 15.269688 13.3.3.2 One DimensionTable ## Frequency Countn For Company, Department print( pd.crosstab(index=mydf.comp, columns=&#39;counter&#39;),&#39;\\n\\n&#39;, pd.crosstab(index=mydf.dept, columns=&#39;counter&#39;)) ## col_0 counter ## comp ## C1 76 ## C2 57 ## C3 67 ## ## col_0 counter ## dept ## D1 41 ## D2 51 ## D3 35 ## D4 32 ## D5 41 13.3.3.3 Two Dimension Table pd.crosstab(index=mydf.comp, columns=mydf.dept) ## dept D1 D2 D3 D4 D5 ## comp ## C1 19 20 12 12 13 ## C2 11 17 10 7 12 ## C3 11 14 13 13 16 13.3.3.4 Higher Dimension Table Crosstab header is multi-levels index when more than one column specified. tb = pd.crosstab(index=mydf.comp, columns=[mydf.dept, mydf.grp]) print( tb, &#39;\\n\\n&#39;, tb.columns ) ## dept D1 D2 D3 D4 D5 ## grp G1 G2 G1 G2 G1 G2 G1 G2 G1 G2 ## comp ## C1 10 9 9 11 3 9 4 8 9 4 ## C2 4 7 7 10 4 6 5 2 2 10 ## C3 3 8 5 9 3 10 6 7 9 7 ## ## MultiIndex([(&#39;D1&#39;, &#39;G1&#39;), ## (&#39;D1&#39;, &#39;G2&#39;), ## (&#39;D2&#39;, &#39;G1&#39;), ## (&#39;D2&#39;, &#39;G2&#39;), ## (&#39;D3&#39;, &#39;G1&#39;), ## (&#39;D3&#39;, &#39;G2&#39;), ## (&#39;D4&#39;, &#39;G1&#39;), ## (&#39;D4&#39;, &#39;G2&#39;), ## (&#39;D5&#39;, &#39;G1&#39;), ## (&#39;D5&#39;, &#39;G2&#39;)], ## names=[&#39;dept&#39;, &#39;grp&#39;]) Select sub-dataframe using multi-level referencing. print( &#39;Under D2:\\n&#39;, tb[&#39;D2&#39;], &#39;\\n\\n&#39;, &#39;Under D2-G2:\\n&#39;,tb[&#39;D2&#39;,&#39;G1&#39;]) ## Under D2: ## grp G1 G2 ## comp ## C1 9 11 ## C2 7 10 ## C3 5 9 ## ## Under D2-G2: ## comp ## C1 9 ## C2 7 ## C3 5 ## Name: (D2, G1), dtype: int64 13.3.3.5 Getting Margin Extend the crosstab with ‘margin=True’ to have sum of rows/columns, presented in new column/row named ‘All’. tb = pd.crosstab(index=mydf.dept, columns=mydf.grp, margins=True) tb ## grp G1 G2 All ## dept ## D1 17 24 41 ## D2 21 30 51 ## D3 10 25 35 ## D4 15 17 32 ## D5 20 21 41 ## All 83 117 200 print( &#39;Row Sums: \\n&#39;, tb.loc[:,&#39;All&#39;], &#39;\\n\\nColumn Sums:\\n&#39;, tb.loc[&#39;All&#39;]) ## Row Sums: ## dept ## D1 41 ## D2 51 ## D3 35 ## D4 32 ## D5 41 ## All 200 ## Name: All, dtype: int64 ## ## Column Sums: ## grp ## G1 83 ## G2 117 ## All 200 ## Name: All, dtype: int64 13.3.3.6 Getting Proportion Use matrix operation divide each row with its respective column sum. tb/tb.loc[&#39;All&#39;] ## grp G1 G2 All ## dept ## D1 0.204819 0.205128 0.205 ## D2 0.253012 0.256410 0.255 ## D3 0.120482 0.213675 0.175 ## D4 0.180723 0.145299 0.160 ## D5 0.240964 0.179487 0.205 ## All 1.000000 1.000000 1.000 13.3.4 Concatination 13.3.4.1 Sample Data s1 = pd.Series([&#39;A1&#39;,&#39;A2&#39;,&#39;A3&#39;,&#39;A4&#39;]) s2 = pd.Series([&#39;B1&#39;,&#39;B2&#39;,&#39;B3&#39;,&#39;B4&#39;], name=&#39;B&#39;) s3 = pd.Series([&#39;C1&#39;,&#39;C2&#39;,&#39;C3&#39;,&#39;C4&#39;], name=&#39;C&#39;) df ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;df&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; 13.3.4.2 Column-Wise Combinining Multiple Series Into A New DataFrame - Added series will have 0,1,2,… column names (if Series are not named originally) - None series will be ignored - axis=1 means column-wise pd.concat([s1,s2,s3, None], axis=1) ## 0 B C ## 0 A1 B1 C1 ## 1 A2 B2 C2 ## 2 A3 B3 C3 ## 3 A4 B4 C4 Add Multiple Series Into An Existing DataFrame - No change to original data frame column name - Added columns from series will have 0,1,2,3,.. column name df = pd.DataFrame({ &#39;A&#39;: s1, &#39;B&#39;: s2}) pd.concat([df,s3,s1, None],axis=1) ## A B C 0 ## 0 A1 B1 C1 A1 ## 1 A2 B2 C2 A2 ## 2 A3 B3 C3 A3 ## 3 A4 B4 C4 A4 13.3.4.3 Row-Wise 13.3.5 External Data 13.3.5.1 html_table Parser This method require html5lib library. - Read the web page, create a list: which contain one or more dataframes that maps to each html table found - Scrap all detectable html tables - Auto detect column header - Auto create index using number starting from 0 read_html(url) # return list of dataframe(s) that maps to web table(s) structure df_list = pd.read_html(&#39;https://www.malaysiastock.biz/Listed-Companies.aspx?type=S&amp;s1=18&#39;) ## read all tables df = df_list[6] ## get the specific table print (&#39;Total Table(s) Found : &#39;, len(df_list), &#39;\\n&#39;, &#39;First Table Found: &#39;,df) ## Total Table(s) Found : 10 ## First Table Found: Company Shariah \\ ## 0 AMEDIA (0159)MAINASIA MEDIA GROUP BERHAD NaN ## 1 AMTEL (7031)MAINAMTEL HOLDINGS BERHAD NaN ## 2 ASTRO (6399)MAINASTRO MALAYSIA HOLDINGS BERHAD NaN ## 3 AXIATA (6888)MAINAXIATA GROUP BERHAD NaN ## 4 BINACOM (0195)ACEBINASAT COMMUNICATIONS BERHAD NaN ## .. ... ... ## 28 SRIDGE (0129)ACESILVER RIDGE HOLDINGS BHD NaN ## 29 STAR (6084)MAINSTAR MEDIA GROUP BERHAD NaN ## 30 TIMECOM (5031)MAINTIME DOTCOM BERHAD NaN ## 31 TM (4863)MAINTELEKOM MALAYSIA BERHAD NaN ## 32 XOX (0165)ACEXOX BHD NaN ## ## Sector Market Cap Last Price PE DY ROE ## 0 Media 11.97m 0.05 - 0.00 - ## 1 Telecommunications Equipment 28.45m 0.53 5.69 0.00 9.97 ## 2 Media 5.110b 0.98 8.05 9.18 76.06 ## 3 Telecommunications Service Providers 35.740b 3.90 26.17 2.44 8.42 ## 4 Telecommunications Service Providers 83.28m 0.32 22.66 3.17 4.63 ## .. ... ... ... ... ... ... ## 28 Telecommunications Equipment 51.97m 0.36 - 0.00 -151.00 ## 29 Media 217.88m 0.30 38.31 10.17 0.69 ## 30 Telecommunications Service Providers 5.475b 9.35 17.43 2.20 11.34 ## 31 Telecommunications Service Providers 12.653b 3.36 20.00 2.98 8.60 ## 32 Telecommunications Service Providers 21.85m 0.02 - 0.00 - ## ## [33 rows x 8 columns] 13.3.5.2 CSV Writing Syntax DataFrame.to_csv( path_or_buf=None, ## if not provided, result is returned as string sep=&#39;, &#39;, na_rep=&#39;&#39;, float_format=None, columns=None, ## list of columns name to write, if not provided, all columns are written header=True, ## write out column names index=True, ## write row label index_label=None, mode=&#39;w&#39;, encoding=None, ## if not provided, default to &#39;utf-8&#39; quoting=None, quotechar=&#39;&quot;&#39;, line_terminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal=&#39;.&#39;) Example below shows column value containing different special character. Note that pandas handles these very well by default. mydf = pd.DataFrame({&#39;Id&#39;:[10,20,30,40], &#39;Name&#39;: [&#39;Aaa&#39;,&#39;Bbb&#39;,&#39;Ccc&#39;,&#39;Ddd&#39;], &#39;Funny&#39;: [&quot;world&#39;s most \\clever&quot;, &quot;Bloody, damn, good&quot;, &quot;many\\nmany\\nline&quot;, &#39;Quoting &quot;is&quot; tough&#39;]}) mydf.set_index(&#39;Id&#39;, inplace=True) mydf.to_csv(&#39;data/csv_test.csv&#39;, index=True) mydf ## Name Funny ## Id ## 10 Aaa world&#39;s most \\clever ## 20 Bbb Bloody, damn, good ## 30 Ccc many\\nmany\\nline ## 40 Ddd Quoting &quot;is&quot; tough This is the file saved system(&#39;more data\\\\csv_test.csv&#39;) All content retained when reading back by Pandas pd.read_csv(&#39;data/csv_test.csv&#39;, index_col=&#39;Id&#39;) ## Name Funny ## Id ## 10 Aaa world&#39;s most \\clever ## 20 Bbb Bloody, damn, good ## 30 Ccc many\\nmany\\nline ## 40 Ddd Quoting &quot;is&quot; tough 13.3.5.3 CSV Reading Syntax pandas.read_csv( &#39;url or filePath&#39;, # path to file or url encoding = &#39;utf_8&#39;, # optional: default is &#39;utf_8&#39; index_col = [&#39;colName1&#39;, ...], # optional: specify one or more index column parse_dates = [&#39;dateCol1&#39;, ...], # optional: specify multiple string column to convert to date na_values = [&#39;.&#39;,&#39;na&#39;,&#39;NA&#39;,&#39;N/A&#39;], # optional: values that is considered NA names = [&#39;newColName1&#39;, ... ], # optional: overwrite column names thousands = &#39;.&#39;, # optional: thousand seperator symbol nrows = n, # optional: load only first n rows skiprows = 0, # optional: don&#39;t load first n rows parse_dates = False, # List of date column names infer_datetime_format = False # automatically parse dates ) Refer to full codec Python Codec. Default Import index is sequence of integer 0,1,2… only two data types detection; number (float64/int64) and string (object) date is not parsed, hence stayed as string goo = pd.read_csv(&#39;data/goog.csv&#39;, encoding=&#39;utf_8&#39;) print(goo.head(), &#39;\\n\\n&#39;, goo.info()) ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 61 entries, 0 to 60 ## Data columns (total 6 columns): ## Date 61 non-null object ## Open 61 non-null float64 ## High 61 non-null float64 ## Low 61 non-null float64 ## Close 61 non-null float64 ## Volume 61 non-null int64 ## dtypes: float64(4), int64(1), object(1) ## memory usage: 3.0+ KB ## Date Open High Low Close Volume ## 0 12/19/2016 790.219971 797.659973 786.270020 794.200012 1225900 ## 1 12/20/2016 796.760010 798.650024 793.270020 796.419983 925100 ## 2 12/21/2016 795.840027 796.676025 787.099976 794.559998 1208700 ## 3 12/22/2016 792.359985 793.320007 788.580017 791.260010 969100 ## 4 12/23/2016 790.900024 792.739990 787.280029 789.909973 623400 ## ## None Specify Data Types To customize the data type, use dtype parameter with a dict of definition. d_types = {&#39;Volume&#39;: str} pd.read_csv(&#39;data/goog.csv&#39;, dtype=d_types).info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 61 entries, 0 to 60 ## Data columns (total 6 columns): ## Date 61 non-null object ## Open 61 non-null float64 ## High 61 non-null float64 ## Low 61 non-null float64 ## Close 61 non-null float64 ## Volume 61 non-null object ## dtypes: float64(4), object(2) ## memory usage: 3.0+ KB Parse Datetime You can specify multiple date-alike column for parsing pd.read_csv(&#39;data/goog.csv&#39;, parse_dates=[&#39;Date&#39;]).info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 61 entries, 0 to 60 ## Data columns (total 6 columns): ## Date 61 non-null datetime64[ns] ## Open 61 non-null float64 ## High 61 non-null float64 ## Low 61 non-null float64 ## Close 61 non-null float64 ## Volume 61 non-null int64 ## dtypes: datetime64[ns](1), float64(4), int64(1) ## memory usage: 3.0 KB Parse Datetime, Then Set as Index - Specify names of date column in parse_dates= - When date is set as index, the type is DateTimeIndex goo3 = pd.read_csv(&#39;data/goog.csv&#39;,index_col=&#39;Date&#39;, parse_dates=[&#39;Date&#39;]) goo3.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## DatetimeIndex: 61 entries, 2016-12-19 to 2017-03-17 ## Data columns (total 5 columns): ## Open 61 non-null float64 ## High 61 non-null float64 ## Low 61 non-null float64 ## Close 61 non-null float64 ## Volume 61 non-null int64 ## dtypes: float64(4), int64(1) ## memory usage: 2.9 KB 13.3.6 Inspection 13.3.6.1 Structure info info() is a function that print information to screen. It doesn’t return any object dataframe.info() # display columns and number of rows (that has no missing data) goo.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 61 entries, 0 to 60 ## Data columns (total 6 columns): ## Date 61 non-null object ## Open 61 non-null float64 ## High 61 non-null float64 ## Low 61 non-null float64 ## Close 61 non-null float64 ## Volume 61 non-null int64 ## dtypes: float64(4), int64(1), object(1) ## memory usage: 3.0+ KB 13.3.6.2 head goo.head() ## Date Open High Low Close Volume ## 0 12/19/2016 790.219971 797.659973 786.270020 794.200012 1225900 ## 1 12/20/2016 796.760010 798.650024 793.270020 796.419983 925100 ## 2 12/21/2016 795.840027 796.676025 787.099976 794.559998 1208700 ## 3 12/22/2016 792.359985 793.320007 788.580017 791.260010 969100 ## 4 12/23/2016 790.900024 792.739990 787.280029 789.909973 623400 13.4 class: Timestamp This is an enhanced version to datetime standard library. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html#pandas.Timestamp 13.4.1 Constructor 13.4.1.1 From Number print( pd.Timestamp(year=2017, month=1, day=1),&#39;\\n&#39;, #date-like numbers pd.Timestamp(2017,1,1), &#39;\\n&#39;, # date-like numbers pd.Timestamp(2017,12,11,5,45),&#39;\\n&#39;, # datetime-like numbers pd.Timestamp(2017,12,11,5,45,55,999),&#39;\\n&#39;, # + microseconds pd.Timestamp(2017,12,11,5,45,55,999,8),&#39;\\n&#39;, # + nanoseconds type(pd.Timestamp(2017,12,11,5,45,55,999,8)),&#39;\\n&#39;) ## 2017-01-01 00:00:00 ## 2017-01-01 00:00:00 ## 2017-12-11 05:45:00 ## 2017-12-11 05:45:55.000999 ## 2017-12-11 05:45:55.000999008 ## &lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; 13.4.1.2 From String Observe that pandas support many string input format Year Month Day, default has no timezone print( pd.Timestamp(&#39;2017-12-11&#39;),&#39;\\n&#39;, # date-like string: year-month-day pd.Timestamp(&#39;2017 12 11&#39;),&#39;\\n&#39;, # date-like string: year-month-day pd.Timestamp(&#39;2017 Dec 11&#39;),&#39;\\n&#39;, # date-like string: year-month-day pd.Timestamp(&#39;Dec 11, 2017&#39;)) # date-like string: year-month-day ## 2017-12-11 00:00:00 ## 2017-12-11 00:00:00 ## 2017-12-11 00:00:00 ## 2017-12-11 00:00:00 YMD Hour Minute Second Ms print( pd.Timestamp(&#39;2017-12-11 0545&#39;),&#39;\\n&#39;, ## hour minute pd.Timestamp(&#39;2017-12-11-05:45&#39;),&#39;\\n&#39;, pd.Timestamp(&#39;2017-12-11T0545&#39;),&#39;\\n&#39;, pd.Timestamp(&#39;2017-12-11 054533&#39;),&#39;\\n&#39;, ## hour minute seconds pd.Timestamp(&#39;2017-12-11 05:45:33&#39;)) ## 2017-12-11 05:45:00 ## 2017-12-11 05:45:00 ## 2017-12-11 05:45:00 ## 2017-12-11 05:45:33 ## 2017-12-11 05:45:33 With Timezone can be included in various ways. print( pd.Timestamp(&#39;2017-01-01T0545Z&#39;),&#39;\\n&#39;, # GMT pd.Timestamp(&#39;2017-01-01T0545+9&#39;),&#39;\\n&#39;, # GMT+9 pd.Timestamp(&#39;2017-01-01T0545+0800&#39;),&#39;\\n&#39;, # GMT+0800 pd.Timestamp(&#39;2017-01-01 0545&#39;, tz=&#39;Asia/Singapore&#39;),&#39;\\n&#39;) ## 2017-01-01 05:45:00+00:00 ## 2017-01-01 05:45:00+09:00 ## 2017-01-01 05:45:00+08:00 ## 2017-01-01 05:45:00+08:00 13.4.1.3 From Standard Library datetime and date Object print( pd.Timestamp(date(2017,3,5)),&#39;\\n&#39;, # from date pd.Timestamp(datetime(2017,3,5,4,30)),&#39;\\n&#39;, # from datetime pd.Timestamp(datetime(2017,3,5,4,30), tz=&#39;Asia/Kuala_Lumpur&#39;)) # from datetime, + tz ## 2017-03-05 00:00:00 ## 2017-03-05 04:30:00 ## 2017-03-05 04:30:00+08:00 13.4.2 Attributes We can tell many things about a Timestamp object. ts = pd.Timestamp(&#39;2017-01-01T054533+0800&#39;) # GMT+0800 print( ts.month, &#39;\\n&#39;, ts.day, &#39;\\n&#39;, ts.year, &#39;\\n&#39;, ts.hour, &#39;\\n&#39;, ts.minute, &#39;\\n&#39;, ts.second, &#39;\\n&#39;, ts.microsecond, &#39;\\n&#39;, ts.nanosecond, &#39;\\n&#39;, ts.tz, &#39;\\n&#39;, ts.daysinmonth,&#39;\\n&#39;, ts.dayofyear, &#39;\\n&#39;, ts.is_leap_year, &#39;\\n&#39;, ts.is_month_end, &#39;\\n&#39;, ts.is_month_start, &#39;\\n&#39;, ts.dayofweek) ## 1 ## 1 ## 2017 ## 5 ## 45 ## 33 ## 0 ## 0 ## pytz.FixedOffset(480) ## 31 ## 1 ## False ## False ## True ## 6 Note that timezone (tz) is a pytz object. ts1 = pd.Timestamp(datetime(2017,3,5,4,30), tz=&#39;Asia/Kuala_Lumpur&#39;) # from datetime, + tz ts2 = pd.Timestamp(&#39;2017-01-01T054533+0800&#39;) # GMT+0800 ts3 = pd.Timestamp(&#39;2017-01-01T0545&#39;) print( ts1.tz, &#39;Type:&#39;, type(ts1.tz), &#39;\\n&#39;, ts2.tz, &#39;Type:&#39;, type(ts2.tz), &#39;\\n&#39;, ts3.tz, &#39;Type:&#39;, type(ts3.tz) ) ## Asia/Kuala_Lumpur Type: &lt;class &#39;pytz.tzfile.Asia/Kuala_Lumpur&#39;&gt; ## pytz.FixedOffset(480) Type: &lt;class &#39;pytz._FixedOffset&#39;&gt; ## None Type: &lt;class &#39;NoneType&#39;&gt; 13.4.3 Instance Methods 13.4.3.1 Atribute-like Methods ts = pd.Timestamp(2017,1,1) print( &#39; Weekday: &#39;, ts.weekday(), &#39;\\n&#39;, &#39;ISO Weekday:&#39;, ts.isoweekday(), &#39;\\n&#39;, &#39;Day Name: &#39;, ts.day_name(), &#39;\\n&#39;, &#39;ISO Calendar:&#39;, ts.isocalendar() ) ## Weekday: 6 ## ISO Weekday: 7 ## Day Name: Sunday ## ISO Calendar: (2016, 52, 7) 13.4.3.2 Timezones Adding Timezones and Clock Shifting tz_localize will add the timezone, however will not shift the clock. Once a timestamp had gotten a timezone, you can easily shift the clock to another timezone using tz_convert() ts = pd.Timestamp(2017,1,10,10,34) ## No timezone ts1 = ts.tz_localize(&#39;Asia/Kuala_Lumpur&#39;) ## Add timezone ts2 = ts1.tz_convert(&#39;UTC&#39;) ## Convert timezone print(&#39; Origininal Timestamp :&#39;, ts, &#39;\\n&#39;, &#39;Loacalized Timestamp (added TZ):&#39;, ts1, &#39;\\n&#39;, &#39;Converted Timestamp (shifted) :&#39;,ts2) ## Origininal Timestamp : 2017-01-10 10:34:00 ## Loacalized Timestamp (added TZ): 2017-01-10 10:34:00+08:00 ## Converted Timestamp (shifted) : 2017-01-10 02:34:00+00:00 Removing Timezone Just apply None with tz_localize to remove TZ infomration. ts = pd.Timestamp(2017,1,10,10,34) ## No timezone ts = ts.tz_localize(&#39;Asia/Kuala_Lumpur&#39;) ## Add timezone ts = ts.tz_localize(None) ## Convert timezone ts ## Timestamp(&#39;2017-01-10 10:34:00&#39;) 13.4.3.3 Formatting strftime Use strftime() to customize string format. For complete directive, see below: https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior ts = pd.Timestamp(2017,1,10,10,34) ## No timezone ts = ts.tz_localize(&#39;Asia/Kuala_Lumpur&#39;) ## Add timezone ts.strftime(&quot;%m/%d&quot;) ## &#39;01/10&#39; isoformat Use isoformat() to format ISO string (without timezone) ts = pd.Timestamp(2017,1,10,10,34) ts1 = ts.tz_localize(&#39;Asia/Kuala_Lumpur&#39;) print( &#39; ISO Format without TZ:&#39;, ts.isoformat(), &#39;\\n&#39;, &#39;ISO Format with TZ :&#39;, ts1.isoformat()) ## ISO Format without TZ: 2017-01-10T10:34:00 ## ISO Format with TZ : 2017-01-10T10:34:00+08:00 13.4.3.4 Type Conversion Convert To datetime.datetime/date Use to_pydatetime() to convert into standard library datetime.datetime. From the ‘datetime’ object, apply date() to get datetime.date ts = pd.Timestamp(2017,1,10,7,30,52) print( &#39;Datetime:&#39;, ts.to_pydatetime(), &#39;\\n&#39;, &#39;Date Only:&#39;, ts.to_pydatetime().date()) ## Datetime: 2017-01-10 07:30:52 ## Date Only: 2017-01-10 Convert To numpy.datetime64 Use to_datetime64() to convert into numpy.datetime64 ts = pd.Timestamp(2017,1,10,7,30,52) ts.to_datetime64() ## numpy.datetime64(&#39;2017-01-10T07:30:52.000000000&#39;) 13.4.3.5 ceil print( ts.ceil(freq=&#39;D&#39;) ) # ceiling to day ## 2017-01-11 00:00:00 13.4.3.6 Updating replace() ts.replace(year=2000, month=1,day=1) ## Timestamp(&#39;2000-01-01 07:30:52&#39;) 13.5 class: DateTimeIndex 13.5.1 Creating Refer to Pandas class method above. 13.5.2 Instance Method 13.5.2.1 Data Type Conversion Convert To datetime.datetime Use to_pydatetime to convert into python standard datetime.datetime object print(&#39;Converted to List:&#39;, dti.to_pydatetime(), &#39;\\n\\n&#39;, &#39;Converted Type:&#39;, type(dti.to_pydatetime())) ## Converted to List: [datetime.datetime(2011, 1, 3, 0, 0) datetime.datetime(2018, 4, 13, 0, 0) ## datetime.datetime(2018, 3, 1, 7, 30)] ## ## Converted Type: &lt;class &#39;numpy.ndarray&#39;&gt; 13.5.2.2 Structure Conversion Convert To Series: to_series This creates a Series where index and data with the same value #dti = pd.date_range(&#39;2018-02&#39;, periods=4, freq=&#39;M&#39;) dti.to_series() ## 2011-01-03 00:00:00 2011-01-03 00:00:00 ## 2018-04-13 00:00:00 2018-04-13 00:00:00 ## 2018-03-01 07:30:00 2018-03-01 07:30:00 ## dtype: datetime64[ns] Convert To DataFrame: to_frame() This convert to single column DataFrame with index as the same value dti.to_frame() ## 0 ## 2011-01-03 00:00:00 2011-01-03 00:00:00 ## 2018-04-13 00:00:00 2018-04-13 00:00:00 ## 2018-03-01 07:30:00 2018-03-01 07:30:00 13.5.3 Attributes All Timestamp Attributes can be used upon DateTimeIndex. print( dti.weekday, &#39;\\n&#39;, dti.month, &#39;\\n&#39;, dti.daysinmonth) ## Int64Index([0, 4, 3], dtype=&#39;int64&#39;) ## Int64Index([1, 4, 3], dtype=&#39;int64&#39;) ## Int64Index([31, 30, 31], dtype=&#39;int64&#39;) 13.6 class: Series Series allows different data types (object class) as its element pandas.Series(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False) - data array-like, iterable, dict or scalar - If dtype not specified, it will infer from data. 13.6.1 Constructor 13.6.1.1 Empty Series Passing no data to constructor will result in empty series. By default, empty series dtype is float. s = pd.Series(dtype=&#39;object&#39;) print (s, &#39;\\n&#39;, type(s)) ## Series([], dtype: object) ## &lt;class &#39;pandas.core.series.Series&#39;&gt; 13.6.1.2 From Scalar If data is a scalar value, an index must be provided. The value will be repeated to match the length of index pd.Series( 99, index = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;]) ## a 99 ## b 99 ## c 99 ## d 99 ## dtype: int64 13.6.1.3 From array-like From list pd.Series([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;]) # from Python list ## 0 a ## 1 b ## 2 c ## 3 d ## 4 e ## dtype: object From numpy.array If index is not specified, default to 0 and continue incrementally pd.Series(np.array([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;])) ## 0 a ## 1 b ## 2 c ## 3 d ## 4 e ## dtype: object From DateTimeIndex pd.Series(pd.date_range(&#39;2011-1-1&#39;,&#39;2011-1-3&#39;)) ## 0 2011-01-01 ## 1 2011-01-02 ## 2 2011-01-03 ## dtype: datetime64[ns] 13.6.1.4 From Dictionary The dictionary key will be the index. Order is not sorted. pd.Series({&#39;a&#39; : 0., &#39;c&#39; : 5., &#39;b&#39; : 2.}) ## a 0.0 ## c 5.0 ## b 2.0 ## dtype: float64 If index sequence is specifeid, then Series will forllow the index order Objerve that missing data (index without value) will be marked as NaN pd.Series({&#39;a&#39; : 0., &#39;c&#39; : 1., &#39;b&#39; : 2.},index = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;]) ## a 0.0 ## b 2.0 ## c 1.0 ## d NaN ## dtype: float64 13.6.1.5 Specify Index pd.Series([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;], index=[10,20,30,40,50]) ## 10 a ## 20 b ## 30 c ## 40 d ## 50 e ## dtype: object 13.6.1.6 Mix Element Types dType will be ‘object’ when there were mixture of classes ser = pd.Series([&#39;a&#39;,1,2,3]) print(&#39;Object Type : &#39;, type(ser),&#39;\\n&#39;, &#39;Object dType: &#39;, ser.dtype,&#39;\\n&#39;, &#39;Element 1 Type: &#39;,type(ser[0]),&#39;\\n&#39;, &#39;Elmeent 2 Type: &#39;,type(ser[1])) ## Object Type : &lt;class &#39;pandas.core.series.Series&#39;&gt; ## Object dType: object ## Element 1 Type: &lt;class &#39;str&#39;&gt; ## Elmeent 2 Type: &lt;class &#39;int&#39;&gt; 13.6.1.7 Specify Data Types By default, dtype is inferred from data. ser1 = pd.Series([1,2,3]) ser2 = pd.Series([1,2,3], dtype=&quot;int8&quot;) ser3 = pd.Series([1,2,3], dtype=&quot;object&quot;) print(&#39; Inferred: &#39;,ser1.dtype, &#39;\\n&#39;, &#39;Specified int8: &#39;,ser2.dtype, &#39;\\n&#39;, &#39;Specified object:&#39;,ser3.dtype) ## Inferred: int64 ## Specified int8: int8 ## Specified object: object 13.6.2 Accessing Series series ( single/list/range_of_row_label/number ) # can cause confusion series.loc ( single/list/range_of_row_label ) series.iloc( single/list/range_of_row_number ) 13.6.2.1 Sample Data s = pd.Series([1,2,3,4,5],index=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;]) s ## a 1 ## b 2 ## c 3 ## d 4 ## e 5 ## dtype: int64 13.6.2.2 by Row Number(s) Single Item. Notice that inputing a number and list of number give different result. print( &#39;Referencing by number:&#39;,s.iloc[1],&#39;\\n\\n&#39;, &#39;\\nReferencing by list of number:\\n&#39;,s.iloc[[1]]) ## Referencing by number: 2 ## ## ## Referencing by list of number: ## b 2 ## dtype: int64 Multiple Items s.iloc[[1,3]] ## b 2 ## d 4 ## dtype: int64 Range (First 3) s.iloc[:3] ## a 1 ## b 2 ## c 3 ## dtype: int64 Range (Last 3) s.iloc[-3:] ## c 3 ## d 4 ## e 5 ## dtype: int64 Range (in between) s.iloc[2:3] ## c 3 ## dtype: int64 13.6.2.3 by Index(es) Single Label. Notice the difference referencing input: single index and list of index. Warning: if index is invalid, this will result in error. print( s.loc[&#39;c&#39;], &#39;\\n&#39;, s[[&#39;c&#39;]]) ## 3 ## c 3 ## dtype: int64 Multiple Labels If index is not found, it will return NaN s.loc[[&#39;k&#39;,&#39;c&#39;]] ## k NaN ## c 3.0 ## dtype: float64 ## ## C:/ProgramData/Anaconda3/python.exe:1: FutureWarning: ## Passing list-likes to .loc or [] with any missing label will raise ## KeyError in the future, you can use .reindex() as an alternative. ## ## See the documentation here: ## https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike ** Range of Labels ** s.loc[&#39;b&#39;:&#39;d&#39;] ## b 2 ## c 3 ## d 4 ## dtype: int64 13.6.2.4 Filtering Use logical array to filter s = pd.Series(range(1,8)) s[s&lt;5] ## 0 1 ## 1 2 ## 2 3 ## 3 4 ## dtype: int64 Use where The where method is an application of the if-then idiom. For each element in the calling Series, if cond is True the element is used; otherwise other is used. .where(cond, other=nan, inplace=False) print(s.where(s&lt;4),&#39;\\n\\n&#39;, s.where(s&lt;4,other=None) ) ## 0 1.0 ## 1 2.0 ## 2 3.0 ## 3 NaN ## 4 NaN ## 5 NaN ## 6 NaN ## dtype: float64 ## ## 0 1 ## 1 2 ## 2 3 ## 3 None ## 4 None ## 5 None ## 6 None ## dtype: object 13.6.3 Updating Series 13.6.3.1 by Row Number(s) s = pd.Series(range(1,7), index=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;]) s[2] = 999 s[[3,4]] = 888,777 s ## a 1 ## b 2 ## c 999 ## d 888 ## e 777 ## f 6 ## dtype: int64 13.6.3.2 by Index(es) s = pd.Series(range(1,7), index=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;]) s[&#39;e&#39;] = 888 s[[&#39;c&#39;,&#39;d&#39;]] = 777,888 s ## a 1 ## b 2 ## c 777 ## d 888 ## e 888 ## f 6 ## dtype: int64 13.6.4 Series Attributes 13.6.4.1 The Data s = pd.Series([1,2,3,4,5],index=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;],name=&#39;SuperHero&#39;) s ## a 1 ## b 2 ## c 3 ## d 4 ## e 5 ## Name: SuperHero, dtype: int64 13.6.4.2 The Attributes print( &#39; Series Index: &#39;,s.index, &#39;\\n&#39;, &#39;Series dType: &#39;, s.dtype, &#39;\\n&#39;, &#39;Series Size: &#39;, s.size, &#39;\\n&#39;, &#39;Series Shape: &#39;, s.shape, &#39;\\n&#39;, &#39;Series Dimension:&#39;, s.ndim) ## Series Index: Index([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;], dtype=&#39;object&#39;) ## Series dType: int64 ## Series Size: 5 ## Series Shape: (5,) ## Series Dimension: 1 13.6.5 Instance Methods 13.6.5.1 .reset_index () Resetting index will: - Convert index to a normal column, header is ‘index’ - Index renumbered to ,1,2,3 - Retrun DataFrame (became two columns) print(s) ## a 1 ## b 2 ## c 3 ## d 4 ## e 5 ## Name: SuperHero, dtype: int64 print(s.reset_index()) ## index SuperHero ## 0 a 1 ## 1 b 2 ## 2 c 3 ## 3 d 4 ## 4 e 5 13.6.5.2 Structure Conversion A series structure contain value (in numpy array), its dtype (data type of the numpy array). Use values to retrieve into `numpy.ndarray. Use dtype to understand the data type. s = pd.Series([1,2,3,4,5]) print(&#39; Series value: &#39;, s.values, &#39;\\n&#39;, &#39;Series value type: &#39;, type(s.values), &#39;\\n&#39;, &#39;Series dtype: &#39;,s.dtype) ## Series value: [1 2 3 4 5] ## Series value type: &lt;class &#39;numpy.ndarray&#39;&gt; ## Series dtype: int64 Use pandas.Series.tolist() to convert into standard python `list pd.Series.tolist(s) ## [1, 2, 3, 4, 5] 13.6.5.3 DataType Conversion Use astype() to convert to another numpy supproted datatypes, results in a new Series. Warning: casting to incompatible type will result in error s.astype(&#39;int8&#39;) ## 0 1 ## 1 2 ## 2 3 ## 3 4 ## 4 5 ## dtype: int8 13.6.6 Series Operators The result of applying operator (arithmetic or logic) to Series object returns a new Series object 13.6.6.1 Arithmetic Operator s1 = pd.Series( [100,200,300,400,500] ) s2 = pd.Series( [10, 20, 30, 40, 50] ) Apply To One Series Object s1 - 100 ## 0 0 ## 1 100 ## 2 200 ## 3 300 ## 4 400 ## dtype: int64 Apply To Two Series Objects s1 - s2 ## 0 90 ## 1 180 ## 2 270 ## 3 360 ## 4 450 ## dtype: int64 13.6.6.2 Logic Operator Apply logic operator to a Series return a new Series of boolean result This can be used for Series or DataFrame filtering bs = pd.Series(range(0,10)) bs&gt;3 ## 0 False ## 1 False ## 2 False ## 3 False ## 4 True ## 5 True ## 6 True ## 7 True ## 8 True ## 9 True ## dtype: bool ~((bs&gt;3) &amp; (bs&lt;8) | (bs&gt;7)) ## 0 True ## 1 True ## 2 True ## 3 True ## 4 False ## 5 False ## 6 False ## 7 False ## 8 False ## 9 False ## dtype: bool 13.6.7 Series .str Accesor If the underlying data is str type, then pandas exposed various properties and methos through str accessor. SeriesObj.str.operatorFunction() Available Functions Nearly all Python’s built-in string methods are mirrored by a Pandas vectorized string method. Here is a list of Pandas str methods that mirror Python string methods: len() lower() translate() islower() ljust() upper() startswith() isupper() rjust() find() endswith() isnumeric() center() rfind() isalnum() isdecimal() zfill() index() isalpha() split() strip() rindex() isdigit() rsplit() rstrip() capitalize() isspace() partition() lstrip() swapcase() istitle() rpartition() 13.6.7.1 Regex Extractor Extract capture groups in the regex pattern, by default in DataFrame (expand=True). Series.str.extract(self, pat, flags=0, expand=True) - expand=True: if result is single column, make it a Series instead of Dataframe. s = pd.Series([&#39;a1&#39;, &#39;b2&#39;, &#39;c3&#39;]) print( &#39; Extracted Dataframe:\\n&#39;, s.str.extract(r&#39;([ab])(\\d)&#39;),&#39;\\n\\n&#39;, &#39;Extracted Dataframe witn Names:\\n&#39;, s.str.extract(r&#39;(?P&lt;Letter&gt;[ab])(\\d)&#39;)) ## Extracted Dataframe: ## 0 1 ## 0 a 1 ## 1 b 2 ## 2 NaN NaN ## ## Extracted Dataframe witn Names: ## Letter 1 ## 0 a 1 ## 1 b 2 ## 2 NaN NaN Below ouptut single columne, use expand=False to make the result a Series, instead of DataFrame. r = s.str.extract(r&#39;[ab](\\d)&#39;, expand=False) print( r, &#39;\\n\\n&#39;, type(r) ) ## 0 1 ## 1 2 ## 2 NaN ## dtype: object ## ## &lt;class &#39;pandas.core.series.Series&#39;&gt; 13.6.7.2 Character Extractor monte = pd.Series([&#39;Graham Chapman&#39;, &#39;John Cleese&#39;, &#39;Terry Gilliam&#39;, &#39;Eric Idle&#39;, &#39;Terry Jones&#39;, &#39;Michael Palin&#39;]) monte ## 0 Graham Chapman ## 1 John Cleese ## 2 Terry Gilliam ## 3 Eric Idle ## 4 Terry Jones ## 5 Michael Palin ## dtype: object startwith monte.str.startswith(&#39;T&#39;) ## 0 False ## 1 False ## 2 True ## 3 False ## 4 True ## 5 False ## dtype: bool Slicing monte.str[0:3] ## 0 Gra ## 1 Joh ## 2 Ter ## 3 Eri ## 4 Ter ## 5 Mic ## dtype: object 13.6.7.3 Splitting Split strings around given separator/delimiter in either string or regex. Series.str.split(self, pat=None, n=-1, expand=False) - pat: can be string or regex s = pd.Series([&#39;a_b_c&#39;, &#39;c_d_e&#39;, np.nan, &#39;f_g_h_i_j&#39;]) s ## 0 a_b_c ## 1 c_d_e ## 2 NaN ## 3 f_g_h_i_j ## dtype: object str.split() by default, split will split each item into array s.str.split(&#39;_&#39;) ## 0 [a, b, c] ## 1 [c, d, e] ## 2 NaN ## 3 [f, g, h, i, j] ## dtype: object expand=True will return a dataframe instead of series. By default, expand split into all possible columns. print( s.str.split(&#39;_&#39;, expand=True) ) ## 0 1 2 3 4 ## 0 a b c None None ## 1 c d e None None ## 2 NaN NaN NaN NaN NaN ## 3 f g h i j It is possible to limit the number of columns splitted print( s.str.split(&#39;_&#39;, expand=True, n=1) ) ## 0 1 ## 0 a b_c ## 1 c d_e ## 2 NaN NaN ## 3 f g_h_i_j str.rsplit() rsplit stands for reverse split, it works the same way, except it is reversed print( s.str.rsplit(&#39;_&#39;, expand=True, n=1) ) ## 0 1 ## 0 a_b c ## 1 c_d e ## 2 NaN NaN ## 3 f_g_h_i j 13.6.7.4 Case Conversion SeriesObj.str.upper() SeriesObj.str.lower() SeriesObj.str.capitalize() s = pd.Series([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;aAba&#39;, &#39;bBaca&#39;, np.nan, &#39;cCABA&#39;, &#39;dog&#39;, &#39;cat&#39;]) print( s.str.upper(), &#39;\\n&#39;, s.str.capitalize()) ## 0 A ## 1 B ## 2 C ## 3 AABA ## 4 BBACA ## 5 NaN ## 6 CCABA ## 7 DOG ## 8 CAT ## dtype: object ## 0 A ## 1 B ## 2 C ## 3 Aaba ## 4 Bbaca ## 5 NaN ## 6 Ccaba ## 7 Dog ## 8 Cat ## dtype: object 13.6.7.5 Number of Characters s.str.len() ## 0 1.0 ## 1 1.0 ## 2 1.0 ## 3 4.0 ## 4 5.0 ## 5 NaN ## 6 5.0 ## 7 3.0 ## 8 3.0 ## dtype: float64 13.6.7.6 String Indexing This return specified character from each item. s = pd.Series([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;Aaba&#39;, &#39;Baca&#39;, np.nan,&#39;CABA&#39;, &#39;dog&#39;, &#39;cat&#39;]) s.str[0].values # first char ## array([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;A&#39;, &#39;B&#39;, nan, &#39;C&#39;, &#39;d&#39;, &#39;c&#39;], dtype=object) s.str[0:2].values # first and second char ## array([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;Aa&#39;, &#39;Ba&#39;, nan, &#39;CA&#39;, &#39;do&#39;, &#39;ca&#39;], dtype=object) 13.6.7.7 Series Substring Extraction Sample Data s = pd.Series([&#39;a1&#39;, &#39;b2&#39;, &#39;c3&#39;]) s ## 0 a1 ## 1 b2 ## 2 c3 ## dtype: object Extract absed on regex matching … to improve … type(s.str.extract(&#39;([ab])(\\d)&#39;, expand=False)) ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; 13.6.8 Series DateTime Accessor .dt If the underlying data is datetime64 type, then pandas exposed various properties and methos through dt accessor. 13.6.8.1 Sample Data s = pd.Series([ datetime(2000,1,1,0,0,0), datetime(1999,12,15,12,34,55), datetime(2020,3,8,5,7,12), datetime(2018,1,1,0,0,0), datetime(2003,3,4,5,6,7) ]) s ## 0 2000-01-01 00:00:00 ## 1 1999-12-15 12:34:55 ## 2 2020-03-08 05:07:12 ## 3 2018-01-01 00:00:00 ## 4 2003-03-04 05:06:07 ## dtype: datetime64[ns] 13.6.8.2 Convert To datetime.datetime Use to_pydatetime() to convert into numpy.array of standard library datetime.datetime pdt = s.dt.to_pydatetime() print( type(pdt) ) ## &lt;class &#39;numpy.ndarray&#39;&gt; pdt ## array([datetime.datetime(2000, 1, 1, 0, 0), ## datetime.datetime(1999, 12, 15, 12, 34, 55), ## datetime.datetime(2020, 3, 8, 5, 7, 12), ## datetime.datetime(2018, 1, 1, 0, 0), ## datetime.datetime(2003, 3, 4, 5, 6, 7)], dtype=object) datetime.date Use dt.date to convert into pandas.Series of standard library datetime.date Is it possible to have a pandas.Series of datetime.datetime ? No, because Pandas want it as its own Timestamp. sdt = s.dt.date print( type(sdt[1] )) ## &lt;class &#39;datetime.date&#39;&gt; print( type(sdt)) ## &lt;class &#39;pandas.core.series.Series&#39;&gt; sdt ## 0 2000-01-01 ## 1 1999-12-15 ## 2 2020-03-08 ## 3 2018-01-01 ## 4 2003-03-04 ## dtype: object 13.6.8.3 Timestamp Attributes A Series::DateTime object support below properties: - date - month - day - year - dayofweek - dayofyear - weekday - weekday_name - quarter - daysinmonth - hour - minute Full list below: https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetimelike-properties s.dt.date ## 0 2000-01-01 ## 1 1999-12-15 ## 2 2020-03-08 ## 3 2018-01-01 ## 4 2003-03-04 ## dtype: object s.dt.month ## 0 1 ## 1 12 ## 2 3 ## 3 1 ## 4 3 ## dtype: int64 s.dt.dayofweek ## 0 5 ## 1 2 ## 2 6 ## 3 0 ## 4 1 ## dtype: int64 s.dt.weekday ## 0 5 ## 1 2 ## 2 6 ## 3 0 ## 4 1 ## dtype: int64 s.dt.weekday_name ## 0 Saturday ## 1 Wednesday ## 2 Sunday ## 3 Monday ## 4 Tuesday ## dtype: object s.dt.quarter ## 0 1 ## 1 4 ## 2 1 ## 3 1 ## 4 1 ## dtype: int64 s.dt.daysinmonth ## 0 31 ## 1 31 ## 2 31 ## 3 31 ## 4 31 ## dtype: int64 s.dt.time # extract time as time Object ## 0 00:00:00 ## 1 12:34:55 ## 2 05:07:12 ## 3 00:00:00 ## 4 05:06:07 ## dtype: object s.dt.hour # extract hour as integer ## 0 0 ## 1 12 ## 2 5 ## 3 0 ## 4 5 ## dtype: int64 s.dt.minute # extract minute as integer ## 0 0 ## 1 34 ## 2 7 ## 3 0 ## 4 6 ## dtype: int64 13.7 class: DataFrame 13.7.1 Constructor 13.7.1.1 From Row Oriented Data (List of Lists) Create from List of Lists DataFrame( [row_list1, row_list2, row_list3] ) DataFrame( [row_list1, row_list2, row_list3], column=columnName_list ) DataFrame( [row_list1, row_list2, row_list3], index=row_label_list ) Basic DataFrame with default Row Label and Column Header pd.DataFrame ([[101,&#39;Alice&#39;,40000,2017], [102,&#39;Bob&#39;, 24000, 2017], [103,&#39;Charles&#39;,31000,2017]] ) ## 0 1 2 3 ## 0 101 Alice 40000 2017 ## 1 102 Bob 24000 2017 ## 2 103 Charles 31000 2017 Specify Column Header during Creation pd.DataFrame ([[101,&#39;Alice&#39;,40000,2017], [102,&#39;Bob&#39;, 24000, 2017], [103,&#39;Charles&#39;,31000,2017]], columns = [&#39;empID&#39;,&#39;name&#39;,&#39;salary&#39;,&#39;year&#39;]) ## empID name salary year ## 0 101 Alice 40000 2017 ## 1 102 Bob 24000 2017 ## 2 103 Charles 31000 2017 Specify Row Label during Creation pd.DataFrame ([[101,&#39;Alice&#39;,40000,2017], [102,&#39;Bob&#39;, 24000, 2017], [103,&#39;Charles&#39;,31000,2017]], index = [&#39;r1&#39;,&#39;r2&#39;,&#39;r3&#39;] ) ## 0 1 2 3 ## r1 101 Alice 40000 2017 ## r2 102 Bob 24000 2017 ## r3 103 Charles 31000 2017 13.7.1.2 From Row Oriented Data (List of Dictionary) DataFrame( [dict1, dict2, dict3] ) DataFrame( [row_list1, row_list2, row_list3], column=np.arrange ) DataFrame( [row_list1, row_list2, row_list3], index=row_label_list ) by default,keys will become collumn names, and autosorted Default Column Name Follow Dictionary Key Note missing info as NaN pd.DataFrame ([{&quot;name&quot;:&quot;Yong&quot;, &quot;id&quot;:1,&quot;zkey&quot;:101},{&quot;name&quot;:&quot;Gan&quot;,&quot;id&quot;:2}]) ## name id zkey ## 0 Yong 1 101.0 ## 1 Gan 2 NaN Specify Index pd.DataFrame ([{&quot;name&quot;:&quot;Yong&quot;, &quot;id&quot;:&#39;wd1&#39;},{&quot;name&quot;:&quot;Gan&quot;,&quot;id&quot;:&#39;wd2&#39;}], index = (1,2)) ## name id ## 1 Yong wd1 ## 2 Gan wd2 Specify Column Header during Creation, can acts as column filter and manual arrangement Note missing info as NaN pd.DataFrame ([{&quot;name&quot;:&quot;Yong&quot;, &quot;id&quot;:1, &quot;zkey&quot;:101},{&quot;name&quot;:&quot;Gan&quot;,&quot;id&quot;:2}], columns=(&quot;name&quot;,&quot;id&quot;,&quot;zkey&quot;)) ## name id zkey ## 0 Yong 1 101.0 ## 1 Gan 2 NaN 13.7.1.3 From Column Oriented Data Create from Dictrionary of List DataFrame( { &#39;column1&#39;: list1, &#39;column2&#39;: list2, &#39;column3&#39;: list3 } , index = row_label_list, columns = column_list) By default, DataFrame will arrange the columns alphabetically, unless columns is specified Default Row Label data = {&#39;empID&#39;: [100, 101, 102, 103, 104], &#39;year&#39;: [2017, 2017, 2017, 2018, 2018], &#39;salary&#39;: [40000, 24000, 31000, 20000, 30000], &#39;name&#39;: [&#39;Alice&#39;, &#39;Bob&#39;, &#39;Charles&#39;, &#39;David&#39;, &#39;Eric&#39;]} pd.DataFrame(data) ## empID year salary name ## 0 100 2017 40000 Alice ## 1 101 2017 24000 Bob ## 2 102 2017 31000 Charles ## 3 103 2018 20000 David ## 4 104 2018 30000 Eric Specify Row Label during Creation data = {&#39;empID&#39;: [100, 101, 102, 103, 104], &#39;name&#39;: [&#39;Alice&#39;, &#39;Bob&#39;, &#39;Charles&#39;, &#39;David&#39;, &#39;Eric&#39;], &#39;year&#39;: [2017, 2017, 2017, 2018, 2018], &#39;salary&#39;: [40000, 24000, 31000, 20000, 30000] } pd.DataFrame (data, index=[&#39;r1&#39;,&#39;r2&#39;,&#39;r3&#39;,&#39;r4&#39;,&#39;r5&#39;]) ## empID name year salary ## r1 100 Alice 2017 40000 ## r2 101 Bob 2017 24000 ## r3 102 Charles 2017 31000 ## r4 103 David 2018 20000 ## r5 104 Eric 2018 30000 Manualy Choose Columns and Arrangement data = {&#39;empID&#39;: [100, 101, 102, 103, 104], &#39;name&#39;: [&#39;Alice&#39;, &#39;Bob&#39;, &#39;Charles&#39;, &#39;David&#39;, &#39;Eric&#39;], &#39;year&#39;: [2017, 2017, 2017, 2018, 2018], &#39;salary&#39;: [40000, 24000, 31000, 20000, 30000] } pd.DataFrame (data, columns=(&#39;empID&#39;,&#39;name&#39;,&#39;salary&#39;), index=[&#39;r1&#39;,&#39;r2&#39;,&#39;r3&#39;,&#39;r4&#39;,&#39;r5&#39;]) ## empID name salary ## r1 100 Alice 40000 ## r2 101 Bob 24000 ## r3 102 Charles 31000 ## r4 103 David 20000 ## r5 104 Eric 30000 13.7.2 Operator 13.7.2.1 The Data Two dataframe is created, each with 3 columns and 3 rows. However, only two matching column and row names We shall notice that the operator will perform cell-wise, honoring the row/column name. df1 = pd.DataFrame(data= {&#39;idx&#39;: [&#39;row1&#39;,&#39;row2&#39;,&#39;row3&#39;], &#39;x&#39;: [10, 20, 30], &#39;y&#39;: [1,2,3], &#39;z&#39;: [0.1, 0.2, 0.3]}).set_index(&#39;idx&#39;) df2 = pd.DataFrame(data= {&#39;idx&#39;: [&#39;row1&#39;,&#39;row2&#39;,&#39;row4&#39;], &#39;x&#39;: [13, 23, 33], &#39;z&#39;: [0.1, 0.2, 0.3], &#39;k&#39;: [11,21,31] }).set_index(&#39;idx&#39;) print( df1, &#39;\\n\\n&#39;, df2) ## x y z ## idx ## row1 10 1 0.1 ## row2 20 2 0.2 ## row3 30 3 0.3 ## ## x z k ## idx ## row1 13 0.1 11 ## row2 23 0.2 21 ## row4 33 0.3 31 13.7.2.2 Addition Adding Two DataFrame Using + operator, non-matching row/column names will result in NA. However, when using function add, none matching cells can be assumed as with a value. r1 = df1 + df2 r2 = df1.add(df2,fill_value=1000) print( r1, &#39;\\n\\n&#39;, r2) ## k x y z ## idx ## row1 NaN 23.0 NaN 0.2 ## row2 NaN 43.0 NaN 0.4 ## row3 NaN NaN NaN NaN ## row4 NaN NaN NaN NaN ## ## k x y z ## idx ## row1 1011.0 23.0 1001.0 0.2 ## row2 1021.0 43.0 1002.0 0.4 ## row3 NaN 1030.0 1003.0 1000.3 ## row4 1031.0 1033.0 NaN 1000.3 Adding Series and DataFrame Specify the appropriate axis depending on the orientation of the series data. Column and Row names are respected in this operation. However, fill_value is not applicable when apply on Series. Note that columns in Series that are not found in dataframe, will still be created in the result. This is similar behaviour as operating Dataframe with Dataframe. s3 = pd.Series([1,1,1], index=[&#39;row1&#39;,&#39;row2&#39;,&#39;row4&#39;]) s4 = pd.Series([3,3,3], index=[&#39;x&#39;,&#39;y&#39;,&#39;s&#39;]) print(&#39;Original Data:\\n&#39;,df1,&#39;\\n\\n&#39;, &#39;Add By Rows: \\n&#39;, df1.add(s3, axis=0), &#39;\\n\\n&#39;, &#39;Add By Columns: \\n&#39;, df1.add(s4, axis=1)) ## Original Data: ## x y z ## idx ## row1 10 1 0.1 ## row2 20 2 0.2 ## row3 30 3 0.3 ## ## Add By Rows: ## x y z ## row1 11.0 2.0 1.1 ## row2 21.0 3.0 1.2 ## row3 NaN NaN NaN ## row4 NaN NaN NaN ## ## Add By Columns: ## s x y z ## idx ## row1 NaN 13.0 4.0 NaN ## row2 NaN 23.0 5.0 NaN ## row3 NaN 33.0 6.0 NaN 13.7.2.3 Substraction r1 = df2 - df1 r2 = df2.sub(df1,fill_value=1000) print( r1, &#39;\\n\\n&#39;, r2) ## k x y z ## idx ## row1 NaN 3.0 NaN 0.0 ## row2 NaN 3.0 NaN 0.0 ## row3 NaN NaN NaN NaN ## row4 NaN NaN NaN NaN ## ## k x y z ## idx ## row1 -989.0 3.0 999.0 0.0 ## row2 -979.0 3.0 998.0 0.0 ## row3 NaN 970.0 997.0 999.7 ## row4 -969.0 -967.0 NaN -999.7 r3 = (r2&gt;0) &amp; (r2&lt;=3) print( &#39;Original Data: \\n&#39;, r2, &#39;\\n\\n&#39;, &#39;Logical Operator:\\n&#39;, r3) ## Original Data: ## k x y z ## idx ## row1 -989.0 3.0 999.0 0.0 ## row2 -979.0 3.0 998.0 0.0 ## row3 NaN 970.0 997.0 999.7 ## row4 -969.0 -967.0 NaN -999.7 ## ## Logical Operator: ## k x y z ## idx ## row1 False True False False ## row2 False True False False ## row3 False False False False ## row4 False False False False 13.7.3 Attributes df = pd.DataFrame( { &#39;empID&#39;: [100, 101, 102, 103, 104], &#39;year1&#39;: [2017, 2017, 2017, 2018, 2018], &#39;name&#39;: [&#39;Alice&#39;, &#39;Bob&#39;, &#39;Charles&#39;,&#39;David&#39;, &#39;Eric&#39;], &#39;year2&#39;: [2001, 1907, 2003, 1998, 2011], &#39;salary&#39;: [40000, 24000, 31000, 20000, 30000]}, columns = [&#39;year1&#39;,&#39;salary&#39;,&#39;year2&#39;,&#39;empID&#39;,&#39;name&#39;]) 13.7.3.1 Dimensions df.shape ## (5, 5) 13.7.3.2 Index df.index ## RangeIndex(start=0, stop=5, step=1) Underlying Index values are numpy object df.index.values ## array([0, 1, 2, 3, 4], dtype=int64) 13.7.3.3 Columns df.columns ## Index([&#39;year1&#39;, &#39;salary&#39;, &#39;year2&#39;, &#39;empID&#39;, &#39;name&#39;], dtype=&#39;object&#39;) Underlying Index values are numpy object df.columns.values ## array([&#39;year1&#39;, &#39;salary&#39;, &#39;year2&#39;, &#39;empID&#39;, &#39;name&#39;], dtype=object) 13.7.3.4 Values Underlying Column values are numpy object df.values ## array([[2017, 40000, 2001, 100, &#39;Alice&#39;], ## [2017, 24000, 1907, 101, &#39;Bob&#39;], ## [2017, 31000, 2003, 102, &#39;Charles&#39;], ## [2018, 20000, 1998, 103, &#39;David&#39;], ## [2018, 30000, 2011, 104, &#39;Eric&#39;]], dtype=object) 13.7.4 Index Manipulation index and row label are used interchangeably in this book 13.7.4.1 Sample Data Columns are intentionaly ordered in a messy way df = pd.DataFrame( { &#39;empID&#39;: [100, 101, 102, 103, 104], &#39;year1&#39;: [2017, 2017, 2017, 2018, 2018], &#39;name&#39;: [&#39;Alice&#39;, &#39;Bob&#39;, &#39;Charles&#39;,&#39;David&#39;, &#39;Eric&#39;], &#39;year2&#39;: [2001, 1907, 2003, 1998, 2011], &#39;salary&#39;: [40000, 24000, 31000, 20000, 30000]}, columns = [&#39;year1&#39;,&#39;salary&#39;,&#39;year2&#39;,&#39;empID&#39;,&#39;name&#39;]) print (df, &#39;\\n&#39;) ## year1 salary year2 empID name ## 0 2017 40000 2001 100 Alice ## 1 2017 24000 1907 101 Bob ## 2 2017 31000 2003 102 Charles ## 3 2018 20000 1998 103 David ## 4 2018 30000 2011 104 Eric print (df.index) ## RangeIndex(start=0, stop=5, step=1) 13.7.4.2 Convert Column To Index set_index(&#39;column_name&#39;, inplace=False) inplace=True means don’t create a new dataframe. Modify existing dataframe inplace=False means return a new dataframe print(df) ## year1 salary year2 empID name ## 0 2017 40000 2001 100 Alice ## 1 2017 24000 1907 101 Bob ## 2 2017 31000 2003 102 Charles ## 3 2018 20000 1998 103 David ## 4 2018 30000 2011 104 Eric print(df.index,&#39;\\n&#39;) ## RangeIndex(start=0, stop=5, step=1) df.set_index(&#39;empID&#39;,inplace=True) print(df) ## year1 salary year2 name ## empID ## 100 2017 40000 2001 Alice ## 101 2017 24000 1907 Bob ## 102 2017 31000 2003 Charles ## 103 2018 20000 1998 David ## 104 2018 30000 2011 Eric print(df.index) # return new DataFrameObj ## Int64Index([100, 101, 102, 103, 104], dtype=&#39;int64&#39;, name=&#39;empID&#39;) 13.7.4.3 Convert Index Back To Column Reseting index will resequence the index as 0,1,2 etc Old index column will be converted back as normal column Operation support inplace** option df.reset_index(inplace=True) print(df) ## empID year1 salary year2 name ## 0 100 2017 40000 2001 Alice ## 1 101 2017 24000 1907 Bob ## 2 102 2017 31000 2003 Charles ## 3 103 2018 20000 1998 David ## 4 104 2018 30000 2011 Eric 13.7.4.4 Updating Index ( .index= ) Warning: - Updating index doesn’t reorder the data sequence - Number of elements before and after reorder must match, otherwise error - Same label are allowed to repeat - Not reversable df.index = [101, 101, 101, 102, 103] print( df ) ## empID year1 salary year2 name ## 101 100 2017 40000 2001 Alice ## 101 101 2017 24000 1907 Bob ## 101 102 2017 31000 2003 Charles ## 102 103 2018 20000 1998 David ## 103 104 2018 30000 2011 Eric 13.7.4.5 Reordering Index (. reindex ) Reindex will reorder the rows according to new index The operation is not reversable Start from this original dataframe Change the order of Index, always return a new dataframe df.index = [101,102,103,104,105] print( df ) ## original sequence ## empID year1 salary year2 name ## 101 100 2017 40000 2001 Alice ## 102 101 2017 24000 1907 Bob ## 103 102 2017 31000 2003 Charles ## 104 103 2018 20000 1998 David ## 105 104 2018 30000 2011 Eric print( df.reindex([103,102,101,104,105]) ) ## new sequence, new dataframe ## empID year1 salary year2 name ## 103 102 2017 31000 2003 Charles ## 102 101 2017 24000 1907 Bob ## 101 100 2017 40000 2001 Alice ## 104 103 2018 20000 1998 David ## 105 104 2018 30000 2011 Eric 13.7.5 Subsetting Columns Select Single Column Return Series dataframe.columnName # single column, name based, return Series object dataframe[ single_col_name ] # single column, name based, return Series object dataframe[ [single_col_name] ] # single column, name based, return DataFrame object Select Single/Multiple Columns Return DataFrame dataframe[ single/list_of_col_names ] # name based, return Dataframe object dataframe.loc[ : , single_col_name ] # single column, series dataframe.loc[ : , col_name_list ] # multiple columns, dataframe dataframe.loc[ : , col_name_ranage ] # multiple columns, dataframe dataframe.iloc[ : , col_number ] # single column, series dataframe.iloc[ : , col_number_list ] # multiple columns, dataframe dataframe.iloc[ : , number_range ] # multiple columns, dataframe 13.7.5.1 Select Single Column Selecting single column always return as panda::Series df.name ## 101 Alice ## 102 Bob ## 103 Charles ## 104 David ## 105 Eric ## Name: name, dtype: object df[&#39;name&#39;] ## 101 Alice ## 102 Bob ## 103 Charles ## 104 David ## 105 Eric ## Name: name, dtype: object df.loc[:, &#39;name&#39;] ## 101 Alice ## 102 Bob ## 103 Charles ## 104 David ## 105 Eric ## Name: name, dtype: object df.iloc[:, 3] ## 101 2001 ## 102 1907 ## 103 2003 ## 104 1998 ## 105 2011 ## Name: year2, dtype: int64 13.7.5.2 Select Multiple Columns Multiple columns return as panda::Dataframe object` df[[&#39;name&#39;]] # return one column dataframe ## name ## 101 Alice ## 102 Bob ## 103 Charles ## 104 David ## 105 Eric print(df.columns) ## Index([&#39;empID&#39;, &#39;year1&#39;, &#39;salary&#39;, &#39;year2&#39;, &#39;name&#39;], dtype=&#39;object&#39;) df[[&#39;name&#39;,&#39;year1&#39;]] ## name year1 ## 101 Alice 2017 ## 102 Bob 2017 ## 103 Charles 2017 ## 104 David 2018 ## 105 Eric 2018 df.loc[:,[&#39;name&#39;,&#39;year1&#39;]] ## name year1 ## 101 Alice 2017 ## 102 Bob 2017 ## 103 Charles 2017 ## 104 David 2018 ## 105 Eric 2018 df.loc[:,&#39;year1&#39;:&#39;year2&#39;] # range of columns ## year1 salary year2 ## 101 2017 40000 2001 ## 102 2017 24000 1907 ## 103 2017 31000 2003 ## 104 2018 20000 1998 ## 105 2018 30000 2011 df.iloc[:,[0,3]] ## empID year2 ## 101 100 2001 ## 102 101 1907 ## 103 102 2003 ## 104 103 1998 ## 105 104 2011 df.iloc[:,0:3] ## empID year1 salary ## 101 100 2017 40000 ## 102 101 2017 24000 ## 103 102 2017 31000 ## 104 103 2018 20000 ## 105 104 2018 30000 13.7.5.3 Selection by Data Type df.select_dtypes(include=None, exclude=None) Always return panda::DataFrame, even though only single column matches. Allowed types are: - number (integer and float) - integer / float - datetime - timedelta - category df.get_dtype_counts() ## int64 4 ## object 1 ## dtype: int64 ## ## C:/ProgramData/Anaconda3/python.exe:1: FutureWarning: `get_dtype_counts` has been deprecated and will be removed in a future version. For DataFrames use `.dtypes.value_counts() df.select_dtypes(exclude=&#39;number&#39;) ## name ## 101 Alice ## 102 Bob ## 103 Charles ## 104 David ## 105 Eric df.select_dtypes(exclude=(&#39;number&#39;,&#39;object&#39;)) ## Empty DataFrame ## Columns: [] ## Index: [101, 102, 103, 104, 105] 13.7.5.4 Subset by filter() .filter(items=None, like=None, regex=None, axis=1) like = Substring Matches df.filter( like=&#39;year&#39;, axis=&#39;columns&#39;) ## or axis = 1 ## year1 year2 ## 101 2017 2001 ## 102 2017 1907 ## 103 2017 2003 ## 104 2018 1998 ## 105 2018 2011 items = list of column names df.filter( items=(&#39;year1&#39;,&#39;year2&#39;), axis=1) ## or axis = 1 ## year1 year2 ## 101 2017 2001 ## 102 2017 1907 ## 103 2017 2003 ## 104 2018 1998 ## 105 2018 2011 regex = Regular Expression Select column names that contain integer df.filter(regex=&#39;\\d&#39;) ## default axis=1 if DataFrame ## year1 year2 ## 101 2017 2001 ## 102 2017 1907 ## 103 2017 2003 ## 104 2018 1998 ## 105 2018 2011 13.7.6 Column Manipulation 13.7.6.1 Sample Data df ## empID year1 salary year2 name ## 101 100 2017 40000 2001 Alice ## 102 101 2017 24000 1907 Bob ## 103 102 2017 31000 2003 Charles ## 104 103 2018 20000 1998 David ## 105 104 2018 30000 2011 Eric 13.7.6.2 Renaming Columns Method 1 : Rename All Columns (.columns =) - Construct the new column names, check if there is no missing column names - Missing columns will return error - Direct Assignment to column property result in change to dataframe new_columns = [&#39;year.1&#39;,&#39;salary&#39;,&#39;year.2&#39;,&#39;empID&#39;,&#39;name&#39;] df.columns = new_columns df.head(2) ## year.1 salary year.2 empID name ## 101 100 2017 40000 2001 Alice ## 102 101 2017 24000 1907 Bob Method 2 : Renaming Specific Column (.rename (columns=) ) - Change column name through rename function - Support inpalce option for original dataframe change - Missing column is OK df.rename( columns={&#39;year.1&#39;:&#39;year1&#39;, &#39;year.2&#39;:&#39;year2&#39;}, inplace=True) df.head(2) ## year1 salary year2 empID name ## 101 100 2017 40000 2001 Alice ## 102 101 2017 24000 1907 Bob 13.7.6.3 Reordering Columns Always return a new dataframe. There is no inplace option for reordering columns Method 1 - reindex(columns = ) - reindex may sounds like operation on row labels, but it works - Missmatch column names will result in NA for the unfound column new_colorder = [ &#39;empID&#39;, &#39;name&#39;, &#39;salary&#39;, &#39;year1&#39;, &#39;year2&#39;] df.reindex(columns = new_colorder).head(2) ## empID name salary year1 year2 ## 101 2001 Alice 2017 100 40000 ## 102 1907 Bob 2017 101 24000 Method 2 - [ ] notation - Missmatch column will result in ERROR new_colorder = [ &#39;empID&#39;, &#39;name&#39;, &#39;salary&#39;, &#39;year1&#39;, &#39;year2&#39;] df[new_colorder] ## empID name salary year1 year2 ## 101 2001 Alice 2017 100 40000 ## 102 1907 Bob 2017 101 24000 ## 103 2003 Charles 2017 102 31000 ## 104 1998 David 2018 103 20000 ## 105 2011 Eric 2018 104 30000 13.7.6.4 Duplicating or Replacing Column New Column will be created instantly using [] notation DO NOT USE dot Notation because it is view only attribute df[&#39;year3&#39;] = df.year1 df ## year1 salary year2 empID name year3 ## 101 100 2017 40000 2001 Alice 100 ## 102 101 2017 24000 1907 Bob 101 ## 103 102 2017 31000 2003 Charles 102 ## 104 103 2018 20000 1998 David 103 ## 105 104 2018 30000 2011 Eric 104 13.7.6.5 Dropping Columns (.drop) dataframe.drop( columns=&#39;column_name&#39;, inplace=True/False) # delete single column dataframe.drop( columns=list_of_colnames, inplace=True/False) # delete multiple column dataframe.drop( index=&#39;row_label&#39;, inplace=True/False) # delete single row dataframe.drop( index= list_of_row_labels, inplace=True/False) # delete multiple rows inplace=True means column will be deleted from original dataframe. Default is False, which return a copy of dataframe By Column Name(s) df.drop( columns=&#39;year1&#39;) # drop single column ## salary year2 empID name year3 ## 101 2017 40000 2001 Alice 100 ## 102 2017 24000 1907 Bob 101 ## 103 2017 31000 2003 Charles 102 ## 104 2018 20000 1998 David 103 ## 105 2018 30000 2011 Eric 104 df.drop(columns=[&#39;year2&#39;,&#39;year3&#39;]) # drop multiple columns ## year1 salary empID name ## 101 100 2017 2001 Alice ## 102 101 2017 1907 Bob ## 103 102 2017 2003 Charles ## 104 103 2018 1998 David ## 105 104 2018 2011 Eric By Column Number(s) Use dataframe.columns to produce interim list of column names df.drop( columns=df.columns[[3,4,5]] ) # delete columns by list of column number ## year1 salary year2 ## 101 100 2017 40000 ## 102 101 2017 24000 ## 103 102 2017 31000 ## 104 103 2018 20000 ## 105 104 2018 30000 df.drop( columns=df.columns[3:6] ) # delete columns by range of column number ## year1 salary year2 ## 101 100 2017 40000 ## 102 101 2017 24000 ## 103 102 2017 31000 ## 104 103 2018 20000 ## 105 104 2018 30000 13.7.7 Subsetting Rows dataframe.loc[ row_label ] # return series, single row dataframe.loc[ row_label_list ] # multiple rows dataframe.loc[ boolean_list ] # multiple rows dataframe.iloc[ row_number ] # return series, single row dataframe.iloc[ row_number_list ] # multiple rows dataframe.iloc[ number_range ] # multiple rows dataframe.sample(frac=) # frac = 0.6 means sampling 60% of rows randomly 13.7.7.1 Sample Data df = pd.DataFrame( { &#39;empID&#39;: [100, 101, 102, 103, 104], &#39;year1&#39;: [2017, 2017, 2017, 2018, 2018], &#39;name&#39;: [&#39;Alice&#39;, &#39;Bob&#39;, &#39;Charles&#39;,&#39;David&#39;, &#39;Eric&#39;], &#39;year2&#39;: [2001, 1907, 2003, 1998, 2011], &#39;salary&#39;: [40000, 24000, 31000, 20000, 30000]}, columns = [&#39;year1&#39;,&#39;salary&#39;,&#39;year2&#39;,&#39;empID&#39;,&#39;name&#39;]).set_index([&#39;empID&#39;]) df ## year1 salary year2 name ## empID ## 100 2017 40000 2001 Alice ## 101 2017 24000 1907 Bob ## 102 2017 31000 2003 Charles ## 103 2018 20000 1998 David ## 104 2018 30000 2011 Eric 13.7.7.2 By Index or Boolean Single Index return Series df.loc[101] # by single row label, return series ## year1 2017 ## salary 24000 ## year2 1907 ## name Bob ## Name: 101, dtype: object List or Range of Indexes returns DataFrame df.loc[ [100,103] ] # by multiple row labels ## year1 salary year2 name ## empID ## 100 2017 40000 2001 Alice ## 103 2018 20000 1998 David df.loc[ 100:103 ] # by range of row labels ## year1 salary year2 name ## empID ## 100 2017 40000 2001 Alice ## 101 2017 24000 1907 Bob ## 102 2017 31000 2003 Charles ## 103 2018 20000 1998 David List of Boolean returns DataFrame criteria = (df.salary &gt; 30000) &amp; (df.year1==2017) print (criteria) ## empID ## 100 True ## 101 False ## 102 True ## 103 False ## 104 False ## dtype: bool print (df.loc[criteria]) ## year1 salary year2 name ## empID ## 100 2017 40000 2001 Alice ## 102 2017 31000 2003 Charles 13.7.7.3 By Row Number Single Row return Series df.iloc[1] # by single row number ## year1 2017 ## salary 24000 ## year2 1907 ## name Bob ## Name: 101, dtype: object Multiple rows returned as dataframe object df.iloc[ [0,3] ] # by row numbers ## year1 salary year2 name ## empID ## 100 2017 40000 2001 Alice ## 103 2018 20000 1998 David df.iloc[ 0:3 ] # by row number range ## year1 salary year2 name ## empID ## 100 2017 40000 2001 Alice ## 101 2017 24000 1907 Bob ## 102 2017 31000 2003 Charles 13.7.7.4 query() .query(expr, inplace=False) df.query(&#39;salary&lt;=31000 and year1 == 2017&#39;) ## year1 salary year2 name ## empID ## 101 2017 24000 1907 Bob ## 102 2017 31000 2003 Charles 13.7.7.5 sample() np.random.seed(15) df.sample(frac=0.6) #randomly pick 60% of rows, without replacement ## year1 salary year2 name ## empID ## 102 2017 31000 2003 Charles ## 103 2018 20000 1998 David ## 104 2018 30000 2011 Eric 13.7.8 Row Manipulation 13.7.8.1 Sample Data 13.7.8.2 Appending Rows Appending rows is more computaional intensive then concatenate. Item can be added as single item or multi-items (list form) Append From Another DataFrame When ignore_index=True, pandas will drop the original Index and recreate with 0,1,2,3… It is recommended to ignore index IF the data source index is not unique. New columns will be added in the result, with NaN on original dataframe. my_df = pd.DataFrame( data= {&#39;Id&#39;: [10,20,30], &#39;Name&#39;: [&#39;Aaa&#39;,&#39;Bbb&#39;,&#39;Ccc&#39;]}) # .set_index(&#39;Id&#39;) my_df_new = pd.DataFrame( data= {&#39;Id&#39;: [40,50], &#39;Name&#39;: [&#39;Ddd&#39;,&#39;Eee&#39;], &#39;Age&#39;: [12,13]}) #.set_index(&#39;Id&#39;) my_df_append = my_df.append(my_df_new, ignore_index=False) my_df_noindex = my_df.append(my_df_new, ignore_index=True) print(&quot;Original DataFrame:\\n&quot;, my_df, &quot;\\n\\nTo Be Appended DataFrame:\\n&quot;, my_df_new, &quot;\\n\\nAppended DataFrame (index maintained):\\n&quot;, my_df_append, &quot;\\n\\nAppended DataFrame (index ignored):\\n&quot;, my_df_noindex) ## Original DataFrame: ## Id Name ## 0 10 Aaa ## 1 20 Bbb ## 2 30 Ccc ## ## To Be Appended DataFrame: ## Id Name Age ## 0 40 Ddd 12 ## 1 50 Eee 13 ## ## Appended DataFrame (index maintained): ## Age Id Name ## 0 NaN 10 Aaa ## 1 NaN 20 Bbb ## 2 NaN 30 Ccc ## 0 12.0 40 Ddd ## 1 13.0 50 Eee ## ## Appended DataFrame (index ignored): ## Age Id Name ## 0 NaN 10 Aaa ## 1 NaN 20 Bbb ## 2 NaN 30 Ccc ## 3 12.0 40 Ddd ## 4 13.0 50 Eee Append From Dictionary my_df = pd.DataFrame( data= {&#39;Id&#39;: [10,20,30], &#39;Name&#39;: [&#39;Aaa&#39;,&#39;Bbb&#39;,&#39;Ccc&#39;]}) \\ .set_index(&#39;Id&#39;) new_item1 = {&#39;Id&#39;:40, &#39;Name&#39;: &#39;Ddd&#39;} new_item2 = {&#39;Id&#39;:50, &#39;Name&#39;: &#39;Eee&#39;} new_item3 = {&#39;Id&#39;:60, &#39;Name&#39;: &#39;Fff&#39;} my_df_one = my_df.append( new_item1, ignore_index=True ) my_df_multi = my_df.append( [new_item2, new_item3], ignore_index=True ) print(&quot;Original DataFrame:\\n&quot;, my_df, &quot;\\n\\nAdd One Item (index ignored):\\n&quot;, my_df_one, &quot;\\n\\nAdd Multi Item (index ignored):\\n&quot;, my_df_multi) ## Original DataFrame: ## Name ## Id ## 10 Aaa ## 20 Bbb ## 30 Ccc ## ## Add One Item (index ignored): ## Name Id ## 0 Aaa NaN ## 1 Bbb NaN ## 2 Ccc NaN ## 3 Ddd 40.0 ## ## Add Multi Item (index ignored): ## Id Name ## 0 NaN Aaa ## 1 NaN Bbb ## 2 NaN Ccc ## 3 50.0 Eee ## 4 60.0 Fff Appending None items(s) Adding single None item has no effect (nothing added). Adding None in list form (multiple items) creates rows with None. ignore_index is not important here. single_none = my_df.append( None ) multi_none = my_df.append( [None]) print(&quot;Original DataFrame:\\n&quot;, my_df, &quot;\\n\\nAdd One None (index ignored):\\n&quot;, single_none, &quot;\\n\\nAdd List of None (index ignored):\\n&quot;, multi_none) ## Original DataFrame: ## Name ## Id ## 10 Aaa ## 20 Bbb ## 30 Ccc ## ## Add One None (index ignored): ## Name ## Id ## 10 Aaa ## 20 Bbb ## 30 Ccc ## ## Add List of None (index ignored): ## Name 0 ## 10 Aaa NaN ## 20 Bbb NaN ## 30 Ccc NaN ## 0 NaN None Appending Items Containing None results in ERROR my_df.append( [new_item1, None] ) ## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: &#39;NoneType&#39; object has no attribute &#39;keys&#39; ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py&quot;, line 7124, in append ## other = DataFrame(other) ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py&quot;, line 450, in __init__ ## arrays, columns = to_arrays(data, columns, dtype=dtype) ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py&quot;, line 467, in to_arrays ## data, columns, coerce_float=coerce_float, dtype=dtype ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py&quot;, line 563, in _list_of_dict_to_arrays ## columns = lib.fast_unique_multiple_list_gen(gen, sort=sort) ## File &quot;pandas/_libs/lib.pyx&quot;, line 297, in pandas._libs.lib.fast_unique_multiple_list_gen ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py&quot;, line 560, in &lt;genexpr&gt; ## gen = (list(x.keys()) for x in data) 13.7.8.3 Concatenate Rows 13.7.8.4 Dropping Rows (.drop) .drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise') By Row Label(s) df.drop(index=100) # single row ## year1 salary year2 name ## empID ## 101 2017 24000 1907 Bob ## 102 2017 31000 2003 Charles ## 103 2018 20000 1998 David ## 104 2018 30000 2011 Eric df.drop(index=[100,103]) # multiple rows ## year1 salary year2 name ## empID ## 101 2017 24000 1907 Bob ## 102 2017 31000 2003 Charles ## 104 2018 30000 2011 Eric 13.7.9 Slicing 13.7.9.1 Sample Data df ## year1 salary year2 name ## empID ## 100 2017 40000 2001 Alice ## 101 2017 24000 1907 Bob ## 102 2017 31000 2003 Charles ## 103 2018 20000 1998 David ## 104 2018 30000 2011 Eric 13.7.9.2 Getting One Cell By Row Label and Column Name (loc) dataframe.loc [ row_label , col_name ] # by row label and column names dataframe.loc [ bool_list , col_name ] # by row label and column names dataframe.iloc[ row_number, col_number ] # by row and column number print (df.loc[100,&#39;year1&#39;]) ## 2017 By Row Number and Column Number (iloc) print (df.iloc[1,2]) ## 1907 13.7.9.3 Getting Multiple Cells Specify rows and columns (by individual or range) dataframe.loc [ list/range_of_row_labels , list/range_col_names ] # by row label and column names dataframe.iloc[ list/range_row_numbers, list/range_col_numbers ] # by row number By Index and Column Name (loc) print (df.loc[ [101,103], [&#39;name&#39;,&#39;year1&#39;] ], &#39;\\n&#39;) # by list of row label and column names ## name year1 ## empID ## 101 Bob 2017 ## 103 David 2018 print (df.loc[ 101:104 , &#39;year1&#39;:&#39;year2&#39; ], &#39;\\n&#39;) # by range of row label and column names ## year1 salary year2 ## empID ## 101 2017 24000 1907 ## 102 2017 31000 2003 ## 103 2018 20000 1998 ## 104 2018 30000 2011 By Boolean Row and Column Names (loc) df.loc[df.year1==2017, &#39;year1&#39;:&#39;year2&#39;] ## year1 salary year2 ## empID ## 100 2017 40000 2001 ## 101 2017 24000 1907 ## 102 2017 31000 2003 By Row and Column Number (iloc) print (df.iloc[ [1,4], [0,3]],&#39;\\n&#39; ) # by individual rows/columns ## year1 name ## empID ## 101 2017 Bob ## 104 2018 Eric print (df.iloc[ 1:4 , 0:3], &#39;\\n&#39;) # by range ## year1 salary year2 ## empID ## 101 2017 24000 1907 ## 102 2017 31000 2003 ## 103 2018 20000 1998 13.7.10 Chained Indexing Chained Index Method creates a copy of dataframe, any modification of data on original dataframe does not affect the copy dataframe.loc [...] [...] dataframe.iloc [...] [...] Suggesting, never use chain indexing df = pd.DataFrame( { &#39;empID&#39;: [100, 101, 102, 103, 104], &#39;year1&#39;: [2017, 2017, 2017, 2018, 2018], &#39;name&#39;: [&#39;Alice&#39;, &#39;Bob&#39;, &#39;Charles&#39;,&#39;David&#39;, &#39;Eric&#39;], &#39;year2&#39;: [2001, 1907, 2003, 1998, 2011], &#39;salary&#39;: [40000, 24000, 31000, 20000, 30000]}, columns = [&#39;year1&#39;,&#39;salary&#39;,&#39;year2&#39;,&#39;empID&#39;,&#39;name&#39;]).set_index([&#39;empID&#39;]) df ## year1 salary year2 name ## empID ## 100 2017 40000 2001 Alice ## 101 2017 24000 1907 Bob ## 102 2017 31000 2003 Charles ## 103 2018 20000 1998 David ## 104 2018 30000 2011 Eric df.loc[100][&#39;year&#39;] =2000 ## C:/ProgramData/Anaconda3/python.exe:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame ## ## See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:205: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame ## ## See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy ## self._setitem_with_indexer(indexer, value) df ## notice row label 100 had not been updated, because data was updated on a copy due to chain indexing ## year1 salary year2 name ## empID ## 100 2017 40000 2001 Alice ## 101 2017 24000 1907 Bob ## 102 2017 31000 2003 Charles ## 103 2018 20000 1998 David ## 104 2018 30000 2011 Eric 13.7.11 Cell Value Replacement Slicing deals with square cells selection. Use mask or where to select specific cell(s). These function respect column and row names. 13.7.11.1 mask() mask() replace value with other= when condition is met. Column and row name is respected ori = pd.DataFrame(data={ &#39;x&#39;: [1,4,7], &#39;y&#39;: [2,5,8], &#39;z&#39;: [3,6,9]}, index=[ &#39;row1&#39;,&#39;row2&#39;,&#39;row3&#39;]) df_big = (ori &gt;4)[[&#39;y&#39;,&#39;x&#39;,&#39;z&#39;]] resul1 = ori.mask(df_big, other=999) print(&#39;Original DF: \\n&#39;, ori, &#39;\\n\\n&#39;, &#39;Big DF : \\n&#39;, df_big, &#39;\\n\\n&#39;, &#39;Result : \\n&#39;, resul1) ## Original DF: ## x y z ## row1 1 2 3 ## row2 4 5 6 ## row3 7 8 9 ## ## Big DF : ## y x z ## row1 False False False ## row2 True False True ## row3 True True True ## ## Result : ## x y z ## row1 1 2 3 ## row2 4 999 999 ## row3 999 999 999 13.7.11.2 where() This is reverse of mask(), it will repalce value when the condition is False. df.where(cond=df_big) ## year1 salary year2 name ## empID ## 100 NaN NaN NaN NaN ## 101 NaN NaN NaN NaN ## 102 NaN NaN NaN NaN ## 103 NaN NaN NaN NaN ## 104 NaN NaN NaN NaN 13.7.12 Iteration 13.7.12.1 .iterrows() loop through every ROW df = pd.DataFrame(data= { &#39;empID&#39;: [100, 101, 102, 103, 104], &#39;Name&#39;: [&#39;Alice&#39;, &#39;Bob&#39;, &#39;Charles&#39;,&#39;David&#39;, &#39;Eric&#39;], &#39;Year&#39;: [1999, 1988, 2001, 2010, 2020]}).set_index([&#39;empID&#39;]) for idx, row in df.iterrows(): print(idx, row.Name) ## 100 Alice ## 101 Bob ## 102 Charles ## 103 David ## 104 Eric 13.7.12.2 .items() loop through every Column for label, content in df.items(): print(&#39;Label:&#39;, label, &#39;\\n\\n&#39;, &#39;Content (Series):\\n&#39;, content, &#39;\\n\\n&#39;) ## Label: Name ## ## Content (Series): ## empID ## 100 Alice ## 101 Bob ## 102 Charles ## 103 David ## 104 Eric ## Name: Name, dtype: object ## ## ## Label: Year ## ## Content (Series): ## empID ## 100 1999 ## 101 1988 ## 102 2001 ## 103 2010 ## 104 2020 ## Name: Year, dtype: int64 13.7.13 Data Structure 13.7.13.1 Instance Methods - Structure Find out the column names, data type in a summary. Output is for display only, not a data object df.info() # return text output ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## Int64Index: 5 entries, 100 to 104 ## Data columns (total 2 columns): ## Name 5 non-null object ## Year 5 non-null int64 ## dtypes: int64(1), object(1) ## memory usage: 120.0+ bytes df.get_dtype_counts() # return Series ## int64 1 ## object 1 ## dtype: int64 13.7.13.2 Conversion To Other Format df.to_json() ## &#39;{&quot;Name&quot;:{&quot;100&quot;:&quot;Alice&quot;,&quot;101&quot;:&quot;Bob&quot;,&quot;102&quot;:&quot;Charles&quot;,&quot;103&quot;:&quot;David&quot;,&quot;104&quot;:&quot;Eric&quot;},&quot;Year&quot;:{&quot;100&quot;:1999,&quot;101&quot;:1988,&quot;102&quot;:2001,&quot;103&quot;:2010,&quot;104&quot;:2020}}&#39; df.to_records() ## rec.array([(100, &#39;Alice&#39;, 1999), (101, &#39;Bob&#39;, 1988), ## (102, &#39;Charles&#39;, 2001), (103, &#39;David&#39;, 2010), ## (104, &#39;Eric&#39;, 2020)], ## dtype=[(&#39;empID&#39;, &#39;&lt;i8&#39;), (&#39;Name&#39;, &#39;O&#39;), (&#39;Year&#39;, &#39;&lt;i8&#39;)]) df.to_csv() ## &#39;empID,Name,Year\\r\\n100,Alice,1999\\r\\n101,Bob,1988\\r\\n102,Charles,2001\\r\\n103,David,2010\\r\\n104,Eric,2020\\r\\n&#39; 13.8 class: MultiIndex MultiIndexing are columns with few levels of headers. 13.8.1 The Data df = pd.DataFrame({ &#39;myindex&#39;: [0, 1, 2], &#39;One_X&#39;: [1.1, 1.1, 1.1], &#39;One_Y&#39;: [1.2, 1.2, 1.2], &#39;Two_X&#39;: [1.11, 1.11, 1.11], &#39;Two_Y&#39;: [1.22, 1.22, 1.22]}) df.set_index(&#39;myindex&#39;,inplace=True) df ## One_X One_Y Two_X Two_Y ## myindex ## 0 1.1 1.2 1.11 1.22 ## 1 1.1 1.2 1.11 1.22 ## 2 1.1 1.2 1.11 1.22 13.8.2 Creating MultiIndex Object 13.8.2.1 Create From Tuples MultiIndex can easily created from typles: - Step 1: Create a MultiIndex object by splitting column name into tuples - Step 2: Assign the MultiIndex Object to dataframe columns property. my_tuples = [tuple(c.split(&#39;_&#39;)) for c in df.columns] df.columns = pd.MultiIndex.from_tuples(my_tuples) print(&#39; Column Headers :\\n\\n&#39;, my_tuples, &#39;\\n\\nNew Columns: \\n\\n&#39;, df.columns, &#39;\\n\\nTwo Layers Header DF:\\n\\n&#39;, df) ## Column Headers : ## ## [(&#39;One&#39;, &#39;X&#39;), (&#39;One&#39;, &#39;Y&#39;), (&#39;Two&#39;, &#39;X&#39;), (&#39;Two&#39;, &#39;Y&#39;)] ## ## New Columns: ## ## MultiIndex([(&#39;One&#39;, &#39;X&#39;), ## (&#39;One&#39;, &#39;Y&#39;), ## (&#39;Two&#39;, &#39;X&#39;), ## (&#39;Two&#39;, &#39;Y&#39;)], ## ) ## ## Two Layers Header DF: ## ## One Two ## X Y X Y ## myindex ## 0 1.1 1.2 1.11 1.22 ## 1 1.1 1.2 1.11 1.22 ## 2 1.1 1.2 1.11 1.22 13.8.3 MultiIndex Object 13.8.3.1 Levels MultiIndex object contain multiple leveels, each level (header) is an Index object. Use MultiIndex.get_level_values() to the entire header for the desired level. Note that each level is an Index object print(df.columns.get_level_values(0), &#39;\\n&#39;, df.columns.get_level_values(1)) ## Index([&#39;One&#39;, &#39;One&#39;, &#39;Two&#39;, &#39;Two&#39;], dtype=&#39;object&#39;) ## Index([&#39;X&#39;, &#39;Y&#39;, &#39;X&#39;, &#39;Y&#39;], dtype=&#39;object&#39;) MultiIndex.levels return the unique values of each level. print(df.columns.levels[0], &#39;\\n&#39;, df.columns.levels[1]) ## Index([&#39;One&#39;, &#39;Two&#39;], dtype=&#39;object&#39;) ## Index([&#39;X&#39;, &#39;Y&#39;], dtype=&#39;object&#39;) 13.8.3.2 Convert MultiIndex Back To Tuples df.columns.to_list() ## [(&#39;One&#39;, &#39;X&#39;), (&#39;One&#39;, &#39;Y&#39;), (&#39;Two&#39;, &#39;X&#39;), (&#39;Two&#39;, &#39;Y&#39;)] 13.8.4 Selecting Column(s) 13.8.4.1 Sample Data import itertools test_df = pd.DataFrame max_age = 100 ### Create The Columns Tuple level0_sex = [&#39;Male&#39;,&#39;Female&#39;,&#39;Pondan&#39;] level1_age = [&#39;Medium&#39;,&#39;High&#39;,&#39;Low&#39;] my_columns = list(itertools.product(level0_sex, level1_age)) test_df = pd.DataFrame([ [1,2,3,4,5,6,7,8,9], [11,12,13,14,15,16,17,18,19], [21,22,23,24,25,26,27,28,29]], index=[&#39;row1&#39;,&#39;row2&#39;,&#39;row3&#39;]) ### Create Multiindex From Tuple test_df.columns = pd.MultiIndex.from_tuples(my_columns) print( test_df ) ## Male Female Pondan ## Medium High Low Medium High Low Medium High Low ## row1 1 2 3 4 5 6 7 8 9 ## row2 11 12 13 14 15 16 17 18 19 ## row3 21 22 23 24 25 26 27 28 29 13.8.4.2 Select Level0 Header(s) Use [L0] notation, where L0 is list of header names print( test_df[[&#39;Male&#39;,&#39;Pondan&#39;]] ,&#39;\\n\\n&#39;, ## Include multiple Level0 Header test_df[&#39;Male&#39;] , &#39;\\n\\n&#39;, ## Include single Level0 Header test_df.Male ) ## Same as above ## Male Pondan ## Medium High Low Medium High Low ## row1 1 2 3 7 8 9 ## row2 11 12 13 17 18 19 ## row3 21 22 23 27 28 29 ## ## Medium High Low ## row1 1 2 3 ## row2 11 12 13 ## row3 21 22 23 ## ## Medium High Low ## row1 1 2 3 ## row2 11 12 13 ## row3 21 22 23 Using .loc[] Use .loc[ :, L0 ], where L0 is list of headers names print( test_df.loc[:, [&#39;Male&#39;,&#39;Pondan&#39;]] , &#39;\\n\\n&#39;, ## Multiple Level0 Header test_df.loc[:, &#39;Male&#39;] ) ## Single Level0 Header ## Male Pondan ## Medium High Low Medium High Low ## row1 1 2 3 7 8 9 ## row2 11 12 13 17 18 19 ## row3 21 22 23 27 28 29 ## ## Medium High Low ## row1 1 2 3 ## row2 11 12 13 ## row3 21 22 23 13.8.4.3 Selecting Level 1 Header(s) Use .loc[ :, (All, L1)], where L1 are list of headers names All = slice(None) print( test_df.loc[ : , (All, &#39;High&#39;)], &#39;\\n\\n&#39;, ## Signle L1 header test_df.loc[ : , (All, [&#39;High&#39;,&#39;Low&#39;])] ) ## Multiple L1 headers ## Male Female Pondan ## High High High ## row1 2 5 8 ## row2 12 15 18 ## row3 22 25 28 ## ## Male Female Pondan ## High Low High Low High Low ## row1 2 3 5 6 8 9 ## row2 12 13 15 16 18 19 ## row3 22 23 25 26 28 29 13.8.4.4 Select Level 0 and Level1 Headers Use .loc[ :, (L0, L1)], where L0 and L1 are list of headers names test_df.loc[ : , ([&#39;Male&#39;,&#39;Pondan&#39;], [&#39;Medium&#39;,&#39;High&#39;])] ## Male Pondan ## Medium High Medium High ## row1 1 2 7 8 ## row2 11 12 17 18 ## row3 21 22 27 28 13.8.4.5 Select single L0,L1 Header Use .loc[:, (L0, L1) ], result is a Series Use .loc[:, (L0 ,[L1])], result is a DataFrame print( test_df.loc[ : , (&#39;Female&#39;, &#39;High&#39;)], &#39;\\n\\n&#39;, test_df.loc[ : , (&#39;Female&#39;, [&#39;High&#39;])]) ## row1 5 ## row2 15 ## row3 25 ## Name: (Female, High), dtype: int64 ## ## Female ## High ## row1 5 ## row2 15 ## row3 25 13.8.5 Headers Ordering Note that columns order specifeid by [ ] selection were not respected. This can be remediated either by Sorting and rearranging. 13.8.5.1 Sort Headers Use .sort_index() on DataFrame to sort the headers. Note that when level1 is sorted, it jumble up level0 headers. test_df_sorted_l0 = test_df.sort_index(axis=1, level=0) test_df_sorted_l1 = test_df.sort_index(axis=1, level=1, ascending=False) print(test_df, &#39;\\n\\n&#39;,test_df_sorted_l0, &#39;\\n\\n&#39;, test_df_sorted_l1) ## Male Female Pondan ## Medium High Low Medium High Low Medium High Low ## row1 1 2 3 4 5 6 7 8 9 ## row2 11 12 13 14 15 16 17 18 19 ## row3 21 22 23 24 25 26 27 28 29 ## ## Female Male Pondan ## High Low Medium High Low Medium High Low Medium ## row1 5 6 4 2 3 1 8 9 7 ## row2 15 16 14 12 13 11 18 19 17 ## row3 25 26 24 22 23 21 28 29 27 ## ## Pondan Male Female Pondan Male Female Pondan Male Female ## Medium Medium Medium Low Low Low High High High ## row1 7 1 4 9 3 6 8 2 5 ## row2 17 11 14 19 13 16 18 12 15 ## row3 27 21 24 29 23 26 28 22 25 13.8.5.2 Rearranging Headers Use **.reindex()** on arrange columns in specific order. Example below shows how to control the specific order for level1 headers. cats = [&#39;Low&#39;,&#39;Medium&#39;,&#39;High&#39;] test_df.reindex(cats, level=1, axis=1) ## Male Female Pondan ## Low Medium High Low Medium High Low Medium High ## row1 3 1 2 6 4 5 9 7 8 ## row2 13 11 12 16 14 15 19 17 18 ## row3 23 21 22 26 24 25 29 27 28 13.8.6 Stacking and Unstacking df.stack() ## One Two ## myindex ## 0 X 1.1 1.11 ## Y 1.2 1.22 ## 1 X 1.1 1.11 ## Y 1.2 1.22 ## 2 X 1.1 1.11 ## Y 1.2 1.22 13.8.6.1 Stacking Columns to Rows Stacking with DataFrame.stack(level_no) is moving wide columns into row. print(&#39;Stacking Header Level 0: \\n\\n&#39;, df.stack(0), &#39;\\n\\nStacking Header Level 1: \\n\\n&#39;, df.stack(1)) ## Stacking Header Level 0: ## ## X Y ## myindex ## 0 One 1.10 1.20 ## Two 1.11 1.22 ## 1 One 1.10 1.20 ## Two 1.11 1.22 ## 2 One 1.10 1.20 ## Two 1.11 1.22 ## ## Stacking Header Level 1: ## ## One Two ## myindex ## 0 X 1.1 1.11 ## Y 1.2 1.22 ## 1 X 1.1 1.11 ## Y 1.2 1.22 ## 2 X 1.1 1.11 ## Y 1.2 1.22 13.8.7 Exploratory Analysis 13.8.7.1 Sample Data df ## One Two ## X Y X Y ## myindex ## 0 1.1 1.2 1.11 1.22 ## 1 1.1 1.2 1.11 1.22 ## 2 1.1 1.2 1.11 1.22 13.8.7.2 All Stats in One - .describe() df.describe(include=&#39;number&#39;) # default df.describe(include=&#39;object&#39;) # display for non-numeric columns df.describe(include=&#39;all&#39;) # display both numeric and non-numeric When applied to DataFrame object, describe shows all basic statistic for all numeric columns: - Count (non-NA) - Unique (for string) - Top (for string) - Frequency (for string) - Percentile - Mean - Min / Max - Standard Deviation For Numeric Columns only You can customize the percentiles requred. Notice 0.5 percentile is always there although not specified df.describe() ## One Two ## X Y X Y ## count 3.0 3.0 3.00 3.00 ## mean 1.1 1.2 1.11 1.22 ## std 0.0 0.0 0.00 0.00 ## min 1.1 1.2 1.11 1.22 ## 25% 1.1 1.2 1.11 1.22 ## 50% 1.1 1.2 1.11 1.22 ## 75% 1.1 1.2 1.11 1.22 ## max 1.1 1.2 1.11 1.22 df.describe(percentiles=[0.9,0.3,0.2,0.1]) ## One Two ## X Y X Y ## count 3.0 3.0 3.00 3.00 ## mean 1.1 1.2 1.11 1.22 ## std 0.0 0.0 0.00 0.00 ## min 1.1 1.2 1.11 1.22 ## 10% 1.1 1.2 1.11 1.22 ## 20% 1.1 1.2 1.11 1.22 ## 30% 1.1 1.2 1.11 1.22 ## 50% 1.1 1.2 1.11 1.22 ## 90% 1.1 1.2 1.11 1.22 ## max 1.1 1.2 1.11 1.22 For both Numeric and Object df.describe(include=&#39;all&#39;) ## One Two ## X Y X Y ## count 3.0 3.0 3.00 3.00 ## mean 1.1 1.2 1.11 1.22 ## std 0.0 0.0 0.00 0.00 ## min 1.1 1.2 1.11 1.22 ## 25% 1.1 1.2 1.11 1.22 ## 50% 1.1 1.2 1.11 1.22 ## 75% 1.1 1.2 1.11 1.22 ## max 1.1 1.2 1.11 1.22 13.8.7.3 min/max/mean/median df.min() # default axis=0, column-wise ## One X 1.10 ## Y 1.20 ## Two X 1.11 ## Y 1.22 ## dtype: float64 df.min(axis=1) # axis=1, row-wise ## myindex ## 0 1.1 ## 1 1.1 ## 2 1.1 ## dtype: float64 Observe, sum on string will concatenate column-wise, whereas row-wise only sum up numeric fields df.sum(0) ## One X 3.30 ## Y 3.60 ## Two X 3.33 ## Y 3.66 ## dtype: float64 df.sum(1) ## myindex ## 0 4.63 ## 1 4.63 ## 2 4.63 ## dtype: float64 13.8.8 Plotting 13.9 class: Categories 13.9.1 Creating 13.9.1.1 From List Basic (Auto Category Mapping) Basic syntax return categorical index with sequence with code 0,1,2,3… mapping to first found category In this case, low(0), high(1), medium(2) temp = [&#39;low&#39;,&#39;high&#39;,&#39;medium&#39;,&#39;high&#39;,&#39;high&#39;,&#39;low&#39;,&#39;medium&#39;,&#39;medium&#39;,&#39;high&#39;] temp_cat = pd.Categorical(temp) temp_cat ## [low, high, medium, high, high, low, medium, medium, high] ## Categories (3, object): [high, low, medium] type( temp_cat ) ## &lt;class &#39;pandas.core.arrays.categorical.Categorical&#39;&gt; Manual Category Mapping During creation, we can specify mapping of codes to category: low(0), medium(1), high(2) temp_cat = pd.Categorical(temp, categories=[&#39;low&#39;,&#39;medium&#39;,&#39;high&#39;]) temp_cat ## [low, high, medium, high, high, low, medium, medium, high] ## Categories (3, object): [low, medium, high] 13.9.1.2 From Series We can ‘add’ categorical structure into a Series. With these methods, additional property (.cat) is added as a categorical accessor Through this accessor, you gain access to various properties of the category such as .codes, .categories. But not .get_values() as the information is in the Series itself Can we manual map category ????? temp = [&#39;low&#39;,&#39;high&#39;,&#39;medium&#39;,&#39;high&#39;,&#39;high&#39;,&#39;low&#39;,&#39;medium&#39;,&#39;medium&#39;,&#39;high&#39;] temp_cat = pd.Series(temp, dtype=&#39;category&#39;) print (type(temp_cat)) # Series object ## &lt;class &#39;pandas.core.series.Series&#39;&gt; print (type(temp_cat.cat)) # Categorical Accessor ## &lt;class &#39;pandas.core.arrays.categorical.CategoricalAccessor&#39;&gt; Method below has the same result as above by using .astype(‘category’) It is useful adding category structure into existing series. temp_ser = pd.Series(temp) temp_cat = pd.Series(temp).astype(&#39;category&#39;) print (type(temp_cat)) # Series object ## &lt;class &#39;pandas.core.series.Series&#39;&gt; print (type(temp_cat.cat)) # Categorical Accessor ## &lt;class &#39;pandas.core.arrays.categorical.CategoricalAccessor&#39;&gt; temp_cat.cat.categories ## Index([&#39;high&#39;, &#39;low&#39;, &#39;medium&#39;], dtype=&#39;object&#39;) 13.9.1.3 Ordering Category temp = [&#39;low&#39;,&#39;high&#39;,&#39;medium&#39;,&#39;high&#39;,&#39;high&#39;,&#39;low&#39;,&#39;medium&#39;,&#39;medium&#39;,&#39;high&#39;] temp_cat = pd.Categorical(temp, categories=[&#39;low&#39;,&#39;medium&#39;,&#39;high&#39;], ordered=True) temp_cat ## [low, high, medium, high, high, low, medium, medium, high] ## Categories (3, object): [low &lt; medium &lt; high] temp_cat.get_values() ## array([&#39;low&#39;, &#39;high&#39;, &#39;medium&#39;, &#39;high&#39;, &#39;high&#39;, &#39;low&#39;, &#39;medium&#39;, &#39;medium&#39;, ## &#39;high&#39;], dtype=object) ## ## C:/ProgramData/Anaconda3/python.exe:1: FutureWarning: The &#39;get_values&#39; method is deprecated and will be removed in a future version temp_cat.codes ## array([0, 2, 1, 2, 2, 0, 1, 1, 2], dtype=int8) temp_cat[0] &lt; temp_cat[3] ## False 13.9.2 Properties 13.9.2.1 .categories first element’s code = 0 second element’s code = 1 third element’s code = 2 temp_cat.categories ## Index([&#39;low&#39;, &#39;medium&#39;, &#39;high&#39;], dtype=&#39;object&#39;) 13.9.2.2 .codes Codes are actual integer value stored as array. 1 represent ‘high’, temp_cat.codes ## array([0, 2, 1, 2, 2, 0, 1, 1, 2], dtype=int8) 13.9.3 Rename Category 13.9.3.1 Renamce To New Category Object .rename_categories() method return a new category object with new changed categories temp = [&#39;low&#39;,&#39;high&#39;,&#39;medium&#39;,&#39;high&#39;,&#39;high&#39;,&#39;low&#39;,&#39;medium&#39;,&#39;medium&#39;,&#39;high&#39;] new_temp_cat = temp_cat.rename_categories([&#39;sejuk&#39;,&#39;sederhana&#39;,&#39;panas&#39;]) new_temp_cat ## [sejuk, panas, sederhana, panas, panas, sejuk, sederhana, sederhana, panas] ## Categories (3, object): [sejuk &lt; sederhana &lt; panas] temp_cat # original category object categories not changed ## [low, high, medium, high, high, low, medium, medium, high] ## Categories (3, object): [low &lt; medium &lt; high] 13.9.3.2 Rename Inplace Observe the original categories had been changed using .rename() temp_cat.categories = [&#39;sejuk&#39;,&#39;sederhana&#39;,&#39;panas&#39;] temp_cat # original category object categories is changed ## [sejuk, panas, sederhana, panas, panas, sejuk, sederhana, sederhana, panas] ## Categories (3, object): [sejuk &lt; sederhana &lt; panas] 13.9.4 Adding New Category This return a new category object with added categories temp_cat_more = temp_cat.add_categories([&#39;susah&#39;,&#39;senang&#39;]) temp_cat_more ## [sejuk, panas, sederhana, panas, panas, sejuk, sederhana, sederhana, panas] ## Categories (5, object): [sejuk &lt; sederhana &lt; panas &lt; susah &lt; senang] 13.9.5 Removing Category This is not in place, hence return a new categorical object 13.9.5.1 Remove Specific Categor(ies) Elements with its category removed will become NaN temp = [&#39;low&#39;,&#39;high&#39;,&#39;medium&#39;,&#39;high&#39;,&#39;high&#39;,&#39;low&#39;,&#39;medium&#39;,&#39;medium&#39;,&#39;high&#39;] temp_cat = pd.Categorical(temp) temp_cat_removed = temp_cat.remove_categories(&#39;low&#39;) temp_cat_removed ## [NaN, high, medium, high, high, NaN, medium, medium, high] ## Categories (2, object): [high, medium] 13.9.5.2 Remove Unused Category Since categories removed are not used, there is no impact to the element print (temp_cat_more) ## [sejuk, panas, sederhana, panas, panas, sejuk, sederhana, sederhana, panas] ## Categories (5, object): [sejuk &lt; sederhana &lt; panas &lt; susah &lt; senang] temp_cat_more.remove_unused_categories() ## [sejuk, panas, sederhana, panas, panas, sejuk, sederhana, sederhana, panas] ## Categories (3, object): [sejuk &lt; sederhana &lt; panas] 13.9.6 Add and Remove Categories In One Step - Set() temp = [&#39;low&#39;,&#39;high&#39;,&#39;medium&#39;,&#39;high&#39;,&#39;high&#39;,&#39;low&#39;,&#39;medium&#39;,&#39;medium&#39;,&#39;high&#39;] temp_cat = pd.Categorical(temp, ordered=True) temp_cat ## [low, high, medium, high, high, low, medium, medium, high] ## Categories (3, object): [high &lt; low &lt; medium] temp_cat.set_categories([&#39;low&#39;,&#39;medium&#39;,&#39;sederhana&#39;,&#39;susah&#39;,&#39;senang&#39;]) ## [low, NaN, medium, NaN, NaN, low, medium, medium, NaN] ## Categories (5, object): [low &lt; medium &lt; sederhana &lt; susah &lt; senang] 13.9.7 Categorical Descriptive Analysis 13.9.7.1 At One Glance temp_cat.describe() ## counts freqs ## categories ## high 4 0.444444 ## low 2 0.222222 ## medium 3 0.333333 13.9.7.2 Frequency Count temp_cat.value_counts() ## high 4 ## low 2 ## medium 3 ## dtype: int64 13.9.7.3 Least Frequent Category, Most Frequent Category, and Most Frequent Category ( temp_cat.min(), temp_cat.max(), temp_cat.mode() ) ## (&#39;high&#39;, &#39;medium&#39;, [high] ## Categories (3, object): [high &lt; low &lt; medium]) 13.9.8 Other Methods 13.9.8.1 .get_values() Since actual value stored by categorical object are integer codes, get_values() function return values translated from *.codes** property temp_cat.get_values() #array ## array([&#39;low&#39;, &#39;high&#39;, &#39;medium&#39;, &#39;high&#39;, &#39;high&#39;, &#39;low&#39;, &#39;medium&#39;, &#39;medium&#39;, ## &#39;high&#39;], dtype=object) 13.10 Dummies get_dummies creates columns for each categories The underlying data can be string or pd.Categorical It produces a new pd.DataFrame 13.10.1 Sample Data df = pd.DataFrame ( {&#39;A&#39;: [&#39;A1&#39;, &#39;A2&#39;, &#39;A3&#39;,&#39;A1&#39;,&#39;A3&#39;,&#39;A1&#39;], &#39;B&#39;: [&#39;B1&#39;,&#39;B2&#39;,&#39;B3&#39;,&#39;B1&#39;,&#39;B1&#39;,&#39;B3&#39;], &#39;C&#39;: [&#39;C1&#39;,&#39;C2&#39;,&#39;C3&#39;,&#39;C1&#39;,np.nan,np.nan]}) df ## A B C ## 0 A1 B1 C1 ## 1 A2 B2 C2 ## 2 A3 B3 C3 ## 3 A1 B1 C1 ## 4 A3 B1 NaN ## 5 A1 B3 NaN 13.10.2 Dummies on Array-Like Data pd.get_dummies(df.A) ## A1 A2 A3 ## 0 1 0 0 ## 1 0 1 0 ## 2 0 0 1 ## 3 1 0 0 ## 4 0 0 1 ## 5 1 0 0 13.10.3 Dummies on DataFrame (multiple columns) 13.10.3.1 All Columns pd.get_dummies(df) ## A_A1 A_A2 A_A3 B_B1 B_B2 B_B3 C_C1 C_C2 C_C3 ## 0 1 0 0 1 0 0 1 0 0 ## 1 0 1 0 0 1 0 0 1 0 ## 2 0 0 1 0 0 1 0 0 1 ## 3 1 0 0 1 0 0 1 0 0 ## 4 0 0 1 1 0 0 0 0 0 ## 5 1 0 0 0 0 1 0 0 0 13.10.3.2 Selected Columns cols = [&#39;A&#39;,&#39;B&#39;] pd.get_dummies(df[cols]) ## A_A1 A_A2 A_A3 B_B1 B_B2 B_B3 ## 0 1 0 0 1 0 0 ## 1 0 1 0 0 1 0 ## 2 0 0 1 0 0 1 ## 3 1 0 0 1 0 0 ## 4 0 0 1 1 0 0 ## 5 1 0 0 0 0 1 13.10.4 Dummies with na By default, nan values are ignored pd.get_dummies(df.C) ## C1 C2 C3 ## 0 1 0 0 ## 1 0 1 0 ## 2 0 0 1 ## 3 1 0 0 ## 4 0 0 0 ## 5 0 0 0 Make NaN as a dummy variable pd.get_dummies(df.C,dummy_na=True) ## C1 C2 C3 NaN ## 0 1 0 0 0 ## 1 0 1 0 0 ## 2 0 0 1 0 ## 3 1 0 0 0 ## 4 0 0 0 1 ## 5 0 0 0 1 13.10.5 Specify Prefixes pd.get_dummies(df.A, prefix=&#39;col&#39;) ## col_A1 col_A2 col_A3 ## 0 1 0 0 ## 1 0 1 0 ## 2 0 0 1 ## 3 1 0 0 ## 4 0 0 1 ## 5 1 0 0 pd.get_dummies(df[cols], prefix=[&#39;colA&#39;,&#39;colB&#39;]) ## colA_A1 colA_A2 colA_A3 colB_B1 colB_B2 colB_B3 ## 0 1 0 0 1 0 0 ## 1 0 1 0 0 1 0 ## 2 0 0 1 0 0 1 ## 3 1 0 0 1 0 0 ## 4 0 0 1 1 0 0 ## 5 1 0 0 0 0 1 13.10.6 Dropping First Column Dummies cause colinearity issue for regression as it has redundant column. Dropping a column does not loose any information technically pd.get_dummies(df[cols],drop_first=True) ## A_A2 A_A3 B_B2 B_B3 ## 0 0 0 0 0 ## 1 1 0 1 0 ## 2 0 1 0 1 ## 3 0 0 0 0 ## 4 0 1 0 0 ## 5 0 0 0 1 13.11 GroupBy Aggretation and summarization require creating DataFrameGroupBy object from existing DataFrame The GroupBy object is a very flexible abstraction. In many ways, you can simply treat it as if it’s a collection of DataFrames, and it does the difficult things under the hood company = pd.read_csv(&#39;data/company.csv&#39;) company.head() ## Company Department Name Age Salary Birthdate ## 0 C1 D1 Yong 45 15000 1/1/1970 ## 1 C1 D1 Chew 35 12000 2/1/1980 ## 2 C1 D2 Lim 34 8000 2/19/1977 ## 3 C1 D3 Jessy 23 2500 3/15/1990 ## 4 C1 D3 Hoi Ming 55 25000 4/15/1987 13.11.1 Creating Groups com_grp = company.groupby([&#39;Company&#39;,&#39;Department&#39;]) com_grp ## &lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000000008CF1F28&gt; 13.11.2 Properties 13.11.2.1 Number of Groups Created com_grp.ngroups ## 9 13.11.2.2 Row Numbers Associated For Each Group com_grp.groups # return Dictionary ## {(&#39;C1&#39;, &#39;D1&#39;): Int64Index([0, 1], dtype=&#39;int64&#39;), (&#39;C1&#39;, &#39;D2&#39;): Int64Index([2], dtype=&#39;int64&#39;), (&#39;C1&#39;, &#39;D3&#39;): Int64Index([3, 4, 5], dtype=&#39;int64&#39;), (&#39;C2&#39;, &#39;D1&#39;): Int64Index([6], dtype=&#39;int64&#39;), (&#39;C2&#39;, &#39;D2&#39;): Int64Index([7, 8, 9], dtype=&#39;int64&#39;), (&#39;C2&#39;, &#39;D3&#39;): Int64Index([10, 11, 12], dtype=&#39;int64&#39;), (&#39;C3&#39;, &#39;D1&#39;): Int64Index([14], dtype=&#39;int64&#39;), (&#39;C3&#39;, &#39;D2&#39;): Int64Index([15], dtype=&#39;int64&#39;), (&#39;C3&#39;, &#39;D3&#39;): Int64Index([13, 16, 17], dtype=&#39;int64&#39;)} 13.11.3 Methods 13.11.3.1 Number of Rows In Each Group com_grp.size() # return panda Series object ## Company Department ## C1 D1 2 ## D2 1 ## D3 3 ## C2 D1 1 ## D2 3 ## D3 3 ## C3 D1 1 ## D2 1 ## D3 3 ## dtype: int64 13.11.3.2 Valid (not Null) Data Count For Each Fields In The Group com_grp.count() # return panda DataFrame object ## Name Age Salary Birthdate ## Company Department ## C1 D1 2 2 2 2 ## D2 1 1 1 1 ## D3 3 3 3 3 ## C2 D1 1 1 1 1 ## D2 3 3 3 3 ## D3 3 3 3 3 ## C3 D1 1 1 1 1 ## D2 1 1 1 1 ## D3 3 3 3 3 13.11.4 Retrieve Rows All row retrieval operations return a dataframe 13.11.4.1 Retrieve N Rows For Each Groups Example below retrieve 2 rows from each group com_grp.head(2) ## Company Department Name Age Salary Birthdate ## 0 C1 D1 Yong 45 15000 1/1/1970 ## 1 C1 D1 Chew 35 12000 2/1/1980 ## 2 C1 D2 Lim 34 8000 2/19/1977 ## 3 C1 D3 Jessy 23 2500 3/15/1990 ## 4 C1 D3 Hoi Ming 55 25000 4/15/1987 ## .. ... ... ... ... ... ... ## 11 C2 D3 Jeannie 30 12500 12/31/1980 ## 13 C3 D3 Chang 32 7900 7/26/1973 ## 14 C3 D1 Ong 44 17500 8/21/1980 ## 15 C3 D2 Lily 41 15300 7/17/1990 ## 16 C3 D3 Sally 54 21000 7/19/1968 ## ## [14 rows x 6 columns] 13.11.4.2 Retrieve Rows In One Specific Group com_grp.get_group((&#39;C1&#39;,&#39;D3&#39;)) ## Company Department Name Age Salary Birthdate ## 3 C1 D3 Jessy 23 2500 3/15/1990 ## 4 C1 D3 Hoi Ming 55 25000 4/15/1987 ## 5 C1 D3 Sui Wei 56 3000 6/15/1990 13.11.4.3 Retrieve n-th Row From Each Group Row number is 0-based com_grp.nth(-1) # retireve last row from each group ## Name Age Salary Birthdate ## Company Department ## C1 D1 Chew 35 12000 2/1/1980 ## D2 Lim 34 8000 2/19/1977 ## D3 Sui Wei 56 3000 6/15/1990 ## C2 D1 Anne 18 400 7/15/1997 ## D2 Jimmy 46 14000 10/31/1988 ## D3 Bernard 29 9800 12/1/1963 ## C3 D1 Ong 44 17500 8/21/1980 ## D2 Lily 41 15300 7/17/1990 ## D3 Esther 37 13500 3/16/1969 13.11.5 Iteration DataFrameGroupBy object can be thought as a collection of named groups def print_groups (g): for name,group in g: print (name) print (group[:2]) print_groups (com_grp) ## (&#39;C1&#39;, &#39;D1&#39;) ## Company Department Name Age Salary Birthdate ## 0 C1 D1 Yong 45 15000 1/1/1970 ## 1 C1 D1 Chew 35 12000 2/1/1980 ## (&#39;C1&#39;, &#39;D2&#39;) ## Company Department Name Age Salary Birthdate ## 2 C1 D2 Lim 34 8000 2/19/1977 ## (&#39;C1&#39;, &#39;D3&#39;) ## Company Department Name Age Salary Birthdate ## 3 C1 D3 Jessy 23 2500 3/15/1990 ## 4 C1 D3 Hoi Ming 55 25000 4/15/1987 ## (&#39;C2&#39;, &#39;D1&#39;) ## Company Department Name Age Salary Birthdate ## 6 C2 D1 Anne 18 400 7/15/1997 ## (&#39;C2&#39;, &#39;D2&#39;) ## Company Department Name Age Salary Birthdate ## 7 C2 D2 Deborah 30 8600 8/15/1984 ## 8 C2 D2 Nikalus 51 12000 9/18/2000 ## (&#39;C2&#39;, &#39;D3&#39;) ## Company Department Name Age Salary Birthdate ## 10 C2 D3 Michael 38 17000 11/30/1997 ## 11 C2 D3 Jeannie 30 12500 12/31/1980 ## (&#39;C3&#39;, &#39;D1&#39;) ## Company Department Name Age Salary Birthdate ## 14 C3 D1 Ong 44 17500 8/21/1980 ## (&#39;C3&#39;, &#39;D2&#39;) ## Company Department Name Age Salary Birthdate ## 15 C3 D2 Lily 41 15300 7/17/1990 ## (&#39;C3&#39;, &#39;D3&#39;) ## Company Department Name Age Salary Birthdate ## 13 C3 D3 Chang 32 7900 7/26/1973 ## 16 C3 D3 Sally 54 21000 7/19/1968 com_grp ## &lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000000008CF1F28&gt; 13.11.6 Apply Aggregate Functions to Groups Aggregate apply functions to columns in every groups, and return a summary data for each group 13.11.6.1 Apply One Function to One or More Columns com_grp[&#39;Age&#39;].sum() ## Company Department ## C1 D1 80 ## D2 34 ## D3 134 ## C2 D1 18 ## D2 127 ## D3 97 ## C3 D1 44 ## D2 41 ## D3 123 ## Name: Age, dtype: int64 com_grp[[&#39;Age&#39;,&#39;Salary&#39;]].sum() ## Age Salary ## Company Department ## C1 D1 80 27000 ## D2 34 8000 ## D3 134 30500 ## C2 D1 18 400 ## D2 127 34600 ## D3 97 39300 ## C3 D1 44 17500 ## D2 41 15300 ## D3 123 42400 13.11.6.2 Apply One or More Functions To All Columns com_grp.agg(np.mean) ## Age Salary ## Company Department ## C1 D1 40.000000 13500.000000 ## D2 34.000000 8000.000000 ## D3 44.666667 10166.666667 ## C2 D1 18.000000 400.000000 ## D2 42.333333 11533.333333 ## D3 32.333333 13100.000000 ## C3 D1 44.000000 17500.000000 ## D2 41.000000 15300.000000 ## D3 41.000000 14133.333333 com_grp.agg([np.mean,np.sum]) ## Age Salary ## mean sum mean sum ## Company Department ## C1 D1 40.000000 80 13500.000000 27000 ## D2 34.000000 34 8000.000000 8000 ## D3 44.666667 134 10166.666667 30500 ## C2 D1 18.000000 18 400.000000 400 ## D2 42.333333 127 11533.333333 34600 ## D3 32.333333 97 13100.000000 39300 ## C3 D1 44.000000 44 17500.000000 17500 ## D2 41.000000 41 15300.000000 15300 ## D3 41.000000 123 14133.333333 42400 13.11.6.3 Apply Different Functions To Different Columns com_grp.agg({&#39;Age&#39;:np.mean, &#39;Salary&#39;: [np.min,np.max]}) ## Age Salary ## mean amin amax ## Company Department ## C1 D1 40.000000 12000 15000 ## D2 34.000000 8000 8000 ## D3 44.666667 2500 25000 ## C2 D1 18.000000 400 400 ## D2 42.333333 8600 14000 ## D3 32.333333 9800 17000 ## C3 D1 44.000000 17500 17500 ## D2 41.000000 15300 15300 ## D3 41.000000 7900 21000 13.11.7 Transform Transform is an operation used combined with DataFrameGroupBy object transform() return a new DataFrame object grp = company.groupby(&#39;Company&#39;) grp.size() ## Company ## C1 6 ## C2 7 ## C3 5 ## dtype: int64 transform() perform a function to a group, and expands and replicate it to multiple rows according to original DataFrame grp[[&#39;Age&#39;,&#39;Salary&#39;]].transform(&#39;sum&#39;) ## Age Salary ## 0 248 65500 ## 1 248 65500 ## 2 248 65500 ## 3 248 65500 ## 4 248 65500 ## .. ... ... ## 13 208 75200 ## 14 208 75200 ## 15 208 75200 ## 16 208 75200 ## 17 208 75200 ## ## [18 rows x 2 columns] grp.transform( lambda x:x+10 ) ## Age Salary ## 0 55 15010 ## 1 45 12010 ## 2 44 8010 ## 3 33 2510 ## 4 65 25010 ## .. ... ... ## 13 42 7910 ## 14 54 17510 ## 15 51 15310 ## 16 64 21010 ## 17 47 13510 ## ## [18 rows x 2 columns] 13.12 Fundamental Analysis 13.13 Missing Data 13.13.1 What Is Considered Missing Data ? 13.13.2 Sample Data df = pd.DataFrame( np.random.randn(5, 3), index =[&#39;a&#39;, &#39;c&#39;, &#39;e&#39;, &#39;f&#39;, &#39;h&#39;], columns =[&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]) df[&#39;four&#39;] = &#39;bar&#39; df[&#39;five&#39;] = df[&#39;one&#39;] &gt; 0 #df df.reindex([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;, &#39;h&#39;]) ## one two three four five ## a -0.155909 -0.501790 0.235569 bar False ## b NaN NaN NaN NaN NaN ## c -1.763605 -1.095862 -1.087766 bar False ## d NaN NaN NaN NaN NaN ## e -0.305170 -0.473748 -0.200595 bar False ## f 0.355197 0.689518 0.410590 bar True ## g NaN NaN NaN NaN NaN ## h -0.564978 0.599391 -0.162936 bar False How Missing Data For Each Column ? df.count() ## one 5 ## two 5 ## three 5 ## four 5 ## five 5 ## dtype: int64 len(df.index) - df.count() ## one 0 ## two 0 ## three 0 ## four 0 ## five 0 ## dtype: int64 df.isnull() ## one two three four five ## a False False False False False ## c False False False False False ## e False False False False False ## f False False False False False ## h False False False False False df.describe() ## one two three ## count 5.000000 5.000000 5.000000 ## mean -0.486893 -0.156498 -0.161028 ## std 0.788635 0.772882 0.579752 ## min -1.763605 -1.095862 -1.087766 ## 25% -0.564978 -0.501790 -0.200595 ## 50% -0.305170 -0.473748 -0.162936 ## 75% -0.155909 0.599391 0.235569 ## max 0.355197 0.689518 0.410590 "],
["matplotlib-1.html", "14 matplotlib 14.1 Library 14.2 Sample Data 14.3 MATLAB-like API 14.4 Object-Oriented API 14.5 Histogram 14.6 Scatter Plot 14.7 Bar Chart", " 14 matplotlib from IPython.core.display import display, HTML display(HTML(&quot;&lt;style&gt;.container { width:75% !important; margin-left:350px; }&lt;/style&gt;&quot;)) #%matplotlib inline ## &lt;IPython.core.display.HTML object&gt; import numpy as np import pandas as pd import matplotlib.pyplot as plt import math import seaborn as sns pd.set_option( &#39;display.notebook_repr_html&#39;, False) # render Series and DataFrame as text, not HTML pd.set_option( &#39;display.max_column&#39;, 10) # number of columns pd.set_option( &#39;display.max_rows&#39;, 10) # number of rows pd.set_option( &#39;display.width&#39;, 90) # number of characters per row 14.1 Library import matplotlib import matplotlib.pyplot as plt from plydata import define, query, select, group_by, summarize, arrange, head, rename import plotnine from plotnine import * import os os.environ[&#39;QT_QPA_PLATFORM_PLUGIN_PATH&#39;] = &quot;C:\\ProgramData\\Anaconda3\\Library\\plugins\\platforms&quot; 14.2 Sample Data This chapter uses the sample data generate with below code. The idea is to simulate two categorical-alike feature, and two numeric value feature: com is random character between ?C1?, ?C2? and ?C3? dept is random character between ?D1?, ?D2?, ?D3?, ?D4? and ?D5? grp is random character with randomly generated ?G1?, ?G2? value1 represents numeric value, normally distributed at mean 50 value2 is numeric value, normally distributed at mean 25 n = 200 comp = [&#39;C&#39; + i for i in np.random.randint( 1,4, size = n).astype(str)] # 3x Company dept = [&#39;D&#39; + i for i in np.random.randint( 1,6, size = n).astype(str)] # 5x Department grp = [&#39;G&#39; + i for i in np.random.randint( 1,3, size = n).astype(str)] # 2x Groups value1 = np.random.normal( loc=50 , scale=5 , size = n) value2 = np.random.normal( loc=20 , scale=3 , size = n) value3 = np.random.normal( loc=5 , scale=30 , size = n) mydf = pd.DataFrame({ &#39;comp&#39;:comp, &#39;dept&#39;:dept, &#39;grp&#39;: grp, &#39;value1&#39;:value1, &#39;value2&#39;:value2, &#39;value3&#39;:value3 }) mydf.head() ## comp dept grp value1 value2 value3 ## 0 C3 D1 G2 58.001073 20.797796 4.565181 ## 1 C2 D4 G2 47.122054 20.691133 36.084163 ## 2 C3 D5 G1 49.497686 15.715835 7.232685 ## 3 C1 D3 G2 50.129623 26.790262 -6.875583 ## 4 C3 D3 G1 51.620032 23.704337 11.909618 mydf.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 200 entries, 0 to 199 ## Data columns (total 6 columns): ## comp 200 non-null object ## dept 200 non-null object ## grp 200 non-null object ## value1 200 non-null float64 ## value2 200 non-null float64 ## value3 200 non-null float64 ## dtypes: float64(3), object(3) ## memory usage: 9.5+ KB 14.3 MATLAB-like API The good thing about the pylab MATLAB-style API is that it is easy to get started with if you are familiar with MATLAB, and it has a minumum of coding overhead for simple plots. However, I’d encourrage not using the MATLAB compatible API for anything but the simplest figures. Instead, I recommend learning and using matplotlib’s object-oriented plotting API. It is remarkably powerful. For advanced figures with subplots, insets and other components it is very nice to work with. 14.3.1 Sample Data # Sample Data x = np.linspace(0,5,10) y = x ** 2 14.3.2 Single Plot #plt.figure() #plt.xlabel(&#39;x&#39;) #plt.ylabel(&#39;y&#39;) #plt.plot(x,y,&#39;red&#39;) #plt.title(&#39;My Good Data&#39;) #plt.show() 14.3.3 Multiple Subplots Each call lto subplot() will create a new container for subsequent plot command plt.figure() plt.subplot(1,2,1) # 1 row, 2 cols, at first box plt.plot(x,y,&#39;r--&#39;) plt.subplot(1,2,2) # 1 row, 2 cols, at second box plt.plot(y,x,&#39;g*-&#39;) plt.show() 14.4 Object-Oriented API 14.4.1 Sample Data # Sample Data x = np.linspace(0,5,10) y = x ** 2 14.4.2 Single Plot One figure, one axes fig = plt.figure() axes = fig.add_axes([0,0,1,1]) # left, bottom, width, height (range 0 to 1) axes.plot(x, y, &#39;r&#39;) axes.set_xlabel(&#39;x&#39;) axes.set_ylabel(&#39;y&#39;) axes.set_title(&#39;title&#39;) plt.show() 14.4.3 Multiple Axes In One Plot This is still considered a single plot, but with multiple axes fig = plt.figure() ax1 = fig.add_axes([0, 0, 1, 1]) # main axes ax2 = fig.add_axes([0.2, 0.5, 0.4, 0.3]) # inset axes ax1.plot(x,y,&#39;r&#39;) ax1.set_xlabel(&#39;x&#39;) ax1.set_ylabel(&#39;y&#39;) ax2.plot(y, x, &#39;g&#39;) ax2.set_xlabel(&#39;y&#39;) ax2.set_ylabel(&#39;x&#39;) ax2.set_title(&#39;insert title&#39;) plt.show() 14.4.4 Multiple Subplots One figure can contain multiple subplots Each subplot has one axes 14.4.4.1 Simple Subplots - all same size subplots() function return axes object that is iterable. Single Row Grid Single row grid means axes is an 1-D array. Hence can use for to iterate through axes fig, axes = plt.subplots( nrows=1,ncols=3 ) print (axes.shape) ## (3,) for ax in axes: ax.plot(x, y, &#39;r&#39;) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.set_title(&#39;title&#39;) ax.text(0.2,0.5,&#39;One&#39;) plt.show() Multiple Row Grid Multile row grid means axes is an 2-D array. Hence can use two levels of for loop to iterate through each row and column fig, axes = plt.subplots(2, 3, sharex=&#39;col&#39;, sharey=&#39;row&#39;) print (axes.shape) ## (2, 3) for i in range(axes.shape[0]): for j in range(axes.shape[1]): axes[i, j].text(0.5, 0.5, str((i, j)), fontsize=18, ha=&#39;center&#39;) plt.show() 14.4.4.2 Complicated Subplots - different size GridSpec specify grid size of the figure Manually specify each subplot and their relevant grid position and size plt.figure(figsize=(5,5)) grid = plt.GridSpec(2, 3, hspace=0.4, wspace=0.4) plt.subplot(grid[0, 0]) #row 0, col 0 plt.subplot(grid[0, 1:]) #row 0, col 1 to : plt.subplot(grid[1, :2]) #row 1, col 0:2 plt.subplot(grid[1, 2]); #row 1, col 2 plt.show() plt.figure(figsize=(5,5)) grid = plt.GridSpec(4, 4, hspace=0.8, wspace=0.4) plt.subplot(grid[:3, 0]) # row 0:3, col 0 plt.subplot(grid[:3, 1: ]) # row 0:3, col 1: plt.subplot(grid[3, 1: ]); # row 3, col 1: plt.show() -1 means last row or column plt.figure(figsize=(6,6)) grid = plt.GridSpec(4, 4, hspace=0.4, wspace=1.2) plt.subplot(grid[:-1, 0 ]) # row 0 till last row (not including last row), col 0 plt.subplot(grid[:-1, 1:]) # row 0 till last row (not including last row), col 1 till end plt.subplot(grid[-1, 1: ]); # row last row, col 1 till end plt.show() 14.4.5 Figure Customization 14.4.5.1 Avoid Overlap - Use tight_layout() Sometimes when the figure size is too small, plots will overlap each other. - tight_layout() will introduce extra white space in between the subplots to avoid overlap. - The figure became wider. fig, axes = plt.subplots( nrows=1,ncols=2) for ax in axes: ax.plot(x, y, &#39;r&#39;) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.set_title(&#39;title&#39;) fig.tight_layout() # adjust the positions of axes so that there is no overlap plt.show() 14.4.5.2 Avoid Overlap - Change Figure Size fig, axes = plt.subplots( nrows=1,ncols=2,figsize=(12,3)) for ax in axes: ax.plot(x, y, &#39;r&#39;) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.set_title(&#39;title&#39;) plt.show() 14.4.5.3 Text Within Figure fig = plt.figure() fig.text(0.5, 0.5, &#39;This Is A Sample&#39;,fontsize=18, ha=&#39;center&#39;); axes = fig.add_axes([0,0,1,1]) # left, bottom, width, height (range 0 to 1) plt.show() 14.4.6 Axes Customization 14.4.6.1 Y-Axis Limit fig = plt.figure() fig.add_axes([0,0,1,1], ylim=(-2,5)); plt.show() 14.4.6.2 Text Within Axes fig, ax = plt.subplots(2, 3, sharex=&#39;col&#39;, sharey=&#39;row&#39;) for i in range(2): for j in range(3): ax[i, j].text(0.5, 0.5, str((i, j)), fontsize=18, ha=&#39;center&#39;) plt.show() plt.text(0.5, 0.5, &#39;one&#39;,fontsize=18, ha=&#39;center&#39;) plt.show() 14.4.6.3 Share Y Axis Label fig, ax = plt.subplots(2, 3, sharex=&#39;col&#39;, sharey=&#39;row&#39;) # removed inner label plt.show() 14.4.6.4 Create Subplot Individually Each call lto subplot() will create a new container for subsequent plot command plt.subplot(2,4,1) plt.text(0.5, 0.5, &#39;one&#39;,fontsize=18, ha=&#39;center&#39;) plt.subplot(2,4,8) plt.text(0.5, 0.5, &#39;eight&#39;,fontsize=18, ha=&#39;center&#39;) plt.show() Iterate through subplots (ax) to populate them fig, ax = plt.subplots(2, 3, sharex=&#39;col&#39;, sharey=&#39;row&#39;) for i in range(2): for j in range(3): ax[i, j].text(0.5, 0.5, str((i, j)), fontsize=18, ha=&#39;center&#39;) plt.show() 14.5 Histogram plt.hist(mydf.value1, bins=12); ## (array([ 2., 4., 10., 16., 35., 38., 31., 27., 22., 6., 6., 3.]), array([37.09329803, 39.25349925, 41.41370047, 43.57390169, 45.73410291, ## 47.89430413, 50.05450535, 52.21470657, 54.37490779, 56.53510901, ## 58.69531023, 60.85551145, 63.01571267]), &lt;a list of 12 Patch objects&gt;) plt.show() 14.6 Scatter Plot plt.scatter(mydf.value1, mydf.value2) plt.show() 14.7 Bar Chart com_grp = mydf.groupby(&#39;comp&#39;) grpdf = com_grp[&#39;value1&#39;].sum().reset_index() grpdf ## comp value1 ## 0 C1 3988.891405 ## 1 C2 3007.230028 ## 2 C3 3017.883718 plt.bar(grpdf.comp, grpdf.value1); ## &lt;BarContainer object of 3 artists&gt; plt.xlabel(&#39;Company&#39;) plt.ylabel(&#39;Sum of Value 1&#39;) plt.show() "],
["seaborn.html", "15 seaborn 15.1 Seaborn and Matplotlib 15.2 Sample Data 15.3 Scatter Plot 15.4 Histogram 15.5 Bar Chart 15.6 Faceting 15.7 Pair Grid", " 15 seaborn 15.1 Seaborn and Matplotlib seaborn returns a matplotlib object that can be modified by the options in the pyplot module Often, these options are wrapped by seaborn and .plot() in pandas and available as arguments 15.2 Sample Data n = 100 comp = [&#39;C&#39; + i for i in np.random.randint( 1,4, size = n).astype(str)] # 3x Company dept = [&#39;D&#39; + i for i in np.random.randint( 1,4, size = n).astype(str)] # 5x Department grp = [&#39;G&#39; + i for i in np.random.randint( 1,4, size = n).astype(str)] # 2x Groups value1 = np.random.normal( loc=50 , scale=5 , size = n) value2 = np.random.normal( loc=20 , scale=3 , size = n) value3 = np.random.normal( loc=5 , scale=30 , size = n) mydf = pd.DataFrame({ &#39;comp&#39;:comp, &#39;dept&#39;:dept, &#39;grp&#39;: grp, &#39;value1&#39;:value1, &#39;value2&#39;:value2, &#39;value3&#39;:value3 }) mydf.head() ## comp dept grp value1 value2 value3 ## 0 C2 D2 G1 58.413310 17.257990 8.308861 ## 1 C2 D2 G1 40.941753 15.972926 -40.682326 ## 2 C1 D1 G2 50.870315 19.156772 11.312435 ## 3 C2 D1 G2 34.040375 20.682569 -2.149571 ## 4 C3 D2 G1 43.585637 20.666183 59.560983 15.3 Scatter Plot 15.3.1 2x Numeric sns.lmplot(x=&#39;value1&#39;, y=&#39;value2&#39;, data=mydf) ## &lt;seaborn.axisgrid.FacetGrid object at 0x000000002DB43668&gt; plt.show() sns.lmplot(x=&#39;value1&#39;, y=&#39;value2&#39;, fit_reg=False, data=mydf); #hide regresion line ## &lt;seaborn.axisgrid.FacetGrid object at 0x000000002CCF0FD0&gt; plt.show() 15.3.2 2xNumeric + 1x Categorical Use hue to represent additional categorical feature sns.lmplot(x=&#39;value1&#39;, y=&#39;value2&#39;, data=mydf, hue=&#39;comp&#39;, fit_reg=False); ## &lt;seaborn.axisgrid.FacetGrid object at 0x000000002C449FD0&gt; plt.show() 15.3.3 2xNumeric + 2x Categorical Use col and hue to represent two categorical features sns.lmplot(x=&#39;value1&#39;, y=&#39;value2&#39;, col=&#39;comp&#39;,hue=&#39;grp&#39;, fit_reg=False, data=mydf); ## &lt;seaborn.axisgrid.FacetGrid object at 0x000000002CAF2710&gt; plt.show() 15.3.4 2xNumeric + 3x Categorical Use row, col and hue to represent three categorical features sns.lmplot(x=&#39;value1&#39;, y=&#39;value2&#39;, row=&#39;dept&#39;,col=&#39;comp&#39;, hue=&#39;grp&#39;, fit_reg=False, data=mydf); ## &lt;seaborn.axisgrid.FacetGrid object at 0x000000002CA89DA0&gt; plt.show() 15.3.5 Customization 15.3.5.1 size size: height in inch for each facet sns.lmplot(x=&#39;value1&#39;, y=&#39;value2&#39;, col=&#39;comp&#39;,hue=&#39;grp&#39;, size=3,fit_reg=False, data=mydf) ## &lt;seaborn.axisgrid.FacetGrid object at 0x000000002ED44C18&gt; ## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\regression.py:546: UserWarning: The `size` paramter has been renamed to `height`; please update your code. ## warnings.warn(msg, UserWarning) ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## fig, axes = plt.subplots(nrow, ncol, **kwargs) plt.show() Observe that even size is very large, lmplot will fit (shrink) everything into one row by deafult. See example below. sns.lmplot(x=&#39;value1&#39;, y=&#39;value2&#39;, col=&#39;comp&#39;,hue=&#39;grp&#39;, size=5,fit_reg=False, data=mydf) ## &lt;seaborn.axisgrid.FacetGrid object at 0x000000002EDB0AC8&gt; plt.show() 15.3.5.2 col_wrap To avoid lmplot from shrinking the chart, we use col_wrap=&lt;col_number to wrap the output. Compare the size (height of each facet) with the above without col_wrap. Below chart is larger. sns.lmplot(x=&#39;value1&#39;, y=&#39;value2&#39;, col=&#39;comp&#39;,hue=&#39;grp&#39;, size=5, col_wrap=2, fit_reg=False, data=mydf) ## &lt;seaborn.axisgrid.FacetGrid object at 0x000000002C7E64A8&gt; ## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\axisgrid.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## fig = plt.figure(figsize=figsize) plt.show() 15.4 Histogram seaborn.distplot( a, # Series, 1D Array or List bins=None, hist=True, rug = False, vertical=False ) 15.4.1 1x Numeric sns.distplot(mydf.value1) plt.show() sns.distplot(mydf.value1,hist=True,rug=True,vertical=True, bins=30,color=&#39;g&#39;) plt.show() 15.5 Bar Chart com_grp = mydf.groupby(&#39;comp&#39;) grpdf = com_grp[&#39;value1&#39;].sum().reset_index() grpdf ## comp value1 ## 0 C1 1343.247772 ## 1 C2 1871.515883 ## 2 C3 1783.986137 15.5.1 1x Categorical, 1x Numeric sns.barplot(x=&#39;comp&#39;,y=&#39;value1&#39;,data=grpdf) plt.show() 15.5.2 Customization 15.5.2.1 Ordering sns.barplot(x=&#39;comp&#39;,y=&#39;value2&#39;, hue=&#39;grp&#39;, order=[&#39;C3&#39;,&#39;C2&#39;,&#39;C1&#39;], hue_order=[&#39;G1&#39;,&#39;G2&#39;,&#39;G3&#39;], data=mydf ) plt.show() 15.5.2.2 Flipping X/Y Axis sns.barplot(x=&#39;value2&#39;,y=&#39;comp&#39;, hue=&#39;grp&#39;,data=mydf) plt.show() 15.6 Faceting Faceting in Seaborn is a generic function that works with matplotlib various plot utility. It support matplotlib as well as seaborn plotting utility. 15.6.1 Faceting Histogram g = sns.FacetGrid(mydf, col=&quot;comp&quot;, row=&#39;dept&#39;) g.map(plt.hist, &quot;value1&quot;) ## &lt;seaborn.axisgrid.FacetGrid object at 0x000000000A7A53C8&gt; plt.show() g = sns.FacetGrid(mydf, col=&quot;comp&quot;, row=&#39;dept&#39;) g.map(plt.hist, &quot;value1&quot;) ## &lt;seaborn.axisgrid.FacetGrid object at 0x000000000BB719B0&gt; plt.show() 15.6.2 Faceting Scatter Plot g = sns.FacetGrid(mydf, col=&quot;comp&quot;, row=&#39;dept&#39;,hue=&#39;grp&#39;) g.map(plt.scatter, &quot;value1&quot;,&quot;value2&quot;,alpha=0.7); ## &lt;seaborn.axisgrid.FacetGrid object at 0x000000000BB71DD8&gt; g.add_legend() ## &lt;seaborn.axisgrid.FacetGrid object at 0x000000000BB71DD8&gt; plt.show() 15.7 Pair Grid 15.7.1 Simple Pair Grid g = sns.PairGrid(mydf, hue=&#39;comp&#39;) ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\axisgrid.py:1270: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## squeeze=False) g.map(plt.scatter); ## &lt;seaborn.axisgrid.PairGrid object at 0x000000002EAF2AC8&gt; g.add_legend() ## &lt;seaborn.axisgrid.PairGrid object at 0x000000002EAF2AC8&gt; plt.show() 15.7.2 Different Diag and OffDiag g = sns.PairGrid(mydf, hue=&#39;comp&#39;) g.map_diag(plt.hist, bins=15) ## &lt;seaborn.axisgrid.PairGrid object at 0x00000000228377F0&gt; g.map_offdiag(plt.scatter) ## &lt;seaborn.axisgrid.PairGrid object at 0x00000000228377F0&gt; g.add_legend() ## &lt;seaborn.axisgrid.PairGrid object at 0x00000000228377F0&gt; plt.show() "],
["plotnine.html", "16 plotnine 16.1 Histogram 16.2 Scatter Plot 16.3 Line Chart 16.4 Bar Chart", " 16 plotnine 16.1 Histogram 16.1.1 1xNumeric plotnine.ggplot( dataframe, aex(x=&#39;colName&#39;)) + geom_histogram( bins=10 ) plotnine.ggplot( dataframe, aex(x=&#39;colName&#39;)) + geom_histogram( binwidth=? ) plotnine.options.figure_size = (3, 3) ggplot(mydf, aes(x=&#39;value1&#39;)) + geom_histogram() # default bins = 10 ## &lt;ggplot: (12311542)&gt; ## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:93: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## if cbook.iterable(self.breaks) and cbook.iterable(self.labels): ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\stats\\stat_bin.py:93: UserWarning: &#39;stat_bin()&#39; using &#39;bins = 10&#39;. Pick better value with &#39;binwidth&#39;. ## warn(msg.format(params[&#39;bins&#39;])) ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\ggplot.py:367: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## figure = plt.figure() ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\utils.py:553: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## return cbook.iterable(var) and not is_string(var) ggplot(mydf, aes(x=&#39;value1&#39;)) + geom_histogram(bins = 15) ## &lt;ggplot: (-9223372036805580754)&gt; ## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:93: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## if cbook.iterable(self.breaks) and cbook.iterable(self.labels): ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\ggplot.py:367: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## figure = plt.figure() ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\utils.py:553: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## return cbook.iterable(var) and not is_string(var) ggplot(mydf, aes(x=&#39;value1&#39;)) + geom_histogram(binwidth = 3) ## &lt;ggplot: (-9223372036807204885)&gt; ## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:93: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## if cbook.iterable(self.breaks) and cbook.iterable(self.labels): ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\ggplot.py:367: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## figure = plt.figure() ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\utils.py:553: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## return cbook.iterable(var) and not is_string(var) 16.1.2 1xNumeric + 1xCategorical plotnine.ggplot( dataframe, aes(x=&#39;colName&#39;), fill=&#39;categorical-alike-colName&#39;) + geom_histogram() ggplot(mydf, aes(x=&#39;value1&#39;, fill=&#39;grp&#39;)) + geom_histogram(bins=15) ## &lt;ggplot: (-9223372036805826118)&gt; ## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:93: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## if cbook.iterable(self.breaks) and cbook.iterable(self.labels): ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\ggplot.py:367: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## figure = plt.figure() ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\utils.py:553: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## return cbook.iterable(var) and not is_string(var) 16.2 Scatter Plot 16.2.1 2x Numeric ggplot(mydf, aes(x=&#39;value1&#39;,y=&#39;value2&#39;)) + geom_point() ## &lt;ggplot: (-9223372036807760503)&gt; ## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:93: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## if cbook.iterable(self.breaks) and cbook.iterable(self.labels): ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\ggplot.py:367: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## figure = plt.figure() ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\utils.py:553: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## return cbook.iterable(var) and not is_string(var) 16.2.2 2x Numeric + 1x Categorical ggplot( DataFrame, aes(x=&#39;colName1&#39;,y=&#39;colName2&#39;)) + geom_point( aes( color=&#39;categorical-alike-colName&#39;, size=&#39;numberColName&#39; )) ggplot(mydf, aes(x=&#39;value1&#39;,y=&#39;value2&#39;)) + geom_point(aes(color=&#39;grp&#39;)) ## &lt;ggplot: (-9223372036843873308)&gt; ## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:93: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## if cbook.iterable(self.breaks) and cbook.iterable(self.labels): ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\ggplot.py:367: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## figure = plt.figure() ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\utils.py:553: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## return cbook.iterable(var) and not is_string(var) ggplot(mydf, aes(x=&#39;value1&#39;,y=&#39;value2&#39;,color=&#39;grp&#39;)) + geom_point() ## &lt;ggplot: (-9223372036843734549)&gt; ## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:93: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## if cbook.iterable(self.breaks) and cbook.iterable(self.labels): ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\ggplot.py:367: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## figure = plt.figure() ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\utils.py:553: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## return cbook.iterable(var) and not is_string(var) ggplot(mydf, aes(x=&#39;value1&#39;,y=&#39;value2&#39;)) + \\ geom_point(aes( color=&#39;grp&#39; )) ## &lt;ggplot: (49170415)&gt; ## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:93: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## if cbook.iterable(self.breaks) and cbook.iterable(self.labels): ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\ggplot.py:367: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## figure = plt.figure() ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\utils.py:553: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## return cbook.iterable(var) and not is_string(var) 16.2.3 2x Numeric + 1x Numeric + 1x Categorical ggplot(mydf, aes(x=&#39;value1&#39;,y=&#39;value2&#39;)) + \\ geom_point(aes( color=&#39;grp&#39;, size=&#39;value3&#39; )) ## &lt;ggplot: (-9223372036843742972)&gt; ## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:93: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## if cbook.iterable(self.breaks) and cbook.iterable(self.labels): ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\ggplot.py:367: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## figure = plt.figure() ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\utils.py:553: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## return cbook.iterable(var) and not is_string(var) 16.2.4 Overlay Smooth Line ggplot(mydf, aes(x=&#39;value1&#39;, y=&#39;value2&#39;)) + \\ geom_point() + \\ geom_smooth() # default method=&#39;loess&#39; ## &lt;ggplot: (-9223372036805620750)&gt; ## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:93: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## if cbook.iterable(self.breaks) and cbook.iterable(self.labels): ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\stats\\smoothers.py:146: UserWarning: Confidence intervals are not yet implementedfor lowess smoothings. ## warnings.warn(&quot;Confidence intervals are not yet implemented&quot; ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\ggplot.py:367: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## figure = plt.figure() ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\utils.py:553: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## return cbook.iterable(var) and not is_string(var) ggplot(mydf, aes(x=&#39;value1&#39;, y=&#39;value2&#39;,fill=&#39;grp&#39;)) + \\ geom_point() + \\ geom_smooth( se=True, color=&#39;red&#39;, method=&#39;lm&#39;, level=0.75) ## &lt;ggplot: (49242916)&gt; ## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:93: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## if cbook.iterable(self.breaks) and cbook.iterable(self.labels): ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead. ## return ptp(axis=axis, out=out, **kwargs) ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\ggplot.py:367: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## figure = plt.figure() ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\utils.py:553: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## return cbook.iterable(var) and not is_string(var) 16.3 Line Chart 16.3.1 2x Numeric Data ggplot (mydf.head(15), aes(x=&#39;value1&#39;, y=&#39;value2&#39;)) + geom_line() ## &lt;ggplot: (11444039)&gt; ## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:93: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## if cbook.iterable(self.breaks) and cbook.iterable(self.labels): ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\ggplot.py:367: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## figure = plt.figure() ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\utils.py:553: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## return cbook.iterable(var) and not is_string(var) 16.3.2 1x Numeric, 1x Categorical ggplot (mydf.head(15), aes(x=&#39;dept&#39;, y=&#39;value1&#39;)) + geom_line() ## &lt;ggplot: (11478974)&gt; ## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:93: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## if cbook.iterable(self.breaks) and cbook.iterable(self.labels): ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\ggplot.py:367: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## figure = plt.figure() ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\utils.py:553: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## return cbook.iterable(var) and not is_string(var) ggplot (mydf.head(30), aes(x=&#39;dept&#39;, y=&#39;value1&#39;)) + geom_line( aes(group=1)) ## &lt;ggplot: (-9223372036807894868)&gt; ## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\layer.py:517: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## return not cbook.iterable(value) and (cbook.is_numlike(value) or ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\layer.py:517: MatplotlibDeprecationWarning: ## The is_numlike function was deprecated in Matplotlib 3.0 and will be removed in 3.2. Use isinstance(..., numbers.Number) instead. ## return not cbook.iterable(value) and (cbook.is_numlike(value) or ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:93: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## if cbook.iterable(self.breaks) and cbook.iterable(self.labels): ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\ggplot.py:367: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## figure = plt.figure() ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\utils.py:553: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## return cbook.iterable(var) and not is_string(var) 16.3.3 2x Numeric, 1x Categorical ggplot (mydf.head(15), aes(x=&#39;value1&#39;, y=&#39;value2&#39;)) + geom_line( aes(color=&#39;grp&#39;),size=2) ## &lt;ggplot: (-9223372036842577134)&gt; ## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:93: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## if cbook.iterable(self.breaks) and cbook.iterable(self.labels): ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\ggplot.py:367: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## figure = plt.figure() ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\utils.py:553: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## return cbook.iterable(var) and not is_string(var) 16.4 Bar Chart 16.4.0.1 1x Categorical Single categorical variable produces frequency chart. tmpdf = mydf.groupby([&#39;comp&#39;],as_index=False).count() tmpdf ## comp dept grp value1 value2 value3 ## 0 C1 27 27 27 27 27 ## 1 C2 37 37 37 37 37 ## 2 C3 36 36 36 36 36 tmpdf.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## Int64Index: 3 entries, 0 to 2 ## Data columns (total 6 columns): ## comp 3 non-null object ## dept 3 non-null int64 ## grp 3 non-null int64 ## value1 3 non-null int64 ## value2 3 non-null int64 ## value3 3 non-null int64 ## dtypes: int64(5), object(1) ## memory usage: 168.0+ bytes ggplot (tmpdf, aes(x=&#39;comp&#39;, y=&#39;grp&#39;)) +geom_col() ## &lt;ggplot: (12217885)&gt; ## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:93: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## if cbook.iterable(self.breaks) and cbook.iterable(self.labels): ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\ggplot.py:367: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). ## figure = plt.figure() ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotnine\\utils.py:553: MatplotlibDeprecationWarning: ## The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. ## return cbook.iterable(var) and not is_string(var) "],
["sklearn.html", "17 sklearn 17.1 Setup (hidden) 17.2 The Library 17.3 Model Fitting 17.4 Model Tuning 17.5 High Level ML Process 17.6 Built-in Datasets 17.7 Train Test Data Splitting 17.8 Polynomial Transform 17.9 Imputation of Missing Data 17.10 Scaling 17.11 Pipeline 17.12 Cross Validation", " 17 sklearn This is a machine learning library. 17.1 Setup (hidden) from IPython.core.display import display, HTML display(HTML(&quot;&lt;style&gt;.container { width:75% !important; margin-left:350px; }&lt;/style&gt;&quot;)) #matplotlib inline ## &lt;IPython.core.display.HTML object&gt; import numpy as np import pandas as pd import matplotlib.pyplot as plt import math pd.set_option( &#39;display.notebook_repr_html&#39;, False) # render Series and DataFrame as text, not HTML pd.set_option( &#39;display.max_column&#39;, 10) # number of columns pd.set_option( &#39;display.max_rows&#39;, 10) # number of rows pd.set_option( &#39;display.width&#39;, 90) # number of characters per row import matplotlib.pyplot as plt 17.2 The Library sklearn does not automatically import its subpackages. Therefore all subpakcages must be specifically loaded before use. # Sample Data from sklearn import datasets # Model Selection from sklearn.model_selection import train_test_split from sklearn.model_selection import KFold from sklearn.model_selection import LeaveOneOut from sklearn.model_selection import cross_validate # Preprocessing from sklearn.preprocessing import Imputer ## Error in py_call_impl(callable, dots$args, dots$keywords): ImportError: cannot import name &#39;Imputer&#39; from &#39;sklearn.preprocessing&#39; (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\__init__.py) ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; from sklearn.preprocessing import MinMaxScaler from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import Normalizer from sklearn.preprocessing import PolynomialFeatures # Model and Pipeline from sklearn.linear_model import LinearRegression,Lasso from sklearn.pipeline import make_pipeline # Measurement from sklearn.metrics import * import statsmodels.formula.api as smf 17.3 Model Fitting split 17.3.1 Underfitting The model does not fit the training data and therefore misses the trends in the data The model cannot be generalized to new data, this is usually the result of a very simple model (not enough predictors/independent variables) The model will have poor predictive ability For example, we fit a linear model (like linear regression) to data that is not linear 17.3.2 Overfitting The model has trained ?too well? and is now, well, fit too closely to the training dataset The model is too complex (i.e. too many features/variables compared to the number of observations) The model will be very accurate on the training data but will probably be very not accurate on untrained or new data The model is not generalized (or not AS generalized), meaning you can generalize the results The model learns or describes the ?noise? in the training data instead of the actual relationships between variables in the data 17.3.3 Just Right It is worth noting the underfitting is not as prevalent as overfitting Nevertheless, we want to avoid both of those problems in data analysis We want to find the middle ground between under and overfitting our model 17.4 Model Tuning A highly complex model tend to overfit A too flexible model tend to underfit Complexity can be reduced by: - Less features - Less degree of polynomial features - Apply generalization (tuning hyperparameters) split 17.5 High Level ML Process split 17.6 Built-in Datasets sklearn included some popular datasets to play with Each dataset is of type Bunch. It has useful data (array) in the form of properties: - keys (display all data availabe within the dataset) - data (common) - target (common) - DESCR (common) - feature_names (some dataset) - target_names (some dataset) - images (some dataset) 17.6.1 diabetes (regression) 17.6.1.1 Load Dataset diabetes = datasets.load_diabetes() print (type(diabetes)) ## &lt;class &#39;sklearn.utils.Bunch&#39;&gt; 17.6.1.2 keys diabetes.keys() ## dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;data_filename&#39;, &#39;target_filename&#39;]) 17.6.1.3 Features and Target .data = features - two dimension array .target = target - one dimension array print (type(diabetes.data)) ## &lt;class &#39;numpy.ndarray&#39;&gt; print (type(diabetes.target)) ## &lt;class &#39;numpy.ndarray&#39;&gt; print (diabetes.data.shape) ## (442, 10) print (diabetes.target.shape) ## (442,) 17.6.1.4 Load with X,y (Convenient Method) using return_X_y = True, data is loaded into X, target is loaded into y X,y = datasets.load_diabetes(return_X_y=True) print (X.shape) ## (442, 10) print (y.shape) ## (442,) 17.6.2 digits (Classification) This is a copy of the test set of the UCI ML hand-written digits datasets digits = datasets.load_digits() print (type(digits)) ## &lt;class &#39;sklearn.utils.Bunch&#39;&gt; print (type(digits.data)) ## &lt;class &#39;numpy.ndarray&#39;&gt; digits.keys() ## dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;images&#39;, &#39;DESCR&#39;]) digits.target_names ## array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 17.6.2.1 data digits.data.shape # features ## (1797, 64) digits.target.shape # target ## (1797,) 17.6.2.2 Images images is 3 dimensional array There are 1797 samples, each sample is 8x8 pixels digits.images.shape ## (1797, 8, 8) type(digits.images) ## &lt;class &#39;numpy.ndarray&#39;&gt; Each element represent the data that make its target print (digits.target[100]) ## 4 print (digits.images[100]) ## [[ 0. 0. 0. 2. 13. 0. 0. 0.] ## [ 0. 0. 0. 8. 15. 0. 0. 0.] ## [ 0. 0. 5. 16. 5. 2. 0. 0.] ## [ 0. 0. 15. 12. 1. 16. 4. 0.] ## [ 0. 4. 16. 2. 9. 16. 8. 0.] ## [ 0. 0. 10. 14. 16. 16. 4. 0.] ## [ 0. 0. 0. 0. 13. 8. 0. 0.] ## [ 0. 0. 0. 0. 13. 6. 0. 0.]] plt.matshow(digits.images[100]) 17.6.2.3 Loading Into X,y (Convenient Method) X,y = datasets.load_digits(return_X_y=True) X.shape ## (1797, 64) y.shape ## (1797,) 17.6.3 iris (Classification) iris = datasets.load_iris() iris.keys() ## dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;]) 17.6.3.1 Feature Names iris.feature_names ## [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;] 17.6.3.2 target iris.target_names ## array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&lt;U10&#39;) iris.target ## array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ## 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ## 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ## 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ## 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 17.7 Train Test Data Splitting 17.7.1 Sample Data Generate 100 rows of data, with 3x features (X1,X2,X3), and one dependant variable (Y) n = 21 # number of samples I = 5 # intercept value E = np.random.randint( 1,20, n) # Error x1 = np.random.randint( 1,n+1, n) x2 = np.random.randint( 1,n+1, n) x3 = np.random.randint( 1,n+1, n) y = 0.1*x1 + 0.2*x2 + 0.3*x3 + E + I mydf = pd.DataFrame({ &#39;y&#39;:y, &#39;x1&#39;:x1, &#39;x2&#39;:x2, &#39;x3&#39;:x3 }) mydf.shape ## (21, 4) 17.7.2 One Time Split sklearn::train_test_split() has two forms: - Take one DF, split into 2 DF (most of sklearn modeling use this method - Take two DFs, split into 4 DF mydf.head() ## y x1 x2 x3 ## 0 14.4 14 7 12 ## 1 14.3 18 5 5 ## 2 26.4 14 11 6 ## 3 14.8 3 21 1 ## 4 11.5 4 13 5 17.7.2.1 Method 1: Split One Dataframe Into Two (Train &amp; Test) traindf, testdf = train_test_split( df, test_size=, random_state= ) # random_state : seed number (integer), optional # test_size : fraction of 1, 0.2 means 20% split traindf, testdf = train_test_split(mydf,test_size=0.2, random_state=25) print (len(traindf)) ## 16 print (len(testdf)) ## 5 17.7.2.2 Method 2: Split Two DataFrame (X,Y) into Four x_train/test, y_train/test x_train, x_test, y_train, y_test = train_test_split( X,Y, test_size=, random_state= ) # random_state : seed number (integer), optional # test_size : fraction of 1, 0.2 means 20% split Split DataFrame into X and Y First feature_cols = [&#39;x1&#39;,&#39;x2&#39;,&#39;x3&#39;] X = mydf[feature_cols] Y = mydf.y Then Split X/Y into x_train/test, y_train/test x_train, x_test, y_train, y_test = train_test_split( X,Y, test_size=0.2, random_state=25) print (len(x_train)) ## 16 print (len(x_test)) ## 5 17.7.3 K-Fold KFold(n_splits=3, shuffle=False, random_state=None) split suffle=False (default), meaning index number is taken continously kf = KFold(n_splits=7) for train_index, test_index in kf.split(X): print (train_index, test_index) ## [ 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [0 1 2] ## [ 0 1 2 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [3 4 5] ## [ 0 1 2 3 4 5 9 10 11 12 13 14 15 16 17 18 19 20] [6 7 8] ## [ 0 1 2 3 4 5 6 7 8 12 13 14 15 16 17 18 19 20] [ 9 10 11] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 15 16 17 18 19 20] [12 13 14] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 18 19 20] [15 16 17] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17] [18 19 20] shuffle=True kf = KFold(n_splits=7, shuffle=True) for train_index, test_index in kf.split(X): print (train_index, test_index) ## [ 0 1 2 3 4 6 7 8 9 10 11 12 13 14 15 16 17 19] [ 5 18 20] ## [ 0 1 3 4 5 6 7 8 10 11 12 13 15 16 17 18 19 20] [ 2 9 14] ## [ 0 1 2 3 4 5 6 7 8 9 11 12 14 15 16 17 18 20] [10 13 19] ## [ 1 2 3 5 6 7 8 9 10 11 12 13 14 15 17 18 19 20] [ 0 4 16] ## [ 0 2 4 5 6 7 8 9 10 12 13 14 15 16 17 18 19 20] [ 1 3 11] ## [ 0 1 2 3 4 5 6 9 10 11 12 13 14 15 16 18 19 20] [ 7 8 17] ## [ 0 1 2 3 4 5 7 8 9 10 11 13 14 16 17 18 19 20] [ 6 12 15] 17.7.4 Leave One Out For a dataset of N rows, Leave One Out will split N-1 times, each time leaving one row as test, remaning as training set. Due to the high number of test sets (which is the same as the number of samples-1) this cross-validation method can be very costly. For large datasets one should favor KFold. loo = LeaveOneOut() for train_index, test_index in loo.split(X): print (train_index, test_index) ## [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [0] ## [ 0 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [1] ## [ 0 1 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [2] ## [ 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [3] ## [ 0 1 2 3 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [4] ## [ 0 1 2 3 4 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [5] ## [ 0 1 2 3 4 5 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [6] ## [ 0 1 2 3 4 5 6 8 9 10 11 12 13 14 15 16 17 18 19 20] [7] ## [ 0 1 2 3 4 5 6 7 9 10 11 12 13 14 15 16 17 18 19 20] [8] ## [ 0 1 2 3 4 5 6 7 8 10 11 12 13 14 15 16 17 18 19 20] [9] ## [ 0 1 2 3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 19 20] [10] ## [ 0 1 2 3 4 5 6 7 8 9 10 12 13 14 15 16 17 18 19 20] [11] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 13 14 15 16 17 18 19 20] [12] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 14 15 16 17 18 19 20] [13] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 15 16 17 18 19 20] [14] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 16 17 18 19 20] [15] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17 18 19 20] [16] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 18 19 20] [17] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 19 20] [18] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 20] [19] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19] [20] X ## x1 x2 x3 ## 0 14 7 12 ## 1 18 5 5 ## 2 14 11 6 ## 3 3 21 1 ## 4 4 13 5 ## .. .. .. .. ## 16 1 3 13 ## 17 8 2 19 ## 18 15 14 9 ## 19 2 17 4 ## 20 5 9 7 ## ## [21 rows x 3 columns] 17.8 Polynomial Transform This can be used as part of feature engineering, to introduce new features for data that seems to fit with quadradic model. 17.8.1 Single Variable 17.8.1.1 Sample Data Data must be 2-D before polynomial features can be applied. Code below convert 1D array into 2D array. x = np.array([1, 2, 3, 4, 5]) X = x[:,np.newaxis] X ## array([[1], ## [2], ## [3], ## [4], ## [5]]) 17.8.1.2 Degree 1 One Degree means maintain original features. No new features is created. PolynomialFeatures(degree=1, include_bias=False).fit_transform(X) ## array([[1.], ## [2.], ## [3.], ## [4.], ## [5.]]) 17.8.1.3 Degree 2 Degree-1 original feature: x Degree-2 additional features: x^2 PolynomialFeatures(degree=2, include_bias=False).fit_transform(X) ## array([[ 1., 1.], ## [ 2., 4.], ## [ 3., 9.], ## [ 4., 16.], ## [ 5., 25.]]) 17.8.1.4 Degree 3 Degree-1 original feature: x Degree-2 additional features: x^2 Degree-3 additional features: x^3 PolynomialFeatures(degree=3, include_bias=False).fit_transform(X) ## array([[ 1., 1., 1.], ## [ 2., 4., 8.], ## [ 3., 9., 27.], ## [ 4., 16., 64.], ## [ 5., 25., 125.]]) 17.8.1.5 Degree 4 Degree-1 original feature: x Degree-2 additional features: x^2 Degree-3 additional features: x^3 Degree-3 additional features: x^4 PolynomialFeatures(degree=4, include_bias=False).fit_transform(X) ## array([[ 1., 1., 1., 1.], ## [ 2., 4., 8., 16.], ## [ 3., 9., 27., 81.], ## [ 4., 16., 64., 256.], ## [ 5., 25., 125., 625.]]) 17.8.2 Two Variables 17.8.2.1 Sample Data X = pd.DataFrame( {&#39;x1&#39;: [1, 2, 3, 4, 5 ], &#39;x2&#39;: [6, 7, 8, 9, 10]}) X ## x1 x2 ## 0 1 6 ## 1 2 7 ## 2 3 8 ## 3 4 9 ## 4 5 10 17.8.2.2 Degree 2 Degree-1 original features: x1, x2 Degree-2 additional features: x1^2, x2^2, x1:x2 PolynomialFeatures(degree=2, include_bias=False).fit_transform(X) ## array([[ 1., 6., 1., 6., 36.], ## [ 2., 7., 4., 14., 49.], ## [ 3., 8., 9., 24., 64.], ## [ 4., 9., 16., 36., 81.], ## [ 5., 10., 25., 50., 100.]]) 17.8.2.3 Degree 3 Degree-1 original features: x1, x2 Degree-2 additional features: x1^2, x2^2, x1:x2 Degree-3 additional features: x1^3, x2^3 x1:x2^2 x2:x1^2 PolynomialFeatures(degree=3, include_bias=False).fit_transform(X) ## array([[ 1., 6., 1., 6., 36., 1., 6., 36., 216.], ## [ 2., 7., 4., 14., 49., 8., 28., 98., 343.], ## [ 3., 8., 9., 24., 64., 27., 72., 192., 512.], ## [ 4., 9., 16., 36., 81., 64., 144., 324., 729.], ## [ 5., 10., 25., 50., 100., 125., 250., 500., 1000.]]) 17.9 Imputation of Missing Data 17.9.1 Sample Data from numpy import nan X = np.array([[ nan, 0, 3 ], [ 3, 7, 9 ], [ 3, 5, 2 ], [ 4, nan, 6 ], [ 8, 8, 1 ]]) y = np.array([14, 16, -1, 8, -5]) 17.9.2 Imputer 17.9.2.1 mean strategy imp = Imputer(strategy=&#39;mean&#39;) ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;Imputer&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; X2 = imp.fit_transform(X) ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;imp&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; X2 ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;X2&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; 17.10 Scaling It is possible that some insignificant variable with larger range will be dominating the objective function. We can remove this problem by scaling down all the features to a same range. 17.10.1 Sample Data X=mydf.filter(like=&#39;x&#39;)[:5] X ## x1 x2 x3 ## 0 14 7 12 ## 1 18 5 5 ## 2 14 11 6 ## 3 3 21 1 ## 4 4 13 5 17.10.2 MinMax Scaler MinMaxScaler( feature_range(0,1), copy=True ) # default feature range (output result) from 0 to 1 # default return a copy of new array, copy=False will inplace original array Define Scaler Object scaler = MinMaxScaler() Transform Data scaler.fit_transform(X) ## array([[0.73333333, 0.125 , 1. ], ## [1. , 0. , 0.36363636], ## [0.73333333, 0.375 , 0.45454545], ## [0. , 1. , 0. ], ## [0.06666667, 0.5 , 0.36363636]]) Scaler Attributes data_min_: minimum value of the feature (before scaling) data_max_: maximum value of the feature (before scaling) pd.DataFrame(list(zip(scaler.data_min_, scaler.data_max_)), columns=[&#39;data_min&#39;,&#39;data_max&#39;], index=X.columns) ## data_min data_max ## x1 3.0 18.0 ## x2 5.0 21.0 ## x3 1.0 12.0 17.10.3 Standard Scaler It is most suitable for techniques that assume a Gaussian distribution in the input variables and work better with rescaled data, such as linear regression, logistic regression and linear discriminate analysis. StandardScaler(copy=True, with_mean=True, with_std=True) # copy=True : return a copy of data, instead of inplace # with_mean=True : centre all features by substracting with its mean # with_std=True : centre all features by dividing with its std Define Scaler Object scaler = StandardScaler() Transform Data scaler.fit_transform(X) ## array([[ 0.56793014, -0.78975397, 1.74943121], ## [ 1.23608324, -1.14873305, -0.22573306], ## [ 0.56793014, -0.07179582, 0.05643326], ## [-1.2694909 , 1.72309958, -1.35439836], ## [-1.10245262, 0.28718326, -0.22573306]]) Scaler Attributes After the data transformation step above, scaler will have the mean and variance information for each feature. pd.DataFrame(list(zip(scaler.mean_, scaler.var_)), columns=[&#39;mean&#39;,&#39;variance&#39;], index=X.columns) ## mean variance ## x1 10.6 35.84 ## x2 11.4 31.04 ## x3 5.8 12.56 17.11 Pipeline With any of the preceding examples, it can quickly become tedious to do the transformations by hand, especially if you wish to string together multiple steps. For example, we might want a processing pipeline that looks something like this: Impute missing values using the mean Transform features to quadratic Fit a linear regression make_pipeline takes list of functions as parameters. When calling fit() on a pipeline object, these functions will be performed in sequential with data flow from one function to another. make_pipeline ( function_1 (), function_2 (), function_3 () ) 17.11.1 Sample Data X ## x1 x2 x3 ## 0 14 7 12 ## 1 18 5 5 ## 2 14 11 6 ## 3 3 21 1 ## 4 4 13 5 y ## array([14, 16, -1, 8, -5]) 17.11.2 Create Pipeline my_pipe = make_pipeline ( Imputer (strategy=&#39;mean&#39;), PolynomialFeatures (degree=2), LinearRegression () ) ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;Imputer&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 2, in &lt;module&gt; type(my_pipe) ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;my_pipe&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; my_pipe ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;my_pipe&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; 17.11.3 Executing Pipeline my_pipe.fit( X, y) # execute the pipeline ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;my_pipe&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; print (y) ## [14 16 -1 8 -5] print (my_pipe.predict(X)) ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;my_pipe&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; type(my_pipe) ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;my_pipe&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; 17.12 Cross Validation 17.12.1 Load Data X,y = datasets.load_diabetes(return_X_y=True) 17.12.2 Choose An Cross Validator kf = KFold(n_splits=5) 17.12.3 Run Cross Validation Single Scorer Use default scorer of the estimator (if available) lasso = Lasso() cv_results1 = cross_validate(lasso, X,y,cv=kf, return_train_score=False) Multiple Scorer Specify the scorer http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter cv_results2 = cross_validate(lasso, X,y,cv=kf, scoring=(&quot;neg_mean_absolute_error&quot;,&quot;neg_mean_squared_error&quot;,&quot;r2&quot;), return_train_score=False) 17.12.4 The Result Result is a dictionary cv_results1.keys() ## dict_keys([&#39;fit_time&#39;, &#39;score_time&#39;, &#39;test_score&#39;]) cv_results2.keys() ## dict_keys([&#39;fit_time&#39;, &#39;score_time&#39;, &#39;test_neg_mean_absolute_error&#39;, &#39;test_neg_mean_squared_error&#39;, &#39;test_r2&#39;]) cv_results1 ## {&#39;fit_time&#39;: array([0.00100017, 0.00100112, 0.00099874, 0.00100088, 0. ]), &#39;score_time&#39;: array([0.00100088, 0.00100541, 0.00099897, 0.00100112, 0.00100064]), &#39;test_score&#39;: array([0.28349047, 0.35157959, 0.3533813 , 0.33481474, 0.36453281])} cv_results2 ## {&#39;fit_time&#39;: array([0.00100136, 0.00101447, 0.0010016 , 0.00099134, 0. ]), &#39;score_time&#39;: array([0.00099921, 0.00098586, 0.00100279, 0. , 0.00199986]), &#39;test_neg_mean_absolute_error&#39;: array([-50.09003423, -52.54110842, -55.02813846, -50.81121806, ## -55.60471593]), &#39;test_neg_mean_squared_error&#39;: array([-3491.74009759, -4113.86002091, -4046.91780932, -3489.74018715, ## -4111.92401769]), &#39;test_r2&#39;: array([0.28349047, 0.35157959, 0.3533813 , 0.33481474, 0.36453281])} "],
["nlp.html", "18 NLP 18.1 Regular Expression 18.2 Word Tokenizer 18.3 Sentence Tokenizer 18.4 N-Gram 18.5 Stopwords 18.6 Normalizing 18.7 Wordnet 18.8 Part Of Speech (POS) 18.9 Sentiment 18.10 Feature Representation 18.11 Appliction 18.12 Naive Bayes", " 18 NLP Natural Language Processing 18.1 Regular Expression Rgular expressions (called REs or regexes) is mandatory skill for NLP. The re is a **built-in* library It is essentially a tiny, highly specialized programming language embedded inside Python and made available through the re module Regular expression patterns are compiled into a series of bytecodes which are then executed by a matching engine written in C 18.1.1 Syntax There are two methods to emply re. Below method compile a regex first, then apply it multiple times in subsequent code. import re pattern = re.compile(r&#39;put pattern here&#39;) pattern.match(&#39;put text here&#39;) Second method below employ compile and match in single line. The pattern cannot be reused, therefore good for onetime usage only. import re pattern = (r&#39;put pattern here&#39;) re.match(pattern, r&#39;put text here&#39;) # compile and match in single line 18.1.2 Finding 18.1.2.1 Find The First Match There are two ways to find the first match: - re.search find first match anywhere in text, including multiline - re.match find first match at the BEGINNING of text, similar to re.searchwith ^ - Both returns first match, return MatchObject - Both returns None if no match is found pattern1 = re.compile(&#39;123&#39;) pattern2 = re.compile(&#39;123&#39;) pattern3 = re.compile(&#39;^123&#39;) # equivalent to above text = &#39;abc123xyz&#39; ## Single Line Text Example print( &#39;re.search found a match somewhere:\\n&#39;, pattern1.search(text), &#39;\\n&#39;, ## found &#39;\\nre.match did not find anything at the beginning:\\n&#39;, pattern2.match(text), &#39;\\n&#39;, &#39;\\nre.search did not find anything at beginning too:\\n&#39;, pattern3.search(text)) ## None ## re.search found a match somewhere: ## &lt;re.Match object; span=(3, 6), match=&#39;123&#39;&gt; ## ## re.match did not find anything at the beginning: ## None ## ## re.search did not find anything at beginning too: ## None Returned MatchObject provides useful information about the matched string. age_pattern = re.compile(r&#39;\\d+&#39;) age_text = &#39;Ali is my teacher. He is 109 years old. his kid is 40 years old.&#39; first_found = age_pattern.search(age_text) print(&#39;Found Object: &#39;, first_found, &#39;\\nInput Text: &#39;, first_found.string, &#39;\\nInput Pattern: &#39;, first_found.re, &#39;\\nFirst Found string: &#39;, first_found.group(), &#39;\\nFound Start Position: &#39;, first_found.start(), &#39;\\nFound End Position: &#39;, first_found.end(), &#39;\\nFound Span: &#39;, first_found.span(),) ## Found Object: &lt;re.Match object; span=(25, 28), match=&#39;109&#39;&gt; ## Input Text: Ali is my teacher. He is 109 years old. his kid is 40 years old. ## Input Pattern: re.compile(&#39;\\\\d+&#39;) ## First Found string: 109 ## Found Start Position: 25 ## Found End Position: 28 ## Found Span: (25, 28) 18.1.2.2 Find All Matches findall() returns all matching string as list. If no matches found, it return an empty list. print( &#39;Finding Two Digits:&#39;, re.findall(r&#39;\\d\\d&#39;,&#39;abc123xyz456&#39;), &#39;\\n&#39;, &#39;\\nFound Nothing:&#39;, re.findall(r&#39;\\d\\d&#39;,&#39;abcxyz&#39;)) ## Finding Two Digits: [&#39;12&#39;, &#39;45&#39;] ## ## Found Nothing: [] 18.1.3 Matching Condition 18.1.3.1 Meta Characters [] match any single character within the bracket [1234] is the same as [1-4] [0-39] is the same as [01239] [a-e] is the same as [abcde] [^abc] means any character except a,b,c [^0-9] means any character except 0-9 a|b: a or b {n,m} at least n repetition, but maximum m repetition () grouping pattern = re.compile(r&#39;[a-z]+&#39;) text1 = &quot;tempo&quot; text2 = &quot;tempo1&quot; text3 = &quot;123 tempo1&quot; text4 = &quot; tempo&quot; print( &#39;Matching Text1:&#39;, pattern.match(text1), &#39;\\nMatching Text2:&#39;, pattern.match(text2), &#39;\\nMatching Text3:&#39;, pattern.match(text3), &#39;\\nMatching Text4:&#39;, pattern.match(text4)) ## Matching Text1: &lt;re.Match object; span=(0, 5), match=&#39;tempo&#39;&gt; ## Matching Text2: &lt;re.Match object; span=(0, 5), match=&#39;tempo&#39;&gt; ## Matching Text3: None ## Matching Text4: None 18.1.3.2 Special Sequence . : [^\\n] \\d: [0-9] \\D: [^0-9] \\s: [ \\t\\n\\r\\f\\v] \\S: [^ \\t\\n\\r\\f\\v] \\w: [a-zA-Z0-9_] \\W: [^a-zA-Z0-9_] \\t: tab \\n: newline \\b: word boundry (delimited by space, \\t, \\n) Word Boundry Using \\b: \\bABC match if specified characters at the beginning of word (delimited by space, ), or beginning of newline ABC\\b match if specified characters at the end of word (delimited by space, ), or end of the line text = &quot;ABCD ABC XYZABC&quot; pattern1 = re.compile(r&#39;\\bABC&#39;) pattern2 = re.compile(r&#39;ABC\\b&#39;) pattern3 = re.compile(r&#39;\\bABC\\b&#39;) print(&#39;Match word that begins ABC:&#39;, pattern1.findall(text), &#39;\\n&#39;, &#39;Match word that ends with ABC:&#39;, pattern2.findall(text),&#39;\\n&#39;, &#39;Match isolated word with ABC:&#39;, pattern3.findall(text)) ## Match word that begins ABC: [&#39;ABC&#39;, &#39;ABC&#39;] ## Match word that ends with ABC: [&#39;ABC&#39;, &#39;ABC&#39;] ## Match isolated word with ABC: [&#39;ABC&#39;] 18.1.3.3 Repetition When repetition is used, re will be greedy; it try to repeat as many times as possible. If later portions of the pattern don’t match, the matching engine will then back up and try again with fewer repetitions. ?: zero or 1 occurance *: zero or more occurance +: one or more occurance ? Zero or 1 Occurance text = &#39;abcbcdd&#39; pattern = re.compile(r&#39;a[bcd]?b&#39;) pattern.findall(text) ## [&#39;ab&#39;] + At Least One Occurance text = &#39;abcbcdd&#39; pattern = re.compile(r&#39;a[bcd]+b&#39;) pattern.findall(text) ## [&#39;abcb&#39;] * Zero Or More Occurance Occurance text = &#39;abcbcdd&#39; pattern = re.compile(r&#39;a[bcd]*b&#39;) pattern.findall(text) ## [&#39;abcb&#39;] 18.1.3.4 Greedy vs Non-Greedy The *, +, and ? qualifiers are all greedy; they match as much text as possible If the &lt;.*&gt; is matched against &lt;a&gt; b &lt;c&gt;, it will match the entire string, and not just &lt;a&gt; Adding ? after the qualifier makes it perform the match in non-greedy; as few characters as possible will be matched. Using the RE &lt;.*?&gt; will match only ‘’ text = &#39;&lt;a&gt; ali baba &lt;c&gt;&#39; greedy_pattern = re.compile(r&#39;&lt;.*&gt;&#39;) non_greedy_pattern = re.compile(r&#39;&lt;.*?&gt;&#39;) print( &#39;Greedy: &#39; , greedy_pattern.findall(text), &#39;\\n&#39;, &#39;Non Greedy: &#39;, non_greedy_pattern.findall(text) ) ## Greedy: [&#39;&lt;a&gt; ali baba &lt;c&gt;&#39;] ## Non Greedy: [&#39;&lt;a&gt;&#39;, &#39;&lt;c&gt;&#39;] 18.1.4 Grouping When () is used in the pattern, retrive the grouping components in MatchObject with .groups(). Result is in list. Example below extract hours, minutes and am/pm into a list. 18.1.4.1 Capturing Group text = &#39;Today at Wednesday, 10:50pm, we go for a walk&#39; pattern = re.compile(r&#39;(\\d\\d):(\\d\\d)(am|pm)&#39;) m = pattern.search(text) print( &#39;All Gropus: &#39;, m.groups(), &#39;\\n&#39;, &#39;Group 1: &#39;, m.group(1), &#39;\\n&#39;, &#39;Group 2: &#39;, m.group(2), &#39;\\n&#39;, &#39;Group 3: &#39;, m.group(3) ) ## All Gropus: (&#39;10&#39;, &#39;50&#39;, &#39;pm&#39;) ## Group 1: 10 ## Group 2: 50 ## Group 3: pm 18.1.4.2 Non-Capturing Group Having (:? ) means don’t capture this group text = &#39;Today at Wednesday, 10:50pm, we go for a walk&#39; pattern = re.compile(r&#39;(:?\\d\\d):(?:\\d\\d)(am|pm)&#39;) m = pattern.search(text) print( &#39;All Gropus: &#39;, m.groups(), &#39;\\n&#39;, &#39;Group 1: &#39;, m.group(1), &#39;\\n&#39;, &#39;Group 2: &#39;, m.group(2) ) ## All Gropus: (&#39;10&#39;, &#39;pm&#39;) ## Group 1: 10 ## Group 2: pm 18.1.5 Splittitng Pattern is used to match delimters. 18.1.5.1 Use re.split() print( re.split(&#39;@&#39;, &quot;aa@bb @ cc &quot;), &#39;\\n&#39;, re.split(&#39;\\|&#39;, &quot;aa|bb | cc &quot;), &#39;\\n&#39;, re.split(&#39;\\n&#39;, &quot;sentence1\\nsentence2\\nsentence3&quot;) ) ## [&#39;aa&#39;, &#39;bb &#39;, &#39; cc &#39;] ## [&#39;aa&#39;, &#39;bb &#39;, &#39; cc &#39;] ## [&#39;sentence1&#39;, &#39;sentence2&#39;, &#39;sentence3&#39;] 18.1.5.2 Use re.compile().split() pattern = re.compile(r&quot;\\|&quot;) pattern.split(&quot;aa|bb | cc &quot;) ## [&#39;aa&#39;, &#39;bb &#39;, &#39; cc &#39;] 18.1.6 Substitution re.sub() 18.1.6.1 Found Match Example below repalce anything within {{.*}} re.sub(r&#39;({{.*}})&#39;, &#39;Durian&#39;, &#39;I like to eat {{Food}}.&#39;, flags=re.IGNORECASE) ## &#39;I like to eat Durian.&#39; Replace AND with &amp;. This does not require () grouping re.sub(r&#39;\\sAND\\s&#39;, &#39; &amp; &#39;, &#39;Baked Beans And Spam&#39;, flags=re.IGNORECASE) ## &#39;Baked Beans &amp; Spam&#39; 18.1.6.2 No Match If not pattern not found, return the original text. re.sub(r&#39;({{.*}})&#39;, &#39;Durian&#39;, &#39;I like to eat &lt;Food&gt;.&#39;, flags=re.IGNORECASE) ## &#39;I like to eat &lt;Food&gt;.&#39; 18.1.7 Practical Examples 18.1.7.1 Extracting Float re_float = re.compile(r&#39;\\d+(\\.\\d+)?&#39;) def extract_float(x): money = x.replace(&#39;,&#39;,&#39;&#39;) result = re_float.search(money) return float(result.group()) if result else float(0) print( extract_float(&#39;123,456.78&#39;), &#39;\\n&#39;, extract_float(&#39;rm 123.78 (30%)&#39;), &#39;\\n&#39;, extract_float(&#39;rm 123,456.78 (30%)&#39;) ) ## 123456.78 ## 123.78 ## 123456.78 18.2 Word Tokenizer 18.2.1 Custom Tokenizer 18.2.1.1 Split By Regex Pattern Use regex to split words based on specific punctuation as delimeter. The rule is: split input text when any one or more continuous occurances of specified character. import re pattern = re.compile(r&quot;[-\\s.,;!?]+&quot;) pattern.split(&quot;hi @ali--baba, you are aweeeeeesome! isn&#39;t it. Believe it.:)&quot;) ## [&#39;hi&#39;, &#39;@ali&#39;, &#39;baba&#39;, &#39;you&#39;, &#39;are&#39;, &#39;aweeeeeesome&#39;, &quot;isn&#39;t&quot;, &#39;it&#39;, &#39;Believe&#39;, &#39;it&#39;, &#39;:)&#39;] 18.2.1.2 Pick By Regex Pattern nltk.tokenize.RegexpTokenizer Any sequence of chars fall within the bracket are considered tokens. Any chars not within the bracket are removed. from nltk.tokenize import RegexpTokenizer my_tokenizer = RegexpTokenizer(r&#39;[a-zA-Z0-9\\&#39;]+&#39;) my_tokenizer.tokenize(&quot;hi @ali--baba, you are aweeeeeesome! isn&#39;t it. Believe it.:&quot;) ## [&#39;hi&#39;, &#39;ali&#39;, &#39;baba&#39;, &#39;you&#39;, &#39;are&#39;, &#39;aweeeeeesome&#39;, &quot;isn&#39;t&quot;, &#39;it&#39;, &#39;Believe&#39;, &#39;it&#39;] 18.2.2 nltk.tokenize.word_tokenize() Words and punctuations are considered as tokens! from nltk.tokenize import word_tokenize print( word_tokenize(&quot;hi @ali-baba, you are aweeeeeesome! isn&#39;t it. Believe it.:)&quot;) ) ## [&#39;hi&#39;, &#39;@&#39;, &#39;ali-baba&#39;, &#39;,&#39;, &#39;you&#39;, &#39;are&#39;, &#39;aweeeeeesome&#39;, &#39;!&#39;, &#39;is&#39;, &quot;n&#39;t&quot;, &#39;it&#39;, &#39;.&#39;, &#39;Believe&#39;, &#39;it&#39;, &#39;.&#39;, &#39;:&#39;, &#39;)&#39;] 18.2.3 nltk.tokenize.casual.casual_tokenize() Support emoji Support reduction of repetition chars Support removing userid ((???)) Good for social media text Punctuations are tokens! from nltk.tokenize.casual import casual_tokenize print( casual_tokenize(&quot;hi @ali-baba, you are aweeeeeesome! isn&#39;t it. Believe it. :)&quot;) ) ## [&#39;hi&#39;, &#39;@ali&#39;, &#39;-&#39;, &#39;baba&#39;, &#39;,&#39;, &#39;you&#39;, &#39;are&#39;, &#39;aweeeeeesome&#39;, &#39;!&#39;, &quot;isn&#39;t&quot;, &#39;it&#39;, &#39;.&#39;, &#39;Believe&#39;, &#39;it&#39;, &#39;.&#39;, &#39;:)&#39;] Example below shorten repeating chars, notice aweeeeeesome becomes aweeesome ## shorten repeated chars print( casual_tokenize(&quot;hi @ali-baba, you are aweeeeeesome! isn&#39;t it. Believe it.:)&quot;, reduce_len=True)) ## [&#39;hi&#39;, &#39;@ali&#39;, &#39;-&#39;, &#39;baba&#39;, &#39;,&#39;, &#39;you&#39;, &#39;are&#39;, &#39;aweeesome&#39;, &#39;!&#39;, &quot;isn&#39;t&quot;, &#39;it&#39;, &#39;.&#39;, &#39;Believe&#39;, &#39;it&#39;, &#39;.&#39;, &#39;:)&#39;] Stripping off User ID ## shorten repeated chars, stirp usernames print( casual_tokenize(&quot;hi @ali-baba, you are aweeeeeesome! isn&#39;t it. Believe it.:)&quot;, reduce_len=True, strip_handles=True)) ## [&#39;hi&#39;, &#39;-&#39;, &#39;baba&#39;, &#39;,&#39;, &#39;you&#39;, &#39;are&#39;, &#39;aweeesome&#39;, &#39;!&#39;, &quot;isn&#39;t&quot;, &#39;it&#39;, &#39;.&#39;, &#39;Believe&#39;, &#39;it&#39;, &#39;.&#39;, &#39;:)&#39;] 18.2.4 nltk.tokenize.treebank.TreebankWordTokenizer().tokenize() Treebank assume input text is A sentence, hence any period combined with word is treated as token. from nltk.tokenize.treebank import TreebankWordTokenizer TreebankWordTokenizer().tokenize(&quot;hi @ali-baba, you are aweeeeeesome! isn&#39;t it. Believe it.:)&quot;) ## [&#39;hi&#39;, &#39;@&#39;, &#39;ali-baba&#39;, &#39;,&#39;, &#39;you&#39;, &#39;are&#39;, &#39;aweeeeeesome&#39;, &#39;!&#39;, &#39;is&#39;, &quot;n&#39;t&quot;, &#39;it.&#39;, &#39;Believe&#39;, &#39;it.&#39;, &#39;:&#39;, &#39;)&#39;] 18.2.5 Corpus Token Extractor A corpus is a collection of documents (list of documents). A document is a text string containing one or many sentences. from nltk.tokenize import word_tokenize from nlpia.data.loaders import harry_docs as corpus ## Tokenize each doc to list, then add to a bigger list doc_tokens=[] for doc in corpus: doc_tokens += [word_tokenize(doc.lower())] print(&#39;Corpus (Contain 3 Documents):\\n&#39;,corpus,&#39;\\n&#39;, &#39;\\nTokenized result for each document:&#39;,&#39;\\n&#39;,doc_tokens) ## Corpus (Contain 3 Documents): ## [&#39;The faster Harry got to the store, the faster and faster Harry would get home.&#39;, &#39;Harry is hairy and faster than Jill.&#39;, &#39;Jill is not as hairy as Harry.&#39;] ## ## Tokenized result for each document: ## [[&#39;the&#39;, &#39;faster&#39;, &#39;harry&#39;, &#39;got&#39;, &#39;to&#39;, &#39;the&#39;, &#39;store&#39;, &#39;,&#39;, &#39;the&#39;, &#39;faster&#39;, &#39;and&#39;, &#39;faster&#39;, &#39;harry&#39;, &#39;would&#39;, &#39;get&#39;, &#39;home&#39;, &#39;.&#39;], [&#39;harry&#39;, &#39;is&#39;, &#39;hairy&#39;, &#39;and&#39;, &#39;faster&#39;, &#39;than&#39;, &#39;jill&#39;, &#39;.&#39;], [&#39;jill&#39;, &#39;is&#39;, &#39;not&#39;, &#39;as&#39;, &#39;hairy&#39;, &#39;as&#39;, &#39;harry&#39;, &#39;.&#39;]] Unpack list of token lists from above using sum. To get the vocabulary (unique tokens), convert list to set. ## unpack list of list to list vocab = sum(doc_tokens,[]) print(&#39;\\nCorpus Vacabulary (Unique Tokens):\\n&#39;, sorted(set(vocab))) ## ## Corpus Vacabulary (Unique Tokens): ## [&#39;,&#39;, &#39;.&#39;, &#39;and&#39;, &#39;as&#39;, &#39;faster&#39;, &#39;get&#39;, &#39;got&#39;, &#39;hairy&#39;, &#39;harry&#39;, &#39;home&#39;, &#39;is&#39;, &#39;jill&#39;, &#39;not&#39;, &#39;store&#39;, &#39;than&#39;, &#39;the&#39;, &#39;to&#39;, &#39;would&#39;] 18.3 Sentence Tokenizer This is about detecting sentence boundry and split text into list of sentences 18.3.1 Sample Text text = &#39;&#39;&#39; Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome. The sky is pinkish-blue, Dr. Alba would agree. You shouldn&#39;t eat hard things i.e. cardboard, stones and bushes &#39;&#39;&#39; 18.3.2 ’nltk.tokenize.punkt.PunktSentenceTokenizer` The PunktSentenceTokenizer is an sentence boundary detection algorithm. It is an unsupervised trainable model. This means it can be trained on unlabeled data, aka text that is not split into sentences PunkSentneceTokenizer is based on work published on this paepr: Unsupervised Multilingual Sentence Boundary Detection 18.3.2.1 Default Behavior Vanila tokenizer splits sentences on period ., which is not desirable from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer #nltk.download(&#39;punkt&#39;) tokenizer = PunktSentenceTokenizer() tokenized_text = tokenizer.tokenize(text) for x in tokenized_text: print(x) ## ## Hello Mr. ## Smith, how are you doing today? ## The weather is great, and city is awesome. ## The sky is pinkish-blue, Dr. ## Alba would agree. ## You shouldn&#39;t eat hard things i.e. ## cardboard, stones and bushes 18.3.2.2 Pretrained Model - English Pickle NLTK already includes a pre-trained version of the PunktSentenceTokenizer for English, as you can see, it is quite good tokenizer = nltk.data.load(&#39;tokenizers/punkt/english.pickle&#39;) ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;nltk&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; tokenized_text = tokenizer.tokenize(text) for x in tokenized_text: print(x) ## ## Hello Mr. ## Smith, how are you doing today? ## The weather is great, and city is awesome. ## The sky is pinkish-blue, Dr. ## Alba would agree. ## You shouldn&#39;t eat hard things i.e. ## cardboard, stones and bushes 18.3.2.3 Adding Abbreviations The pretrained tokenizer is not perfect, it wrongly detected ‘i.e.’ as sentence boundary Let’s teach Punkt by adding the abbreviation to its parameter Adding Single Abbreviation tokenizer = nltk.data.load(&#39;tokenizers/punkt/english.pickle&#39;) ## Add apprevaitions to Tokenizer ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;nltk&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; tokenizer._params.abbrev_types.add(&#39;i.e&#39;) tokenized_text = tokenizer.tokenize(text) for x in tokenized_text: print(x) ## ## Hello Mr. ## Smith, how are you doing today? ## The weather is great, and city is awesome. ## The sky is pinkish-blue, Dr. ## Alba would agree. ## You shouldn&#39;t eat hard things i.e. cardboard, stones and bushes Add List of Abbreviations If you have more than one abbreviations, use update() with the list of abbreviations from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters ## Add Abbreviations to Tokenizer tokenizer = nltk.data.load(&#39;tokenizers/punkt/english.pickle&#39;) ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;nltk&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; tokenizer._params.abbrev_types.update([&#39;dr&#39;, &#39;vs&#39;, &#39;mr&#39;, &#39;mrs&#39;, &#39;prof&#39;, &#39;inc&#39;, &#39;i.e&#39;]) sentences = tokenizer.tokenize(text) for x in sentences: print(x) ## ## Hello Mr. Smith, how are you doing today? ## The weather is great, and city is awesome. ## The sky is pinkish-blue, Dr. Alba would agree. ## You shouldn&#39;t eat hard things i.e. cardboard, stones and bushes 18.3.3 nltk.tokenize.sent_tokenize() The sent_tokenize function uses an instance of PunktSentenceTokenizer, which is already been trained and thus very well knows to mark the end and begining of sentence at what characters and punctuation. from nltk.tokenize import sent_tokenize sentences = sent_tokenize(text) for x in sentences: print(x) ## ## Hello Mr. Smith, how are you doing today? ## The weather is great, and city is awesome. ## The sky is pinkish-blue, Dr. Alba would agree. ## You shouldn&#39;t eat hard things i.e. ## cardboard, stones and bushes 18.4 N-Gram To create n-gram, first create 1-gram token from nltk.util import ngrams import re sentence = &quot;Thomas Jefferson began building the city, at the age of 25&quot; pattern = re.compile(r&quot;[-\\s.,;!?]+&quot;) tokens = pattern.split(sentence) print(tokens) ## [&#39;Thomas&#39;, &#39;Jefferson&#39;, &#39;began&#39;, &#39;building&#39;, &#39;the&#39;, &#39;city&#39;, &#39;at&#39;, &#39;the&#39;, &#39;age&#39;, &#39;of&#39;, &#39;25&#39;] ngrams() is a generator, therefore, use list() to convert into full list ngrams(tokens,2) ## &lt;generator object ngrams at 0x0000000031024408&gt; Convert 1-gram to 2-Gram, wrap into list grammy = list( ngrams(tokens,2) ) print(grammy) ## [(&#39;Thomas&#39;, &#39;Jefferson&#39;), (&#39;Jefferson&#39;, &#39;began&#39;), (&#39;began&#39;, &#39;building&#39;), (&#39;building&#39;, &#39;the&#39;), (&#39;the&#39;, &#39;city&#39;), (&#39;city&#39;, &#39;at&#39;), (&#39;at&#39;, &#39;the&#39;), (&#39;the&#39;, &#39;age&#39;), (&#39;age&#39;, &#39;of&#39;), (&#39;of&#39;, &#39;25&#39;)] Combine each 2-gram into a string object [ &quot; &quot;.join(x) for x in grammy] ## [&#39;Thomas Jefferson&#39;, &#39;Jefferson began&#39;, &#39;began building&#39;, &#39;building the&#39;, &#39;the city&#39;, &#39;city at&#39;, &#39;at the&#39;, &#39;the age&#39;, &#39;age of&#39;, &#39;of 25&#39;] 18.5 Stopwords 18.5.1 Custom Stop Words Build the custom stop words dictionary. stop_words = [&#39;a&#39;,&#39;an&#39;,&#39;the&#39;,&#39;on&#39;,&#39;of&#39;,&#39;off&#39;,&#39;this&#39;,&#39;is&#39;,&#39;at&#39;] Tokenize text and remove stop words sentence = &quot;The house is on fire&quot; tokens = word_tokenize(sentence) tokens_without_stopwords = [ x for x in tokens if x not in stop_words ] print(&#39; Original Tokens : &#39;, tokens, &#39;\\n&#39;, &#39;Removed Stopwords: &#39;,tokens_without_stopwords) ## Original Tokens : [&#39;The&#39;, &#39;house&#39;, &#39;is&#39;, &#39;on&#39;, &#39;fire&#39;] ## Removed Stopwords: [&#39;The&#39;, &#39;house&#39;, &#39;fire&#39;] 18.5.2 NLTK Stop Words Contain 179 words, in a list form import nltk #nltk.download(&#39;stopwords&#39;) nltk_stop_words = nltk.corpus.stopwords.words(&#39;english&#39;) print(&#39;Total NLTK Stopwords: &#39;, len(nltk_stop_words),&#39;\\n&#39;, nltk_stop_words) ## Total NLTK Stopwords: 179 ## [&#39;i&#39;, &#39;me&#39;, &#39;my&#39;, &#39;myself&#39;, &#39;we&#39;, &#39;our&#39;, &#39;ours&#39;, &#39;ourselves&#39;, &#39;you&#39;, &quot;you&#39;re&quot;, &quot;you&#39;ve&quot;, &quot;you&#39;ll&quot;, &quot;you&#39;d&quot;, &#39;your&#39;, &#39;yours&#39;, &#39;yourself&#39;, &#39;yourselves&#39;, &#39;he&#39;, &#39;him&#39;, &#39;his&#39;, &#39;himself&#39;, &#39;she&#39;, &quot;she&#39;s&quot;, &#39;her&#39;, &#39;hers&#39;, &#39;herself&#39;, &#39;it&#39;, &quot;it&#39;s&quot;, &#39;its&#39;, &#39;itself&#39;, &#39;they&#39;, &#39;them&#39;, &#39;their&#39;, &#39;theirs&#39;, &#39;themselves&#39;, &#39;what&#39;, &#39;which&#39;, &#39;who&#39;, &#39;whom&#39;, &#39;this&#39;, &#39;that&#39;, &quot;that&#39;ll&quot;, &#39;these&#39;, &#39;those&#39;, &#39;am&#39;, &#39;is&#39;, &#39;are&#39;, &#39;was&#39;, &#39;were&#39;, &#39;be&#39;, &#39;been&#39;, &#39;being&#39;, &#39;have&#39;, &#39;has&#39;, &#39;had&#39;, &#39;having&#39;, &#39;do&#39;, &#39;does&#39;, &#39;did&#39;, &#39;doing&#39;, &#39;a&#39;, &#39;an&#39;, &#39;the&#39;, &#39;and&#39;, &#39;but&#39;, &#39;if&#39;, &#39;or&#39;, &#39;because&#39;, &#39;as&#39;, &#39;until&#39;, &#39;while&#39;, &#39;of&#39;, &#39;at&#39;, &#39;by&#39;, &#39;for&#39;, &#39;with&#39;, &#39;about&#39;, &#39;against&#39;, &#39;between&#39;, &#39;into&#39;, &#39;through&#39;, &#39;during&#39;, &#39;before&#39;, &#39;after&#39;, &#39;above&#39;, &#39;below&#39;, &#39;to&#39;, &#39;from&#39;, &#39;up&#39;, &#39;down&#39;, &#39;in&#39;, &#39;out&#39;, &#39;on&#39;, &#39;off&#39;, &#39;over&#39;, &#39;under&#39;, &#39;again&#39;, &#39;further&#39;, &#39;then&#39;, &#39;once&#39;, &#39;here&#39;, &#39;there&#39;, &#39;when&#39;, &#39;where&#39;, &#39;why&#39;, &#39;how&#39;, &#39;all&#39;, &#39;any&#39;, &#39;both&#39;, &#39;each&#39;, &#39;few&#39;, &#39;more&#39;, &#39;most&#39;, &#39;other&#39;, &#39;some&#39;, &#39;such&#39;, &#39;no&#39;, &#39;nor&#39;, &#39;not&#39;, &#39;only&#39;, &#39;own&#39;, &#39;same&#39;, &#39;so&#39;, &#39;than&#39;, &#39;too&#39;, &#39;very&#39;, &#39;s&#39;, &#39;t&#39;, &#39;can&#39;, &#39;will&#39;, &#39;just&#39;, &#39;don&#39;, &quot;don&#39;t&quot;, &#39;should&#39;, &quot;should&#39;ve&quot;, &#39;now&#39;, &#39;d&#39;, &#39;ll&#39;, &#39;m&#39;, &#39;o&#39;, &#39;re&#39;, &#39;ve&#39;, &#39;y&#39;, &#39;ain&#39;, &#39;aren&#39;, &quot;aren&#39;t&quot;, &#39;couldn&#39;, &quot;couldn&#39;t&quot;, &#39;didn&#39;, &quot;didn&#39;t&quot;, &#39;doesn&#39;, &quot;doesn&#39;t&quot;, &#39;hadn&#39;, &quot;hadn&#39;t&quot;, &#39;hasn&#39;, &quot;hasn&#39;t&quot;, &#39;haven&#39;, &quot;haven&#39;t&quot;, &#39;isn&#39;, &quot;isn&#39;t&quot;, &#39;ma&#39;, &#39;mightn&#39;, &quot;mightn&#39;t&quot;, &#39;mustn&#39;, &quot;mustn&#39;t&quot;, &#39;needn&#39;, &quot;needn&#39;t&quot;, &#39;shan&#39;, &quot;shan&#39;t&quot;, &#39;shouldn&#39;, &quot;shouldn&#39;t&quot;, &#39;wasn&#39;, &quot;wasn&#39;t&quot;, &#39;weren&#39;, &quot;weren&#39;t&quot;, &#39;won&#39;, &quot;won&#39;t&quot;, &#39;wouldn&#39;, &quot;wouldn&#39;t&quot;] 18.5.3 SKLearn Stop Words Contain 318 stop words, in frozenset form from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words print(&#39; Total Sklearn Stopwords: &#39;, len(sklearn_stop_words),&#39;\\n\\n&#39;, sklearn_stop_words) ## Total Sklearn Stopwords: 318 ## ## frozenset({&#39;being&#39;, &#39;is&#39;, &#39;anyway&#39;, &#39;she&#39;, &#39;something&#39;, &#39;whereas&#39;, &#39;seem&#39;, &#39;two&#39;, &#39;formerly&#39;, &#39;take&#39;, &#39;do&#39;, &#39;done&#39;, &#39;become&#39;, &#39;thence&#39;, &#39;behind&#39;, &#39;anywhere&#39;, &#39;where&#39;, &#39;part&#39;, &#39;others&#39;, &#39;out&#39;, &#39;con&#39;, &#39;eight&#39;, &#39;bottom&#39;, &#39;up&#39;, &#39;de&#39;, &#39;less&#39;, &#39;often&#39;, &#39;afterwards&#39;, &#39;whereby&#39;, &#39;seems&#39;, &#39;find&#39;, &#39;whole&#39;, &#39;ours&#39;, &#39;each&#39;, &#39;those&#39;, &#39;over&#39;, &#39;very&#39;, &#39;be&#39;, &#39;of&#39;, &#39;mill&#39;, &#39;whom&#39;, &#39;amount&#39;, &#39;even&#39;, &#39;couldnt&#39;, &#39;i&#39;, &#39;further&#39;, &#39;an&#39;, &#39;many&#39;, &#39;therefore&#39;, &#39;mostly&#39;, &#39;somehow&#39;, &#39;beforehand&#39;, &#39;about&#39;, &#39;both&#39;, &#39;moreover&#39;, &#39;no&#39;, &#39;whence&#39;, &#39;against&#39;, &#39;noone&#39;, &#39;thereafter&#39;, &#39;former&#39;, &#39;describe&#39;, &#39;now&#39;, &#39;yours&#39;, &#39;has&#39;, &#39;someone&#39;, &#39;whither&#39;, &#39;although&#39;, &#39;fifteen&#39;, &#39;every&#39;, &#39;was&#39;, &#39;though&#39;, &#39;thereby&#39;, &#39;hasnt&#39;, &#39;twenty&#39;, &#39;first&#39;, &#39;yet&#39;, &#39;side&#39;, &#39;call&#39;, &#39;same&#39;, &#39;this&#39;, &#39;yourself&#39;, &#39;thus&#39;, &#39;else&#39;, &#39;perhaps&#39;, &#39;always&#39;, &#39;detail&#39;, &#39;together&#39;, &#39;three&#39;, &#39;empty&#39;, &#39;other&#39;, &#39;full&#39;, &#39;sixty&#39;, &#39;once&#39;, &#39;it&#39;, &#39;your&#39;, &#39;why&#39;, &#39;while&#39;, &#39;except&#39;, &#39;into&#39;, &#39;before&#39;, &#39;beside&#39;, &#39;wherein&#39;, &#39;itself&#39;, &#39;a&#39;, &#39;should&#39;, &#39;without&#39;, &#39;name&#39;, &#39;inc&#39;, &#39;meanwhile&#39;, &#39;some&#39;, &#39;during&#39;, &#39;sincere&#39;, &#39;co&#39;, &#39;mine&#39;, &#39;yourselves&#39;, &#39;give&#39;, &#39;there&#39;, &#39;its&#39;, &#39;whose&#39;, &#39;twelve&#39;, &#39;her&#39;, &#39;least&#39;, &#39;still&#39;, &#39;everywhere&#39;, &#39;and&#39;, &#39;himself&#39;, &#39;or&#39;, &#39;their&#39;, &#39;ever&#39;, &#39;latterly&#39;, &#39;hereupon&#39;, &#39;can&#39;, &#39;ten&#39;, &#39;by&#39;, &#39;have&#39;, &#39;hence&#39;, &#39;all&#39;, &#39;hundred&#39;, &#39;here&#39;, &#39;interest&#39;, &#39;third&#39;, &#39;neither&#39;, &#39;everything&#39;, &#39;five&#39;, &#39;own&#39;, &#39;among&#39;, &#39;were&#39;, &#39;alone&#39;, &#39;our&#39;, &#39;toward&#39;, &#39;whereupon&#39;, &#39;fire&#39;, &#39;ltd&#39;, &#39;besides&#39;, &#39;you&#39;, &#39;back&#39;, &#39;upon&#39;, &#39;well&#39;, &#39;not&#39;, &#39;somewhere&#39;, &#39;that&#39;, &#39;ie&#39;, &#39;front&#39;, &#39;thick&#39;, &#39;get&#39;, &#39;am&#39;, &#39;herein&#39;, &#39;one&#39;, &#39;whoever&#39;, &#39;either&#39;, &#39;namely&#39;, &#39;rather&#39;, &#39;themselves&#39;, &#39;with&#39;, &#39;otherwise&#39;, &#39;they&#39;, &#39;show&#39;, &#39;below&#39;, &#39;hers&#39;, &#39;around&#39;, &#39;me&#39;, &#39;go&#39;, &#39;since&#39;, &#39;due&#39;, &#39;everyone&#39;, &#39;whereafter&#39;, &#39;amoungst&#39;, &#39;we&#39;, &#39;whenever&#39;, &#39;will&#39;, &#39;when&#39;, &#39;few&#39;, &#39;etc&#39;, &#39;on&#39;, &#39;between&#39;, &#39;fill&#39;, &#39;his&#39;, &#39;next&#39;, &#39;nevertheless&#39;, &#39;eg&#39;, &#39;indeed&#39;, &#39;might&#39;, &#39;my&#39;, &#39;becomes&#39;, &#39;until&#39;, &#39;fifty&#39;, &#39;if&#39;, &#39;but&#39;, &#39;nine&#39;, &#39;any&#39;, &#39;down&#39;, &#39;seemed&#39;, &#39;which&#39;, &#39;elsewhere&#39;, &#39;re&#39;, &#39;cant&#39;, &#39;he&#39;, &#39;made&#39;, &#39;myself&#39;, &#39;nowhere&#39;, &#39;however&#39;, &#39;please&#39;, &#39;nor&#39;, &#39;the&#39;, &#39;these&#39;, &#39;been&#39;, &#39;must&#39;, &#39;thru&#39;, &#39;for&#39;, &#39;forty&#39;, &#39;whether&#39;, &#39;cry&#39;, &#39;seeming&#39;, &#39;beyond&#39;, &#39;found&#39;, &#39;sometime&#39;, &#39;anything&#39;, &#39;under&#39;, &#39;thin&#39;, &#39;through&#39;, &#39;us&#39;, &#39;at&#39;, &#39;anyhow&#39;, &#39;keep&#39;, &#39;last&#39;, &#39;six&#39;, &#39;within&#39;, &#39;above&#39;, &#39;became&#39;, &#39;un&#39;, &#39;cannot&#39;, &#39;four&#39;, &#39;who&#39;, &#39;amongst&#39;, &#39;several&#39;, &#39;than&#39;, &#39;such&#39;, &#39;what&#39;, &#39;in&#39;, &#39;latter&#39;, &#39;are&#39;, &#39;so&#39;, &#39;across&#39;, &#39;whatever&#39;, &#39;much&#39;, &#39;as&#39;, &#39;put&#39;, &#39;him&#39;, &#39;herself&#39;, &#39;would&#39;, &#39;see&#39;, &#39;most&#39;, &#39;bill&#39;, &#39;only&#39;, &#39;onto&#39;, &#39;system&#39;, &#39;wherever&#39;, &#39;hereafter&#39;, &#39;towards&#39;, &#39;another&#39;, &#39;thereupon&#39;, &#39;already&#39;, &#39;per&#39;, &#39;hereby&#39;, &#39;serious&#39;, &#39;how&#39;, &#39;also&#39;, &#39;therein&#39;, &#39;nothing&#39;, &#39;nobody&#39;, &#39;eleven&#39;, &#39;because&#39;, &#39;along&#39;, &#39;may&#39;, &#39;then&#39;, &#39;had&#39;, &#39;throughout&#39;, &#39;almost&#39;, &#39;never&#39;, &#39;top&#39;, &#39;move&#39;, &#39;sometimes&#39;, &#39;from&#39;, &#39;too&#39;, &#39;via&#39;, &#39;enough&#39;, &#39;them&#39;, &#39;ourselves&#39;, &#39;anyone&#39;, &#39;more&#39;, &#39;off&#39;, &#39;to&#39;, &#39;none&#39;, &#39;after&#39;, &#39;becoming&#39;, &#39;could&#39;, &#39;again&#39;}) 18.5.4 Combined NLTK and SKLearn Stop Words combined_stop_words = list( set(nltk_stop_words) | set(sklearn_stop_words) ) print(&#39;Total combined NLTK and SKLearn Stopwords:&#39;, len( combined_stop_words ),&#39;\\n&#39; &#39;Stopwords shared among NLTK and SKlearn :&#39;, len( list( set(nltk_stop_words) &amp; set(sklearn_stop_words)) )) ## Total combined NLTK and SKLearn Stopwords: 378 ## Stopwords shared among NLTK and SKlearn : 119 18.6 Normalizing Similar things are combined into single normalized form. This will reduced the vocabulary. 18.6.1 Case Folding If tokens aren’t cap normalized, you will end up with large word list. However, some information is often communicated by capitalization of word, such as name of places. If names are important, consider using proper noun. tokens = [&#39;House&#39;,&#39;Visitor&#39;,&#39;Center&#39;] [ x.lower() for x in tokens] ## [&#39;house&#39;, &#39;visitor&#39;, &#39;center&#39;] 18.6.2 Stemming Output of a stemmer is not necessary a proper word Automatically convert words to lower cap Porter stemmer is a lifetime refinement with 300 lines of python code Stemming is faster then Lemmatization from nltk.stem.porter import PorterStemmer stemmer = PorterStemmer() tokens = (&#39;house&#39;,&#39;Housing&#39;,&#39;hOuses&#39;, &#39;Malicious&#39;,&#39;goodness&#39;) [stemmer.stem(x) for x in tokens ] ## [&#39;hous&#39;, &#39;hous&#39;, &#39;hous&#39;, &#39;malici&#39;, &#39;good&#39;] 18.6.3 Lemmatization NLTK uses connections within princeton WordNet graph for word meanings. #nltk.download(&#39;wordnet&#39;) from nltk.stem import WordNetLemmatizer lemmatizer = WordNetLemmatizer() print( lemmatizer.lemmatize(&quot;better&quot;, pos =&#39;a&#39;), &#39;\\n&#39;, lemmatizer.lemmatize(&quot;better&quot;, pos =&#39;n&#39;) ) ## good ## better print( lemmatizer.lemmatize(&quot;good&quot;, pos =&#39;a&#39;), &#39;\\n&#39;, lemmatizer.lemmatize(&quot;good&quot;, pos =&#39;n&#39;) ) ## good ## good 18.6.4 Comparing Stemming and Lemmatization Lemmatization is slower than stemming = Lemmatization is better at retaining meanings Lemmatization produce valid english word Stemming not necessary produce valid english word Both reduce vocabulary size, but increase ambiguity For search engine application, stemming and lemmatization will improve recall as it associate more documents with the same query words, however with the cost of reducing precision and accuracy. For search-based chatbot where accuracy is more important, it should first search with unnormalzied words. 18.7 Wordnet WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. WordNet superficially resembles a thesaurus, in that it groups words together based on their meanings. However, there are some important distinctions: - WordNet interlinks not just word forms—strings of letters—but specific senses of words. As a result, words that are found in close proximity to one another in the network are semantically disambiguated - WordNet labels the semantic relations among words, whereas the groupings of words in a thesaurus does not follow any explicit pattern other than meaning similarity Wordnet Princeton Wordnet Online Browser 18.7.1 NLTK and Wordnet NLTK (version 3.7.6) includes the English WordNet (147,307 words and 117,659 synonym sets) from nltk.corpus import wordnet as wn s = set( wn.all_synsets() ) w = set(wn.words()) print(&#39;Total words in wordnet : &#39; , len(w), &#39;\\nTotal synsets in wordnet: &#39; , len(s) ) ## Total words in wordnet : 147306 ## Total synsets in wordnet: 117659 18.7.2 Synset 18.7.2.1 Notation A synset is the basic construct of a word in wordnet. It contains the Word itself, with its POS tag and Usage: word.pos.nn wn.synset(&#39;breakdown.n.03&#39;) ## Synset(&#39;breakdown.n.03&#39;) Breaking down the construct: &#39;breakdown&#39; = Word &#39;n&#39; = Part of Speech &#39;03&#39; = Usage (01 for most common usage and a higher number would indicate lesser common usages) 18.7.2.2 Part of Speech Wordnet support five POS tags n - NOUN v - VERB a - ADJECTIVE s - ADJECTIVE SATELLITE r - ADVERB print(wn.ADJ, wn.ADJ_SAT, wn.ADV, wn.NOUN, wn.VERB) ## a s r n v 18.7.2.3 Synset Similarity Let’s see how similar are the below two nouns w1 = wn.synset(&#39;dog.n.01&#39;) w2 = wn.synset(&#39;ship.n.01&#39;) print(w1.wup_similarity(w2)) ## 0.4 w1 = wn.synset(&#39;ship.n.01&#39;) w2 = wn.synset(&#39;boat.n.01&#39;) print(w1.wup_similarity(w2)) ## 0.9090909090909091 18.7.3 Synsets Synsets is a collection of synsets, which are synonyms that share a common meaning A synset (member of Synsets) is identified with a 3-part name of the form: A synset can contain one or more lemmas, which represent a specific sense of a specific word A synset can contain one or more Hyponyms and Hypernyms. These are specific and generalized concepts respectively. For example, ‘beach house’ and ‘guest house’ are hyponyms of ‘house’. They are more specific concepts of ‘house’. And ‘house’ is a hypernym of ‘guest house’ because it is the general concept Hyponyms and Hypernyms are also called lexical relations dogs = wn.synsets(&#39;dog&#39;) # get all synsets for word &#39;dog&#39; for d in dogs: ## iterate through each Synset print(d,&#39;:\\nDefinition:&#39;, d.definition(), &#39;\\nExample:&#39;, d.examples(), &#39;\\nLemmas:&#39;, d.lemma_names(), &#39;\\nHyponyms:&#39;, d.hyponyms(), &#39;\\nHypernyms:&#39;, d.hypernyms(), &#39;\\n\\n&#39;) ## Synset(&#39;dog.n.01&#39;) : ## Definition: a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds ## Example: [&#39;the dog barked all night&#39;] ## Lemmas: [&#39;dog&#39;, &#39;domestic_dog&#39;, &#39;Canis_familiaris&#39;] ## Hyponyms: [Synset(&#39;basenji.n.01&#39;), Synset(&#39;corgi.n.01&#39;), Synset(&#39;cur.n.01&#39;), Synset(&#39;dalmatian.n.02&#39;), Synset(&#39;great_pyrenees.n.01&#39;), Synset(&#39;griffon.n.02&#39;), Synset(&#39;hunting_dog.n.01&#39;), Synset(&#39;lapdog.n.01&#39;), Synset(&#39;leonberg.n.01&#39;), Synset(&#39;mexican_hairless.n.01&#39;), Synset(&#39;newfoundland.n.01&#39;), Synset(&#39;pooch.n.01&#39;), Synset(&#39;poodle.n.01&#39;), Synset(&#39;pug.n.01&#39;), Synset(&#39;puppy.n.01&#39;), Synset(&#39;spitz.n.01&#39;), Synset(&#39;toy_dog.n.01&#39;), Synset(&#39;working_dog.n.01&#39;)] ## Hypernyms: [Synset(&#39;canine.n.02&#39;), Synset(&#39;domestic_animal.n.01&#39;)] ## ## ## Synset(&#39;frump.n.01&#39;) : ## Definition: a dull unattractive unpleasant girl or woman ## Example: [&#39;she got a reputation as a frump&#39;, &quot;she&#39;s a real dog&quot;] ## Lemmas: [&#39;frump&#39;, &#39;dog&#39;] ## Hyponyms: [] ## Hypernyms: [Synset(&#39;unpleasant_woman.n.01&#39;)] ## ## ## Synset(&#39;dog.n.03&#39;) : ## Definition: informal term for a man ## Example: [&#39;you lucky dog&#39;] ## Lemmas: [&#39;dog&#39;] ## Hyponyms: [] ## Hypernyms: [Synset(&#39;chap.n.01&#39;)] ## ## ## Synset(&#39;cad.n.01&#39;) : ## Definition: someone who is morally reprehensible ## Example: [&#39;you dirty dog&#39;] ## Lemmas: [&#39;cad&#39;, &#39;bounder&#39;, &#39;blackguard&#39;, &#39;dog&#39;, &#39;hound&#39;, &#39;heel&#39;] ## Hyponyms: [Synset(&#39;perisher.n.01&#39;)] ## Hypernyms: [Synset(&#39;villain.n.01&#39;)] ## ## ## Synset(&#39;frank.n.02&#39;) : ## Definition: a smooth-textured sausage of minced beef or pork usually smoked; often served on a bread roll ## Example: [] ## Lemmas: [&#39;frank&#39;, &#39;frankfurter&#39;, &#39;hotdog&#39;, &#39;hot_dog&#39;, &#39;dog&#39;, &#39;wiener&#39;, &#39;wienerwurst&#39;, &#39;weenie&#39;] ## Hyponyms: [Synset(&#39;vienna_sausage.n.01&#39;)] ## Hypernyms: [Synset(&#39;sausage.n.01&#39;)] ## ## ## Synset(&#39;pawl.n.01&#39;) : ## Definition: a hinged catch that fits into a notch of a ratchet to move a wheel forward or prevent it from moving backward ## Example: [] ## Lemmas: [&#39;pawl&#39;, &#39;detent&#39;, &#39;click&#39;, &#39;dog&#39;] ## Hyponyms: [] ## Hypernyms: [Synset(&#39;catch.n.06&#39;)] ## ## ## Synset(&#39;andiron.n.01&#39;) : ## Definition: metal supports for logs in a fireplace ## Example: [&#39;the andirons were too hot to touch&#39;] ## Lemmas: [&#39;andiron&#39;, &#39;firedog&#39;, &#39;dog&#39;, &#39;dog-iron&#39;] ## Hyponyms: [] ## Hypernyms: [Synset(&#39;support.n.10&#39;)] ## ## ## Synset(&#39;chase.v.01&#39;) : ## Definition: go after with the intent to catch ## Example: [&#39;The policeman chased the mugger down the alley&#39;, &#39;the dog chased the rabbit&#39;] ## Lemmas: [&#39;chase&#39;, &#39;chase_after&#39;, &#39;trail&#39;, &#39;tail&#39;, &#39;tag&#39;, &#39;give_chase&#39;, &#39;dog&#39;, &#39;go_after&#39;, &#39;track&#39;] ## Hyponyms: [Synset(&#39;hound.v.01&#39;), Synset(&#39;quest.v.02&#39;), Synset(&#39;run_down.v.07&#39;), Synset(&#39;tree.v.03&#39;)] ## Hypernyms: [Synset(&#39;pursue.v.02&#39;)] 18.8 Part Of Speech (POS) In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context—i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph This is useful for Information Retrieval, Text to Speech, Word Sense Disambiguation The primary target of Part-of-Speech(POS) tagging is to identify the grammatical group of a given word. Whether it is a NOUN, PRONOUN, ADJECTIVE, VERB, ADVERBS, etc. based on the context A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc 18.8.1 Tag Sets Schools commonly teach that there are 9 parts of speech in English: noun, verb, article, adjective, preposition, pronoun, adverb, conjunction, and interjection However, there are clearly many more categories and sub-categories nltk.download(&#39;universal_tagset&#39;) 18.8.1.1 Universal Tagset This tagset contains 12 coarse tags VERB - verbs (all tenses and modes) NOUN - nouns (common and proper) PRON - pronouns ADJ - adjectives ADV - adverbs ADP - adpositions (prepositions and postpositions) CONJ - conjunctions DET - determiners NUM - cardinal numbers PRT - particles or other function words X - other: foreign words, typos, abbreviations . - punctuation 18.8.1.2 Penn Treebank Tagset This is the most popular “tag set” for American English, developed in the Penn Treebank project It has 36 POS tags plus 12 others for punctuations and special symbols PENN POS Tagset nltk.help.upenn_tagset() ## $: dollar ## $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$ ## &#39;&#39;: closing quotation mark ## &#39; &#39;&#39; ## (: opening parenthesis ## ( [ { ## ): closing parenthesis ## ) ] } ## ,: comma ## , ## --: dash ## -- ## .: sentence terminator ## . ! ? ## :: colon or ellipsis ## : ; ... ## CC: conjunction, coordinating ## &amp; &#39;n and both but either et for less minus neither nor or plus so ## therefore times v. versus vs. whether yet ## CD: numeral, cardinal ## mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty- ## seven 1987 twenty &#39;79 zero two 78-degrees eighty-four IX &#39;60s .025 ## fifteen 271,124 dozen quintillion DM2,000 ... ## DT: determiner ## all an another any both del each either every half la many much nary ## neither no some such that the them these this those ## EX: existential there ## there ## FW: foreign word ## gemeinschaft hund ich jeux habeas Haementeria Herr K&#39;ang-si vous ## lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte ## terram fiche oui corporis ... ## IN: preposition or conjunction, subordinating ## astride among uppon whether out inside pro despite on by throughout ## below within for towards near behind atop around if like until below ## next into if beside ... ## JJ: adjective or numeral, ordinal ## third ill-mannered pre-war regrettable oiled calamitous first separable ## ectoplasmic battery-powered participatory fourth still-to-be-named ## multilingual multi-disciplinary ... ## JJR: adjective, comparative ## bleaker braver breezier briefer brighter brisker broader bumper busier ## calmer cheaper choosier cleaner clearer closer colder commoner costlier ## cozier creamier crunchier cuter ... ## JJS: adjective, superlative ## calmest cheapest choicest classiest cleanest clearest closest commonest ## corniest costliest crassest creepiest crudest cutest darkest deadliest ## dearest deepest densest dinkiest ... ## LS: list item marker ## A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005 ## SP-44007 Second Third Three Two * a b c d first five four one six three ## two ## MD: modal auxiliary ## can cannot could couldn&#39;t dare may might must need ought shall should ## shouldn&#39;t will would ## NN: noun, common, singular or mass ## common-carrier cabbage knuckle-duster Casino afghan shed thermostat ## investment slide humour falloff slick wind hyena override subhumanity ## machinist ... ## NNP: noun, proper, singular ## Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos ## Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA ## Shannon A.K.C. Meltex Liverpool ... ## NNPS: noun, proper, plural ## Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists ## Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques ## Apache Apaches Apocrypha ... ## NNS: noun, common, plural ## undergraduates scotches bric-a-brac products bodyguards facets coasts ## divestitures storehouses designs clubs fragrances averages ## subjectivists apprehensions muses factory-jobs ... ## PDT: pre-determiner ## all both half many quite such sure this ## POS: genitive marker ## &#39; &#39;s ## PRP: pronoun, personal ## hers herself him himself hisself it itself me myself one oneself ours ## ourselves ownself self she thee theirs them themselves they thou thy us ## PRP$: pronoun, possessive ## her his mine my our ours their thy your ## RB: adverb ## occasionally unabatingly maddeningly adventurously professedly ## stirringly prominently technologically magisterially predominately ## swiftly fiscally pitilessly ... ## RBR: adverb, comparative ## further gloomier grander graver greater grimmer harder harsher ## healthier heavier higher however larger later leaner lengthier less- ## perfectly lesser lonelier longer louder lower more ... ## RBS: adverb, superlative ## best biggest bluntest earliest farthest first furthest hardest ## heartiest highest largest least less most nearest second tightest worst ## RP: particle ## aboard about across along apart around aside at away back before behind ## by crop down ever fast for forth from go high i.e. in into just later ## low more off on open out over per pie raising start teeth that through ## under unto up up-pp upon whole with you ## SYM: symbol ## % &amp; &#39; &#39;&#39; &#39;&#39;. ) ). * + ,. &lt; = &gt; @ A[fj] U.S U.S.S.R * ** *** ## TO: &quot;to&quot; as preposition or infinitive marker ## to ## UH: interjection ## Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen ## huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly ## man baby diddle hush sonuvabitch ... ## VB: verb, base form ## ask assemble assess assign assume atone attention avoid bake balkanize ## bank begin behold believe bend benefit bevel beware bless boil bomb ## boost brace break bring broil brush build ... ## VBD: verb, past tense ## dipped pleaded swiped regummed soaked tidied convened halted registered ## cushioned exacted snubbed strode aimed adopted belied figgered ## speculated wore appreciated contemplated ... ## VBG: verb, present participle or gerund ## telegraphing stirring focusing angering judging stalling lactating ## hankerin&#39; alleging veering capping approaching traveling besieging ## encrypting interrupting erasing wincing ... ## VBN: verb, past participle ## multihulled dilapidated aerosolized chaired languished panelized used ## experimented flourished imitated reunifed factored condensed sheared ## unsettled primed dubbed desired ... ## VBP: verb, present tense, not 3rd person singular ## predominate wrap resort sue twist spill cure lengthen brush terminate ## appear tend stray glisten obtain comprise detest tease attract ## emphasize mold postpone sever return wag ... ## VBZ: verb, present tense, 3rd person singular ## bases reconstructs marks mixes displeases seals carps weaves snatches ## slumps stretches authorizes smolders pictures emerges stockpiles ## seduces fizzes uses bolsters slaps speaks pleads ... ## WDT: WH-determiner ## that what whatever which whichever ## WP: WH-pronoun ## that what whatever whatsoever which who whom whosoever ## WP$: WH-pronoun, possessive ## whose ## WRB: Wh-adverb ## how however whence whenever where whereby whereever wherein whereof why ## ``: opening quotation mark ## ` `` 18.8.1.3 Claws5 Tagset Claws5 POS Tagset nltk.help.claws5_tagset() ## AJ0: adjective (unmarked) ## good, old ## AJC: comparative adjective ## better, older ## AJS: superlative adjective ## best, oldest ## AT0: article ## THE, A, AN ## AV0: adverb (unmarked) ## often, well, longer, furthest ## AVP: adverb particle ## up, off, out ## AVQ: wh-adverb ## when, how, why ## CJC: coordinating conjunction ## and, or ## CJS: subordinating conjunction ## although, when ## CJT: the conjunction THAT ## that ## CRD: cardinal numeral ## 3, fifty-five, 6609 (excl one) ## DPS: possessive determiner form ## your, their ## DT0: general determiner ## these, some ## DTQ: wh-determiner ## whose, which ## EX0: existential THERE ## there ## ITJ: interjection or other isolate ## oh, yes, mhm ## NN0: noun (neutral for number) ## aircraft, data ## NN1: singular noun ## pencil, goose ## NN2: plural noun ## pencils, geese ## NP0: proper noun ## London, Michael, Mars ## NULL: the null tag (for items not to be tagged) ## ORD: ordinal ## sixth, 77th, last ## PNI: indefinite pronoun ## none, everything ## PNP: personal pronoun ## you, them, ours ## PNQ: wh-pronoun ## who, whoever ## PNX: reflexive pronoun ## itself, ourselves ## POS: the possessive (or genitive morpheme) ## &#39;s or &#39; ## PRF: the preposition OF ## of ## PRP: preposition (except for OF) ## for, above, to ## PUL: punctuation ## left bracket - ( or [ ) ## PUN: punctuation ## general mark - . ! , : ; - ? ... ## PUQ: punctuation ## quotation mark - ` &#39; &quot; ## PUR: punctuation ## right bracket - ) or ] ## TO0: infinitive marker TO ## to ## UNC: &quot;unclassified&quot; items which are not words of the English lexicon ## VBB: the &quot;base forms&quot; of the verb &quot;BE&quot; (except the infinitive) ## am, are ## VBD: past form of the verb &quot;BE&quot; ## was, were ## VBG: -ing form of the verb &quot;BE&quot; ## being ## VBI: infinitive of the verb &quot;BE&quot; ## be ## VBN: past participle of the verb &quot;BE&quot; ## been ## VBZ: -s form of the verb &quot;BE&quot; ## is, &#39;s ## VDB: base form of the verb &quot;DO&quot; (except the infinitive) ## do ## VDD: past form of the verb &quot;DO&quot; ## did ## VDG: -ing form of the verb &quot;DO&quot; ## doing ## VDI: infinitive of the verb &quot;DO&quot; ## do ## VDN: past participle of the verb &quot;DO&quot; ## done ## VDZ: -s form of the verb &quot;DO&quot; ## does ## VHB: base form of the verb &quot;HAVE&quot; (except the infinitive) ## have ## VHD: past tense form of the verb &quot;HAVE&quot; ## had, &#39;d ## VHG: -ing form of the verb &quot;HAVE&quot; ## having ## VHI: infinitive of the verb &quot;HAVE&quot; ## have ## VHN: past participle of the verb &quot;HAVE&quot; ## had ## VHZ: -s form of the verb &quot;HAVE&quot; ## has, &#39;s ## VM0: modal auxiliary verb ## can, could, will, &#39;ll ## VVB: base form of lexical verb (except the infinitive) ## take, live ## VVD: past tense form of lexical verb ## took, lived ## VVG: -ing form of lexical verb ## taking, living ## VVI: infinitive of lexical verb ## take, live ## VVN: past participle form of lex. verb ## taken, lived ## VVZ: -s form of lexical verb ## takes, lives ## XX0: the negative NOT or N&#39;T ## not ## ZZ0: alphabetical symbol ## A, B, c, d 18.8.1.4 Brown Tagset Brown POS Tagset nltk.help.brown_tagset() ## (: opening parenthesis ## ( ## ): closing parenthesis ## ) ## *: negator ## not n&#39;t ## ,: comma ## , ## --: dash ## -- ## .: sentence terminator ## . ? ; ! : ## :: colon ## : ## ABL: determiner/pronoun, pre-qualifier ## quite such rather ## ABN: determiner/pronoun, pre-quantifier ## all half many nary ## ABX: determiner/pronoun, double conjunction or pre-quantifier ## both ## AP: determiner/pronoun, post-determiner ## many other next more last former little several enough most least only ## very few fewer past same Last latter less single plenty &#39;nough lesser ## certain various manye next-to-last particular final previous present ## nuf ## AP$: determiner/pronoun, post-determiner, genitive ## other&#39;s ## AP+AP: determiner/pronoun, post-determiner, hyphenated pair ## many-much ## AT: article ## the an no a every th&#39; ever&#39; ye ## BE: verb &#39;to be&#39;, infinitive or imperative ## be ## BED: verb &#39;to be&#39;, past tense, 2nd person singular or all persons plural ## were ## BED*: verb &#39;to be&#39;, past tense, 2nd person singular or all persons plural, negated ## weren&#39;t ## BEDZ: verb &#39;to be&#39;, past tense, 1st and 3rd person singular ## was ## BEDZ*: verb &#39;to be&#39;, past tense, 1st and 3rd person singular, negated ## wasn&#39;t ## BEG: verb &#39;to be&#39;, present participle or gerund ## being ## BEM: verb &#39;to be&#39;, present tense, 1st person singular ## am ## BEM*: verb &#39;to be&#39;, present tense, 1st person singular, negated ## ain&#39;t ## BEN: verb &#39;to be&#39;, past participle ## been ## BER: verb &#39;to be&#39;, present tense, 2nd person singular or all persons plural ## are art ## BER*: verb &#39;to be&#39;, present tense, 2nd person singular or all persons plural, negated ## aren&#39;t ain&#39;t ## BEZ: verb &#39;to be&#39;, present tense, 3rd person singular ## is ## BEZ*: verb &#39;to be&#39;, present tense, 3rd person singular, negated ## isn&#39;t ain&#39;t ## CC: conjunction, coordinating ## and or but plus &amp; either neither nor yet &#39;n&#39; and/or minus an&#39; ## CD: numeral, cardinal ## two one 1 four 2 1913 71 74 637 1937 8 five three million 87-31 29-5 ## seven 1,119 fifty-three 7.5 billion hundred 125,000 1,700 60 100 six ## ... ## CD$: numeral, cardinal, genitive ## 1960&#39;s 1961&#39;s .404&#39;s ## CS: conjunction, subordinating ## that as after whether before while like because if since for than altho ## until so unless though providing once lest s&#39;posin&#39; till whereas ## whereupon supposing tho&#39; albeit then so&#39;s &#39;fore ## DO: verb &#39;to do&#39;, uninflected present tense, infinitive or imperative ## do dost ## DO*: verb &#39;to do&#39;, uninflected present tense or imperative, negated ## don&#39;t ## DO+PPSS: verb &#39;to do&#39;, past or present tense + pronoun, personal, nominative, not 3rd person singular ## d&#39;you ## DOD: verb &#39;to do&#39;, past tense ## did done ## DOD*: verb &#39;to do&#39;, past tense, negated ## didn&#39;t ## DOZ: verb &#39;to do&#39;, present tense, 3rd person singular ## does ## DOZ*: verb &#39;to do&#39;, present tense, 3rd person singular, negated ## doesn&#39;t don&#39;t ## DT: determiner/pronoun, singular ## this each another that &#39;nother ## DT$: determiner/pronoun, singular, genitive ## another&#39;s ## DT+BEZ: determiner/pronoun + verb &#39;to be&#39;, present tense, 3rd person singular ## that&#39;s ## DT+MD: determiner/pronoun + modal auxillary ## that&#39;ll this&#39;ll ## DTI: determiner/pronoun, singular or plural ## any some ## DTS: determiner/pronoun, plural ## these those them ## DTS+BEZ: pronoun, plural + verb &#39;to be&#39;, present tense, 3rd person singular ## them&#39;s ## DTX: determiner, pronoun or double conjunction ## neither either one ## EX: existential there ## there ## EX+BEZ: existential there + verb &#39;to be&#39;, present tense, 3rd person singular ## there&#39;s ## EX+HVD: existential there + verb &#39;to have&#39;, past tense ## there&#39;d ## EX+HVZ: existential there + verb &#39;to have&#39;, present tense, 3rd person singular ## there&#39;s ## EX+MD: existential there + modal auxillary ## there&#39;ll there&#39;d ## FW-*: foreign word: negator ## pas non ne ## FW-AT: foreign word: article ## la le el un die der ein keine eine das las les Il ## FW-AT+NN: foreign word: article + noun, singular, common ## l&#39;orchestre l&#39;identite l&#39;arcade l&#39;ange l&#39;assistance l&#39;activite ## L&#39;Universite l&#39;independance L&#39;Union L&#39;Unita l&#39;osservatore ## FW-AT+NP: foreign word: article + noun, singular, proper ## L&#39;Astree L&#39;Imperiale ## FW-BE: foreign word: verb &#39;to be&#39;, infinitive or imperative ## sit ## FW-BER: foreign word: verb &#39;to be&#39;, present tense, 2nd person singular or all persons plural ## sind sunt etes ## FW-BEZ: foreign word: verb &#39;to be&#39;, present tense, 3rd person singular ## ist est ## FW-CC: foreign word: conjunction, coordinating ## et ma mais und aber och nec y ## FW-CD: foreign word: numeral, cardinal ## une cinq deux sieben unam zwei ## FW-CS: foreign word: conjunction, subordinating ## bevor quam ma ## FW-DT: foreign word: determiner/pronoun, singular ## hoc ## FW-DT+BEZ: foreign word: determiner + verb &#39;to be&#39;, present tense, 3rd person singular ## c&#39;est ## FW-DTS: foreign word: determiner/pronoun, plural ## haec ## FW-HV: foreign word: verb &#39;to have&#39;, present tense, not 3rd person singular ## habe ## FW-IN: foreign word: preposition ## ad de en a par con dans ex von auf super post sine sur sub avec per ## inter sans pour pendant in di ## FW-IN+AT: foreign word: preposition + article ## della des du aux zur d&#39;un del dell&#39; ## FW-IN+NN: foreign word: preposition + noun, singular, common ## d&#39;etat d&#39;hotel d&#39;argent d&#39;identite d&#39;art ## FW-IN+NP: foreign word: preposition + noun, singular, proper ## d&#39;Yquem d&#39;Eiffel ## FW-JJ: foreign word: adjective ## avant Espagnol sinfonica Siciliana Philharmonique grand publique haute ## noire bouffe Douce meme humaine bel serieuses royaux anticus presto ## Sovietskaya Bayerische comique schwarzen ... ## FW-JJR: foreign word: adjective, comparative ## fortiori ## FW-JJT: foreign word: adjective, superlative ## optimo ## FW-NN: foreign word: noun, singular, common ## ballet esprit ersatz mano chatte goutte sang Fledermaus oud def kolkhoz ## roi troika canto boite blutwurst carne muzyka bonheur monde piece force ## ... ## FW-NN$: foreign word: noun, singular, common, genitive ## corporis intellectus arte&#39;s dei aeternitatis senioritatis curiae ## patronne&#39;s chambre&#39;s ## FW-NNS: foreign word: noun, plural, common ## al culpas vopos boites haflis kolkhozes augen tyrannis alpha-beta- ## gammas metis banditos rata phis negociants crus Einsatzkommandos ## kamikaze wohaws sabinas zorrillas palazzi engages coureurs corroborees ## yori Ubermenschen ... ## FW-NP: foreign word: noun, singular, proper ## Karshilama Dieu Rundfunk Afrique Espanol Afrika Spagna Gott Carthago ## deus ## FW-NPS: foreign word: noun, plural, proper ## Svenskarna Atlantes Dieux ## FW-NR: foreign word: noun, singular, adverbial ## heute morgen aujourd&#39;hui hoy ## FW-OD: foreign word: numeral, ordinal ## 18e 17e quintus ## FW-PN: foreign word: pronoun, nominal ## hoc ## FW-PP$: foreign word: determiner, possessive ## mea mon deras vos ## FW-PPL: foreign word: pronoun, singular, reflexive ## se ## FW-PPL+VBZ: foreign word: pronoun, singular, reflexive + verb, present tense, 3rd person singular ## s&#39;excuse s&#39;accuse ## FW-PPO: pronoun, personal, accusative ## lui me moi mi ## FW-PPO+IN: foreign word: pronoun, personal, accusative + preposition ## mecum tecum ## FW-PPS: foreign word: pronoun, personal, nominative, 3rd person singular ## il ## FW-PPSS: foreign word: pronoun, personal, nominative, not 3rd person singular ## ich vous sie je ## FW-PPSS+HV: foreign word: pronoun, personal, nominative, not 3rd person singular + verb &#39;to have&#39;, present tense, not 3rd person singular ## j&#39;ai ## FW-QL: foreign word: qualifier ## minus ## FW-RB: foreign word: adverb ## bas assai deja um wiederum cito velociter vielleicht simpliciter non zu ## domi nuper sic forsan olim oui semper tout despues hors ## FW-RB+CC: foreign word: adverb + conjunction, coordinating ## forisque ## FW-TO+VB: foreign word: infinitival to + verb, infinitive ## d&#39;entretenir ## FW-UH: foreign word: interjection ## sayonara bien adieu arigato bonjour adios bueno tchalo ciao o ## FW-VB: foreign word: verb, present tense, not 3rd person singular, imperative or infinitive ## nolo contendere vive fermate faciunt esse vade noli tangere dites duces ## meminisse iuvabit gosaimasu voulez habla ksu&#39;u&#39;peli&#39;afo lacheln miuchi ## say allons strafe portant ## FW-VBD: foreign word: verb, past tense ## stabat peccavi audivi ## FW-VBG: foreign word: verb, present participle or gerund ## nolens volens appellant seq. obliterans servanda dicendi delenda ## FW-VBN: foreign word: verb, past participle ## vue verstrichen rasa verboten engages ## FW-VBZ: foreign word: verb, present tense, 3rd person singular ## gouverne sinkt sigue diapiace ## FW-WDT: foreign word: WH-determiner ## quo qua quod que quok ## FW-WPO: foreign word: WH-pronoun, accusative ## quibusdam ## FW-WPS: foreign word: WH-pronoun, nominative ## qui ## HV: verb &#39;to have&#39;, uninflected present tense, infinitive or imperative ## have hast ## HV*: verb &#39;to have&#39;, uninflected present tense or imperative, negated ## haven&#39;t ain&#39;t ## HV+TO: verb &#39;to have&#39;, uninflected present tense + infinitival to ## hafta ## HVD: verb &#39;to have&#39;, past tense ## had ## HVD*: verb &#39;to have&#39;, past tense, negated ## hadn&#39;t ## HVG: verb &#39;to have&#39;, present participle or gerund ## having ## HVN: verb &#39;to have&#39;, past participle ## had ## HVZ: verb &#39;to have&#39;, present tense, 3rd person singular ## has hath ## HVZ*: verb &#39;to have&#39;, present tense, 3rd person singular, negated ## hasn&#39;t ain&#39;t ## IN: preposition ## of in for by considering to on among at through with under into ## regarding than since despite according per before toward against as ## after during including between without except upon out over ... ## IN+IN: preposition, hyphenated pair ## f&#39;ovuh ## IN+PPO: preposition + pronoun, personal, accusative ## t&#39;hi-im ## JJ: adjective ## ecent over-all possible hard-fought favorable hard meager fit such ## widespread outmoded inadequate ambiguous grand clerical effective ## orderly federal foster general proportionate ... ## JJ$: adjective, genitive ## Great&#39;s ## JJ+JJ: adjective, hyphenated pair ## big-large long-far ## JJR: adjective, comparative ## greater older further earlier later freer franker wider better deeper ## firmer tougher faster higher bigger worse younger lighter nicer slower ## happier frothier Greater newer Elder ... ## JJR+CS: adjective + conjunction, coordinating ## lighter&#39;n ## JJS: adjective, semantically superlative ## top chief principal northernmost master key head main tops utmost ## innermost foremost uppermost paramount topmost ## JJT: adjective, superlative ## best largest coolest calmest latest greatest earliest simplest ## strongest newest fiercest unhappiest worst youngest worthiest fastest ## hottest fittest lowest finest smallest staunchest ... ## MD: modal auxillary ## should may might will would must can could shall ought need wilt ## MD*: modal auxillary, negated ## cannot couldn&#39;t wouldn&#39;t can&#39;t won&#39;t shouldn&#39;t shan&#39;t mustn&#39;t musn&#39;t ## MD+HV: modal auxillary + verb &#39;to have&#39;, uninflected form ## shouldda musta coulda must&#39;ve woulda could&#39;ve ## MD+PPSS: modal auxillary + pronoun, personal, nominative, not 3rd person singular ## willya ## MD+TO: modal auxillary + infinitival to ## oughta ## NN: noun, singular, common ## failure burden court fire appointment awarding compensation Mayor ## interim committee fact effect airport management surveillance jail ## doctor intern extern night weekend duty legislation Tax Office ... ## NN$: noun, singular, common, genitive ## season&#39;s world&#39;s player&#39;s night&#39;s chapter&#39;s golf&#39;s football&#39;s ## baseball&#39;s club&#39;s U.&#39;s coach&#39;s bride&#39;s bridegroom&#39;s board&#39;s county&#39;s ## firm&#39;s company&#39;s superintendent&#39;s mob&#39;s Navy&#39;s ... ## NN+BEZ: noun, singular, common + verb &#39;to be&#39;, present tense, 3rd person singular ## water&#39;s camera&#39;s sky&#39;s kid&#39;s Pa&#39;s heat&#39;s throat&#39;s father&#39;s money&#39;s ## undersecretary&#39;s granite&#39;s level&#39;s wife&#39;s fat&#39;s Knife&#39;s fire&#39;s name&#39;s ## hell&#39;s leg&#39;s sun&#39;s roulette&#39;s cane&#39;s guy&#39;s kind&#39;s baseball&#39;s ... ## NN+HVD: noun, singular, common + verb &#39;to have&#39;, past tense ## Pa&#39;d ## NN+HVZ: noun, singular, common + verb &#39;to have&#39;, present tense, 3rd person singular ## guy&#39;s Knife&#39;s boat&#39;s summer&#39;s rain&#39;s company&#39;s ## NN+IN: noun, singular, common + preposition ## buncha ## NN+MD: noun, singular, common + modal auxillary ## cowhand&#39;d sun&#39;ll ## NN+NN: noun, singular, common, hyphenated pair ## stomach-belly ## NNS: noun, plural, common ## irregularities presentments thanks reports voters laws legislators ## years areas adjustments chambers $100 bonds courts sales details raises ## sessions members congressmen votes polls calls ... ## NNS$: noun, plural, common, genitive ## taxpayers&#39; children&#39;s members&#39; States&#39; women&#39;s cutters&#39; motorists&#39; ## steelmakers&#39; hours&#39; Nations&#39; lawyers&#39; prisoners&#39; architects&#39; tourists&#39; ## Employers&#39; secretaries&#39; Rogues&#39; ... ## NNS+MD: noun, plural, common + modal auxillary ## duds&#39;d oystchers&#39;ll ## NP: noun, singular, proper ## Fulton Atlanta September-October Durwood Pye Ivan Allen Jr. Jan. ## Alpharetta Grady William B. Hartsfield Pearl Williams Aug. Berry J. M. ## Cheshire Griffin Opelika Ala. E. Pelham Snodgrass ... ## NP$: noun, singular, proper, genitive ## Green&#39;s Landis&#39; Smith&#39;s Carreon&#39;s Allison&#39;s Boston&#39;s Spahn&#39;s Willie&#39;s ## Mickey&#39;s Milwaukee&#39;s Mays&#39; Howsam&#39;s Mantle&#39;s Shaw&#39;s Wagner&#39;s Rickey&#39;s ## Shea&#39;s Palmer&#39;s Arnold&#39;s Broglio&#39;s ... ## NP+BEZ: noun, singular, proper + verb &#39;to be&#39;, present tense, 3rd person singular ## W.&#39;s Ike&#39;s Mack&#39;s Jack&#39;s Kate&#39;s Katharine&#39;s Black&#39;s Arthur&#39;s Seaton&#39;s ## Buckhorn&#39;s Breed&#39;s Penny&#39;s Rob&#39;s Kitty&#39;s Blackwell&#39;s Myra&#39;s Wally&#39;s ## Lucille&#39;s Springfield&#39;s Arlene&#39;s ## NP+HVZ: noun, singular, proper + verb &#39;to have&#39;, present tense, 3rd person singular ## Bill&#39;s Guardino&#39;s Celie&#39;s Skolman&#39;s Crosson&#39;s Tim&#39;s Wally&#39;s ## NP+MD: noun, singular, proper + modal auxillary ## Gyp&#39;ll John&#39;ll ## NPS: noun, plural, proper ## Chases Aderholds Chapelles Armisteads Lockies Carbones French Marskmen ## Toppers Franciscans Romans Cadillacs Masons Blacks Catholics British ## Dixiecrats Mississippians Congresses ... ## NPS$: noun, plural, proper, genitive ## Republicans&#39; Orioles&#39; Birds&#39; Yanks&#39; Redbirds&#39; Bucs&#39; Yankees&#39; Stevenses&#39; ## Geraghtys&#39; Burkes&#39; Wackers&#39; Achaeans&#39; Dresbachs&#39; Russians&#39; Democrats&#39; ## Gershwins&#39; Adventists&#39; Negroes&#39; Catholics&#39; ... ## NR: noun, singular, adverbial ## Friday home Wednesday Tuesday Monday Sunday Thursday yesterday tomorrow ## tonight West East Saturday west left east downtown north northeast ## southeast northwest North South right ... ## NR$: noun, singular, adverbial, genitive ## Saturday&#39;s Monday&#39;s yesterday&#39;s tonight&#39;s tomorrow&#39;s Sunday&#39;s ## Wednesday&#39;s Friday&#39;s today&#39;s Tuesday&#39;s West&#39;s Today&#39;s South&#39;s ## NR+MD: noun, singular, adverbial + modal auxillary ## today&#39;ll ## NRS: noun, plural, adverbial ## Sundays Mondays Saturdays Wednesdays Souths Fridays ## OD: numeral, ordinal ## first 13th third nineteenth 2d 61st second sixth eighth ninth twenty- ## first eleventh 50th eighteenth- Thirty-ninth 72nd 1/20th twentieth ## mid-19th thousandth 350th sixteenth 701st ... ## PN: pronoun, nominal ## none something everything one anyone nothing nobody everybody everyone ## anybody anything someone no-one nothin ## PN$: pronoun, nominal, genitive ## one&#39;s someone&#39;s anybody&#39;s nobody&#39;s everybody&#39;s anyone&#39;s everyone&#39;s ## PN+BEZ: pronoun, nominal + verb &#39;to be&#39;, present tense, 3rd person singular ## nothing&#39;s everything&#39;s somebody&#39;s nobody&#39;s someone&#39;s ## PN+HVD: pronoun, nominal + verb &#39;to have&#39;, past tense ## nobody&#39;d ## PN+HVZ: pronoun, nominal + verb &#39;to have&#39;, present tense, 3rd person singular ## nobody&#39;s somebody&#39;s one&#39;s ## PN+MD: pronoun, nominal + modal auxillary ## someone&#39;ll somebody&#39;ll anybody&#39;d ## PP$: determiner, possessive ## our its his their my your her out thy mine thine ## PP$$: pronoun, possessive ## ours mine his hers theirs yours ## PPL: pronoun, singular, reflexive ## itself himself myself yourself herself oneself ownself ## PPLS: pronoun, plural, reflexive ## themselves ourselves yourselves ## PPO: pronoun, personal, accusative ## them it him me us you &#39;em her thee we&#39;uns ## PPS: pronoun, personal, nominative, 3rd person singular ## it he she thee ## PPS+BEZ: pronoun, personal, nominative, 3rd person singular + verb &#39;to be&#39;, present tense, 3rd person singular ## it&#39;s he&#39;s she&#39;s ## PPS+HVD: pronoun, personal, nominative, 3rd person singular + verb &#39;to have&#39;, past tense ## she&#39;d he&#39;d it&#39;d ## PPS+HVZ: pronoun, personal, nominative, 3rd person singular + verb &#39;to have&#39;, present tense, 3rd person singular ## it&#39;s he&#39;s she&#39;s ## PPS+MD: pronoun, personal, nominative, 3rd person singular + modal auxillary ## he&#39;ll she&#39;ll it&#39;ll he&#39;d it&#39;d she&#39;d ## PPSS: pronoun, personal, nominative, not 3rd person singular ## they we I you ye thou you&#39;uns ## PPSS+BEM: pronoun, personal, nominative, not 3rd person singular + verb &#39;to be&#39;, present tense, 1st person singular ## I&#39;m Ahm ## PPSS+BER: pronoun, personal, nominative, not 3rd person singular + verb &#39;to be&#39;, present tense, 2nd person singular or all persons plural ## we&#39;re you&#39;re they&#39;re ## PPSS+BEZ: pronoun, personal, nominative, not 3rd person singular + verb &#39;to be&#39;, present tense, 3rd person singular ## you&#39;s ## PPSS+BEZ*: pronoun, personal, nominative, not 3rd person singular + verb &#39;to be&#39;, present tense, 3rd person singular, negated ## &#39;tain&#39;t ## PPSS+HV: pronoun, personal, nominative, not 3rd person singular + verb &#39;to have&#39;, uninflected present tense ## I&#39;ve we&#39;ve they&#39;ve you&#39;ve ## PPSS+HVD: pronoun, personal, nominative, not 3rd person singular + verb &#39;to have&#39;, past tense ## I&#39;d you&#39;d we&#39;d they&#39;d ## PPSS+MD: pronoun, personal, nominative, not 3rd person singular + modal auxillary ## you&#39;ll we&#39;ll I&#39;ll we&#39;d I&#39;d they&#39;ll they&#39;d you&#39;d ## PPSS+VB: pronoun, personal, nominative, not 3rd person singular + verb &#39;to verb&#39;, uninflected present tense ## y&#39;know ## QL: qualifier, pre ## well less very most so real as highly fundamentally even how much ## remarkably somewhat more completely too thus ill deeply little overly ## halfway almost impossibly far severly such ... ## QLP: qualifier, post ## indeed enough still &#39;nuff ## RB: adverb ## only often generally also nevertheless upon together back newly no ## likely meanwhile near then heavily there apparently yet outright fully ## aside consistently specifically formally ever just ... ## RB$: adverb, genitive ## else&#39;s ## RB+BEZ: adverb + verb &#39;to be&#39;, present tense, 3rd person singular ## here&#39;s there&#39;s ## RB+CS: adverb + conjunction, coordinating ## well&#39;s soon&#39;s ## RBR: adverb, comparative ## further earlier better later higher tougher more harder longer sooner ## less faster easier louder farther oftener nearer cheaper slower tighter ## lower worse heavier quicker ... ## RBR+CS: adverb, comparative + conjunction, coordinating ## more&#39;n ## RBT: adverb, superlative ## most best highest uppermost nearest brightest hardest fastest deepest ## farthest loudest ... ## RN: adverb, nominal ## here afar then ## RP: adverb, particle ## up out off down over on in about through across after ## RP+IN: adverb, particle + preposition ## out&#39;n outta ## TO: infinitival to ## to t&#39; ## TO+VB: infinitival to + verb, infinitive ## t&#39;jawn t&#39;lah ## UH: interjection ## Hurrah bang whee hmpf ah goodbye oops oh-the-pain-of-it ha crunch say ## oh why see well hello lo alas tarantara rum-tum-tum gosh hell keerist ## Jesus Keeeerist boy c&#39;mon &#39;mon goddamn bah hoo-pig damn ... ## VB: verb, base: uninflected present, imperative or infinitive ## investigate find act follow inure achieve reduce take remedy re-set ## distribute realize disable feel receive continue place protect ## eliminate elaborate work permit run enter force ... ## VB+AT: verb, base: uninflected present or infinitive + article ## wanna ## VB+IN: verb, base: uninflected present, imperative or infinitive + preposition ## lookit ## VB+JJ: verb, base: uninflected present, imperative or infinitive + adjective ## die-dead ## VB+PPO: verb, uninflected present tense + pronoun, personal, accusative ## let&#39;s lemme gimme ## VB+RP: verb, imperative + adverbial particle ## g&#39;ahn c&#39;mon ## VB+TO: verb, base: uninflected present, imperative or infinitive + infinitival to ## wanta wanna ## VB+VB: verb, base: uninflected present, imperative or infinitive; hypenated pair ## say-speak ## VBD: verb, past tense ## said produced took recommended commented urged found added praised ## charged listed became announced brought attended wanted voted defeated ## received got stood shot scheduled feared promised made ... ## VBG: verb, present participle or gerund ## modernizing improving purchasing Purchasing lacking enabling pricing ## keeping getting picking entering voting warning making strengthening ## setting neighboring attending participating moving ... ## VBG+TO: verb, present participle + infinitival to ## gonna ## VBN: verb, past participle ## conducted charged won received studied revised operated accepted ## combined experienced recommended effected granted seen protected ## adopted retarded notarized selected composed gotten printed ... ## VBN+TO: verb, past participle + infinitival to ## gotta ## VBZ: verb, present tense, 3rd person singular ## deserves believes receives takes goes expires says opposes starts ## permits expects thinks faces votes teaches holds calls fears spends ## collects backs eliminates sets flies gives seeks reads ... ## WDT: WH-determiner ## which what whatever whichever whichever-the-hell ## WDT+BER: WH-determiner + verb &#39;to be&#39;, present tense, 2nd person singular or all persons plural ## what&#39;re ## WDT+BER+PP: WH-determiner + verb &#39;to be&#39;, present, 2nd person singular or all persons plural + pronoun, personal, nominative, not 3rd person singular ## whaddya ## WDT+BEZ: WH-determiner + verb &#39;to be&#39;, present tense, 3rd person singular ## what&#39;s ## WDT+DO+PPS: WH-determiner + verb &#39;to do&#39;, uninflected present tense + pronoun, personal, nominative, not 3rd person singular ## whaddya ## WDT+DOD: WH-determiner + verb &#39;to do&#39;, past tense ## what&#39;d ## WDT+HVZ: WH-determiner + verb &#39;to have&#39;, present tense, 3rd person singular ## what&#39;s ## WP$: WH-pronoun, genitive ## whose whosever ## WPO: WH-pronoun, accusative ## whom that who ## WPS: WH-pronoun, nominative ## that who whoever whosoever what whatsoever ## WPS+BEZ: WH-pronoun, nominative + verb &#39;to be&#39;, present, 3rd person singular ## that&#39;s who&#39;s ## WPS+HVD: WH-pronoun, nominative + verb &#39;to have&#39;, past tense ## who&#39;d ## WPS+HVZ: WH-pronoun, nominative + verb &#39;to have&#39;, present tense, 3rd person singular ## who&#39;s that&#39;s ## WPS+MD: WH-pronoun, nominative + modal auxillary ## who&#39;ll that&#39;d who&#39;d that&#39;ll ## WQL: WH-qualifier ## however how ## WRB: WH-adverb ## however when where why whereby wherever how whenever whereon wherein ## wherewith wheare wherefore whereof howsabout ## WRB+BER: WH-adverb + verb &#39;to be&#39;, present, 2nd person singular or all persons plural ## where&#39;re ## WRB+BEZ: WH-adverb + verb &#39;to be&#39;, present, 3rd person singular ## how&#39;s where&#39;s ## WRB+DO: WH-adverb + verb &#39;to do&#39;, present, not 3rd person singular ## howda ## WRB+DOD: WH-adverb + verb &#39;to do&#39;, past tense ## where&#39;d how&#39;d ## WRB+DOD*: WH-adverb + verb &#39;to do&#39;, past tense, negated ## whyn&#39;t ## WRB+DOZ: WH-adverb + verb &#39;to do&#39;, present tense, 3rd person singular ## how&#39;s ## WRB+IN: WH-adverb + preposition ## why&#39;n ## WRB+MD: WH-adverb + modal auxillary ## where&#39;d 18.8.2 Tagging Techniques There are few types of tagging techniques: Lexical-based Rule-based (Brill) Probalistic/Stochastic-based (Conditional Random Fields-CRFs, Hidden Markov Models-HMM) Neural network-based NLTK supports the below taggers: from nltk.tag.brill import BrillTagger from nltk.tag.hunpos import HunposTagger from nltk.tag.stanford import StanfordTagger, StanfordPOSTagger, StanfordNERTagger from nltk.tag.hmm import HiddenMarkovModelTagger, HiddenMarkovModelTrainer from nltk.tag.senna import SennaTagger, SennaChunkTagger, SennaNERTagger from nltk.tag.crf import CRFTagger from nltk.tag.perceptron import PerceptronTagger 18.8.2.1 nltk PerceptronTagger PerceptronTagger produce tags with Penn Treebank tagset from nltk.tag import PerceptronTagger tagger = PerceptronTagger() print(&#39;Tagger Classes:&#39;, tagger.classes, &#39;\\n\\n# Classes:&#39;, len(tagger.classes)) ## Tagger Classes: {&#39;)&#39;, &#39;:&#39;, &#39;VBN&#39;, &#39;NNPS&#39;, &#39;PDT&#39;, &#39;VBZ&#39;, &#39;PRP&#39;, &#39;POS&#39;, &#39;IN&#39;, &#39;NNP&#39;, &#39;#&#39;, &#39;DT&#39;, &#39;TO&#39;, &#39;WRB&#39;, &#39;,&#39;, &#39;WP&#39;, &#39;UH&#39;, &#39;CD&#39;, &#39;JJS&#39;, &#39;``&#39;, &#39;LS&#39;, &#39;CC&#39;, &#39;JJ&#39;, &#39;VBD&#39;, &#39;.&#39;, &#39;WDT&#39;, &#39;RBS&#39;, &#39;RP&#39;, &#39;RBR&#39;, &#39;RB&#39;, &#39;$&#39;, &#39;VB&#39;, &#39;PRP$&#39;, &quot;&#39;&#39;&quot;, &#39;JJR&#39;, &#39;(&#39;, &#39;NN&#39;, &#39;EX&#39;, &#39;WP$&#39;, &#39;SYM&#39;, &#39;MD&#39;, &#39;VBP&#39;, &#39;FW&#39;, &#39;NNS&#39;, &#39;VBG&#39;} ## ## # Classes: 45 18.8.3 Performing Tagging nltk.pos_tag() Tagging works sentence by sentence: Document fist must be splitted into sentences Each sentence need to be tokenized into words Default NTLK uses PerceptronTagger #nltk.download(&#39;averaged_perceptron_tagger&#39;) #import nltk #from nltk.tokenize import word_tokenize, sent_tokenize doc = &#39;&#39;&#39;Sukanya, Rajib and Naba are my good friends. Sukanya is getting married next year. Marriage is a big step in one&#39;s life. It is both exciting and frightening. But friendship is a sacred bond between people. It is a special kind of love between us. Many of you must have tried searching for a friend but never found the right one.&#39;&#39;&#39; sentences = nltk.sent_tokenize(doc) for sentence in sentences: tokens = nltk.word_tokenize(sentence) tagged = nltk.pos_tag(tokens) print(tagged) ## [(&#39;Sukanya&#39;, &#39;NNP&#39;), (&#39;,&#39;, &#39;,&#39;), (&#39;Rajib&#39;, &#39;NNP&#39;), (&#39;and&#39;, &#39;CC&#39;), (&#39;Naba&#39;, &#39;NNP&#39;), (&#39;are&#39;, &#39;VBP&#39;), (&#39;my&#39;, &#39;PRP$&#39;), (&#39;good&#39;, &#39;JJ&#39;), (&#39;friends&#39;, &#39;NNS&#39;), (&#39;.&#39;, &#39;.&#39;)] ## [(&#39;Sukanya&#39;, &#39;NNP&#39;), (&#39;is&#39;, &#39;VBZ&#39;), (&#39;getting&#39;, &#39;VBG&#39;), (&#39;married&#39;, &#39;VBN&#39;), (&#39;next&#39;, &#39;JJ&#39;), (&#39;year&#39;, &#39;NN&#39;), (&#39;.&#39;, &#39;.&#39;)] ## [(&#39;Marriage&#39;, &#39;NN&#39;), (&#39;is&#39;, &#39;VBZ&#39;), (&#39;a&#39;, &#39;DT&#39;), (&#39;big&#39;, &#39;JJ&#39;), (&#39;step&#39;, &#39;NN&#39;), (&#39;in&#39;, &#39;IN&#39;), (&#39;one&#39;, &#39;CD&#39;), (&quot;&#39;s&quot;, &#39;POS&#39;), (&#39;life&#39;, &#39;NN&#39;), (&#39;.&#39;, &#39;.&#39;)] ## [(&#39;It&#39;, &#39;PRP&#39;), (&#39;is&#39;, &#39;VBZ&#39;), (&#39;both&#39;, &#39;DT&#39;), (&#39;exciting&#39;, &#39;VBG&#39;), (&#39;and&#39;, &#39;CC&#39;), (&#39;frightening&#39;, &#39;NN&#39;), (&#39;.&#39;, &#39;.&#39;)] ## [(&#39;But&#39;, &#39;CC&#39;), (&#39;friendship&#39;, &#39;NN&#39;), (&#39;is&#39;, &#39;VBZ&#39;), (&#39;a&#39;, &#39;DT&#39;), (&#39;sacred&#39;, &#39;JJ&#39;), (&#39;bond&#39;, &#39;NN&#39;), (&#39;between&#39;, &#39;IN&#39;), (&#39;people&#39;, &#39;NNS&#39;), (&#39;.&#39;, &#39;.&#39;)] ## [(&#39;It&#39;, &#39;PRP&#39;), (&#39;is&#39;, &#39;VBZ&#39;), (&#39;a&#39;, &#39;DT&#39;), (&#39;special&#39;, &#39;JJ&#39;), (&#39;kind&#39;, &#39;NN&#39;), (&#39;of&#39;, &#39;IN&#39;), (&#39;love&#39;, &#39;NN&#39;), (&#39;between&#39;, &#39;IN&#39;), (&#39;us&#39;, &#39;PRP&#39;), (&#39;.&#39;, &#39;.&#39;)] ## [(&#39;Many&#39;, &#39;JJ&#39;), (&#39;of&#39;, &#39;IN&#39;), (&#39;you&#39;, &#39;PRP&#39;), (&#39;must&#39;, &#39;MD&#39;), (&#39;have&#39;, &#39;VB&#39;), (&#39;tried&#39;, &#39;VBN&#39;), (&#39;searching&#39;, &#39;VBG&#39;), (&#39;for&#39;, &#39;IN&#39;), (&#39;a&#39;, &#39;DT&#39;), (&#39;friend&#39;, &#39;NN&#39;), (&#39;but&#39;, &#39;CC&#39;), (&#39;never&#39;, &#39;RB&#39;), (&#39;found&#39;, &#39;VBD&#39;), (&#39;the&#39;, &#39;DT&#39;), (&#39;right&#39;, &#39;JJ&#39;), (&#39;one&#39;, &#39;NN&#39;), (&#39;.&#39;, &#39;.&#39;)] 18.9 Sentiment 18.9.1 NLTK and Senti-Wordnet SentiWordNet extends Wordnet Synsets with positive and negative sentiment scores The extension was achieved via a complex mix of propagation methods and classifiers. It is thus not a gold standard resource like WordNet (which was compiled by humans), but it has proven useful in a wide range of tasks It contains similar number of synsets as wordnet from nltk.corpus import sentiwordnet as swn nltk.download(&#39;sentiwordnet&#39;) ## True ## ## [nltk_data] Downloading package sentiwordnet to C:\\Users\\keh- ## [nltk_data] soon.yong\\AppData\\Roaming\\nltk_data... ## [nltk_data] Package sentiwordnet is already up-to-date! s = set( swn.all_senti_synsets() ) print(&#39;Total synsets in senti-wordnet : &#39; , len(s)) ## Total synsets in senti-wordnet : 117659 18.9.1.1 Senti-Synset Senti-Wordnet extends wordnet with three(3) sentiment scores: positive, negative, objective All three scores added up to value 1.0 breakdown = swn.senti_synset(&#39;breakdown.n.03&#39;) print( breakdown, &#39;\\n&#39; &#39;Positive:&#39;, breakdown.pos_score(), &#39;\\n&#39;, &#39;Negative:&#39;, breakdown.neg_score(), &#39;\\n&#39;, &#39;Objective:&#39;,breakdown.obj_score() ) ## &lt;breakdown.n.03: PosScore=0.0 NegScore=0.25&gt; ## Positive: 0.0 ## Negative: 0.25 ## Objective: 0.75 18.9.1.2 Senti-Synsets Get all the synonmys, with and without the POS information print( list(swn.senti_synsets(&#39;slow&#39;)), &#39;\\n\\n&#39;, ## without POS tag list(swn.senti_synsets(&#39;slow&#39;, &#39;a&#39;)) ) ## with POS tag ## [SentiSynset(&#39;decelerate.v.01&#39;), SentiSynset(&#39;slow.v.02&#39;), SentiSynset(&#39;slow.v.03&#39;), SentiSynset(&#39;slow.a.01&#39;), SentiSynset(&#39;slow.a.02&#39;), SentiSynset(&#39;dense.s.04&#39;), SentiSynset(&#39;slow.a.04&#39;), SentiSynset(&#39;boring.s.01&#39;), SentiSynset(&#39;dull.s.08&#39;), SentiSynset(&#39;slowly.r.01&#39;), SentiSynset(&#39;behind.r.03&#39;)] ## ## [SentiSynset(&#39;slow.a.01&#39;), SentiSynset(&#39;slow.a.02&#39;), SentiSynset(&#39;dense.s.04&#39;), SentiSynset(&#39;slow.a.04&#39;), SentiSynset(&#39;boring.s.01&#39;), SentiSynset(&#39;dull.s.08&#39;)] Get the score for first synset first_synset = list(swn.senti_synsets(&#39;slow&#39;,&#39;a&#39;))[0] print( first_synset, &#39;\\n&#39;, &#39;Positive:&#39;, first_synset.pos_score(), &#39;\\n&#39;, &#39;Negative:&#39;, first_synset.neg_score(), &#39;\\n&#39;, &#39;Objective:&#39;, first_synset.obj_score() ) ## &lt;slow.a.01: PosScore=0.0 NegScore=0.0&gt; ## Positive: 0.0 ## Negative: 0.0 ## Objective: 1.0 18.9.1.3 Converting POS-tag into Wordnet POS-tag Using Function import nltk from nltk.tokenize import word_tokenize from nltk.corpus import wordnet as wn def penn_to_wn(tag): &quot;&quot;&quot; Convert between the PennTreebank tags to simple Wordnet tags &quot;&quot;&quot; if tag.startswith(&#39;J&#39;): return wn.ADJ elif tag.startswith(&#39;N&#39;): return wn.NOUN elif tag.startswith(&#39;R&#39;): return wn.ADV elif tag.startswith(&#39;V&#39;): return wn.VERB return None wt = word_tokenize(&quot;Star Wars is a wonderful movie&quot;) penn_tags = nltk.pos_tag(wt) wordnet_tags = [ (x, penn_to_wn(y)) for (x,y) in penn_tags ] print( &#39;Penn Tags :&#39;, penn_tags, &#39;\\nWordnet Tags :&#39;, wordnet_tags) ## Penn Tags : [(&#39;Star&#39;, &#39;NNP&#39;), (&#39;Wars&#39;, &#39;NNP&#39;), (&#39;is&#39;, &#39;VBZ&#39;), (&#39;a&#39;, &#39;DT&#39;), (&#39;wonderful&#39;, &#39;JJ&#39;), (&#39;movie&#39;, &#39;NN&#39;)] ## Wordnet Tags : [(&#39;Star&#39;, &#39;n&#39;), (&#39;Wars&#39;, &#39;n&#39;), (&#39;is&#39;, &#39;v&#39;), (&#39;a&#39;, None), (&#39;wonderful&#39;, &#39;a&#39;), (&#39;movie&#39;, &#39;n&#39;)] Using defaultdict import nltk from nltk.corpus import wordnet as wn from nltk import word_tokenize, pos_tag from collections import defaultdict tag_map = defaultdict(lambda : None) tag_map[&#39;J&#39;] = wn.ADJ tag_map[&#39;R&#39;] = wn.ADV tag_map[&#39;V&#39;] = wn.VERB tag_map[&#39;N&#39;] = wn.NOUN wt = word_tokenize(&quot;Star Wars is a wonderful movie&quot;) penn_tags = nltk.pos_tag(wt) wordnet_tags = [ (x, tag_map[y[0]]) for (x,y) in penn_tags ] print( &#39;Penn Tags :&#39;, penn_tags, &#39;\\nWordnet Tags :&#39;, wordnet_tags) ## Penn Tags : [(&#39;Star&#39;, &#39;NNP&#39;), (&#39;Wars&#39;, &#39;NNP&#39;), (&#39;is&#39;, &#39;VBZ&#39;), (&#39;a&#39;, &#39;DT&#39;), (&#39;wonderful&#39;, &#39;JJ&#39;), (&#39;movie&#39;, &#39;NN&#39;)] ## Wordnet Tags : [(&#39;Star&#39;, &#39;n&#39;), (&#39;Wars&#39;, &#39;n&#39;), (&#39;is&#39;, &#39;v&#39;), (&#39;a&#39;, None), (&#39;wonderful&#39;, &#39;a&#39;), (&#39;movie&#39;, &#39;n&#39;)] 18.9.2 Vader It is a rule based sentiment analyzer, contain 7503 lexicons It is good for social media because lexicon contain emoji and short form text Contain only 3 n-gram Supported by NTLK or install vader seperately (pip install vaderSentiment) 18.9.2.1 Vader Lexicon The lexicon is a dictionary. To make it iterable, need to convert into list: - Step 1: Convert dict to dict_items, which is a list containing items, each item is one dict - Step 2: Unpack dict_items to list #from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer ## seperate pip installed library from nltk.sentiment.vader import SentimentIntensityAnalyzer vader_lex = SentimentIntensityAnalyzer().lexicon # get the lexicon dictionary vader_list = list(vader_lex.items()) # convert to items then list print( &#39;Total Vader Lexicon:&#39;, len(vader_lex),&#39;\\n&#39;, vader_list[1:10], vader_list[220:240] ) ## Total Vader Lexicon: 7502 ## [(&#39;%)&#39;, -0.4), (&#39;%-)&#39;, -1.5), (&#39;&amp;-:&#39;, -0.4), (&#39;&amp;:&#39;, -0.7), (&quot;( &#39;}{&#39; )&quot;, 1.6), (&#39;(%&#39;, -0.9), (&quot;(&#39;-:&quot;, 2.2), (&quot;(&#39;:&quot;, 2.3), (&#39;((-:&#39;, 2.1)] [(&#39;b^d&#39;, 2.6), (&#39;cwot&#39;, -2.3), (&quot;d-&#39;:&quot;, -2.5), (&#39;d8&#39;, -3.2), (&#39;d:&#39;, 1.2), (&#39;d:&lt;&#39;, -3.2), (&#39;d;&#39;, -2.9), (&#39;d=&#39;, 1.5), (&#39;doa&#39;, -2.3), (&#39;dx&#39;, -3.0), (&#39;ez&#39;, 1.5), (&#39;fav&#39;, 2.0), (&#39;fcol&#39;, -1.8), (&#39;ff&#39;, 1.8), (&#39;ffs&#39;, -2.8), (&#39;fkm&#39;, -2.4), (&#39;foaf&#39;, 1.8), (&#39;ftw&#39;, 2.0), (&#39;fu&#39;, -3.7), (&#39;fubar&#39;, -3.0)] There is only four N-Gram in the lexicon print(&#39;List of N-grams: &#39;) ## List of N-grams: [ (tok,score) for tok, score in vader_list if &quot; &quot; in tok] ## [(&quot;( &#39;}{&#39; )&quot;, 1.6), (&quot;can&#39;t stand&quot;, -2.0), (&#39;fed up&#39;, -1.8), (&#39;screwed up&#39;, -1.5)] If stemming or lemmatization is used, stem/lemmatize the vader lexicon too [ (tok,score) for tok, score in vader_list if &quot;lov&quot; in tok] ## [(&#39;beloved&#39;, 2.3), (&#39;lovable&#39;, 3.0), (&#39;love&#39;, 3.2), (&#39;loved&#39;, 2.9), (&#39;lovelies&#39;, 2.2), (&#39;lovely&#39;, 2.8), (&#39;lover&#39;, 2.8), (&#39;loverly&#39;, 2.8), (&#39;lovers&#39;, 2.4), (&#39;loves&#39;, 2.7), (&#39;loving&#39;, 2.9), (&#39;lovingly&#39;, 3.2), (&#39;lovingness&#39;, 2.7), (&#39;unlovable&#39;, -2.7), (&#39;unloved&#39;, -1.9), (&#39;unlovelier&#39;, -1.9), (&#39;unloveliest&#39;, -1.9), (&#39;unloveliness&#39;, -2.0), (&#39;unlovely&#39;, -2.1), (&#39;unloving&#39;, -2.3)] 18.9.2.2 Polarity Scoring Scoring result is a dictionary of: neg neu pos compound neg, neu, pos adds up to 1.0 Example below shows polarity for two sentences: corpus = [&quot;Python is a very useful but hell difficult to learn&quot;, &quot;:) :) :(&quot;] for doc in corpus: print(doc, &#39;--&gt;&#39;, &quot;\\n:&quot;, SentimentIntensityAnalyzer().polarity_scores(doc) ) ## Python is a very useful but hell difficult to learn --&gt; ## : {&#39;neg&#39;: 0.554, &#39;neu&#39;: 0.331, &#39;pos&#39;: 0.116, &#39;compound&#39;: -0.8735} ## :) :) :( --&gt; ## : {&#39;neg&#39;: 0.326, &#39;neu&#39;: 0.0, &#39;pos&#39;: 0.674, &#39;compound&#39;: 0.4767} 18.10 Feature Representation 18.10.1 The Data A corpus is a collection of multiple documents. In the below example, each document is represented by a sentence. corpus = [ &#39;This is the first document, :)&#39;, &#39;This document is the second document.&#39;, &#39;And this is a third one&#39;, &#39;Is this the first document?&#39;, ] 18.10.2 Frequency Count Using purely frequency count as a feature will obviously bias on long document (which contain a lot of words, hence words within the document will have very high frequency). 18.10.2.1 + Tokenizer Default Tokenizer By default, vectorizer apply tokenizer to select minimum 2-chars alphanumeric words. Below train the vectorizer using fit_transform(). from sklearn.feature_extraction.text import CountVectorizer vec = CountVectorizer() # initialize the vectorizer X = vec.fit_transform(corpus) # FIT the vectorizer, return fitted data print(pd.DataFrame(X.toarray(), columns=vec.get_feature_names()),&#39;\\n\\n&#39;, &#39;Vocabulary: &#39;, vec.vocabulary_) ## and document first is one second the third this ## 0 0 1 1 1 0 0 1 0 1 ## 1 0 2 0 1 0 1 1 0 1 ## 2 1 0 0 1 1 0 0 1 1 ## 3 0 1 1 1 0 0 1 0 1 ## ## Vocabulary: {&#39;this&#39;: 8, &#39;is&#39;: 3, &#39;the&#39;: 6, &#39;first&#39;: 2, &#39;document&#39;: 1, &#39;second&#39;: 5, &#39;and&#39;: 0, &#39;third&#39;: 7, &#39;one&#39;: 4} Custom Tokenizer You can use a custom tokenizer, which is a function that return list of words. Example below uses nltk RegexpTokenizer function, which retains one or more alphanumeric characters. my_tokenizer = RegexpTokenizer(r&#39;[a-zA-Z0-9\\&#39;]+&#39;) ## Custom Tokenizer vec2 = CountVectorizer(tokenizer=my_tokenizer.tokenize) ## custom tokenizer&#39;s function X2 = vec2.fit_transform(corpus) # FIT the vectorizer, return fitted data print(pd.DataFrame(X2.toarray(), columns=vec2.get_feature_names()),&#39;\\n\\n&#39;, &#39;Vocabulary: &#39;, vec.vocabulary_) ## a and document first is one second the third this ## 0 0 0 1 1 1 0 0 1 0 1 ## 1 0 0 2 0 1 0 1 1 0 1 ## 2 1 1 0 0 1 1 0 0 1 1 ## 3 0 0 1 1 1 0 0 1 0 1 ## ## Vocabulary: {&#39;this&#39;: 8, &#39;is&#39;: 3, &#39;the&#39;: 6, &#39;first&#39;: 2, &#39;document&#39;: 1, &#39;second&#39;: 5, &#39;and&#39;: 0, &#39;third&#39;: 7, &#39;one&#39;: 4} 1 and 2-Word-Gram Tokenizer Use ngram_range() to specify range of grams needed. vec3 = CountVectorizer(ngram_range=(1,2)) # initialize the vectorizer X3 = vec3.fit_transform(corpus) # FIT the vectorizer, return fitted data print(pd.DataFrame(X3.toarray(), columns=vec3.get_feature_names()),&#39;\\n\\n&#39;, &#39;Vocabulary: &#39;, vec.vocabulary_) ## and and this document document is first ... third one this this document \\ ## 0 0 0 1 0 1 ... 0 1 0 ## 1 0 0 2 1 0 ... 0 1 1 ## 2 1 1 0 0 0 ... 1 1 0 ## 3 0 0 1 0 1 ... 0 1 0 ## ## this is this the ## 0 1 0 ## 1 0 0 ## 2 1 0 ## 3 0 1 ## ## [4 rows x 22 columns] ## ## Vocabulary: {&#39;this&#39;: 8, &#39;is&#39;: 3, &#39;the&#39;: 6, &#39;first&#39;: 2, &#39;document&#39;: 1, &#39;second&#39;: 5, &#39;and&#39;: 0, &#39;third&#39;: 7, &#39;one&#39;: 4} Apply Trained Vectorizer Once the vectorizer had been trained, you can apply them on new corpus. Tokens not in the vectorizer vocubulary are ignored. new_corpus = [&quot;My Name is Charlie Angel&quot;, &quot;I love to watch Star Wars&quot;] XX = vec.transform(new_corpus) pd.DataFrame(XX.toarray(), columns=vec.get_feature_names()) ## and document first is one second the third this ## 0 0 0 0 1 0 0 0 0 0 ## 1 0 0 0 0 0 0 0 0 0 18.10.2.2 + Stop Words Vectorizer can optionally be use with stop words list. Use stop_words=english to apply filtering using sklearn built-in stop word. You can replace english with other word list object. vec4 = CountVectorizer(stop_words=&#39;english&#39;) ## sklearn stopwords list X4 = vec4.fit_transform(corpus) pd.DataFrame(X4.toarray(), columns=vec4.get_feature_names()) ## document second ## 0 1 0 ## 1 2 1 ## 2 0 0 ## 3 1 0 18.10.3 TFIDF 18.10.3.1 Equation \\[tf(t,d) = \\text{occurances of term t in document t} \\\\ n = \\text{number of documents} \\\\ df(t) = \\text{number of documents containing term t} \\\\ idf(t) = log \\frac{n}{df(t))} + 1 \\\\ idf(t) = log \\frac{1+n}{1+df(t))} + 1 \\text{.... smoothing, prevent zero division} \\\\ tfidf(t) = tf(t) * idf(t,d) \\text{.... raw, no normalization on tf(t)} \\\\ tfidf(t) = \\frac{tf(t,d)}{||V||_2} * idf(t) \\text{.... tf normalized with euclidean norm}\\] 18.10.3.2 TfidfTransformer To generate TFIDF vectors, first run CountVectorizer to get frequency vector matrix. Then take the output into this transformer. from sklearn.feature_extraction.text import TfidfTransformer corpus = [ &quot;apple apple apple apple apple banana&quot;, &quot;apple apple&quot;, &quot;apple apple apple banana&quot;, &quot;durian durian durian&quot;] count_vec = CountVectorizer() X = count_vec.fit_transform(corpus) transformer1 = TfidfTransformer(smooth_idf=False,norm=None) transformer2 = TfidfTransformer(smooth_idf=False,norm=&#39;l2&#39;) transformer3 = TfidfTransformer(smooth_idf=True,norm=&#39;l2&#39;) tfidf1 = transformer1.fit_transform(X) tfidf2 = transformer2.fit_transform(X) tfidf3 = transformer3.fit_transform(X) print( &#39;Frequency Count: \\n&#39;, pd.DataFrame(X.toarray(), columns=count_vec.get_feature_names()), &#39;\\n\\nVocabulary: &#39;, count_vec.vocabulary_, &#39;\\n\\nTFIDF Without Norm:\\n&#39;,tfidf1.toarray(), &#39;\\n\\nTFIDF with L2 Norm:\\n&#39;,tfidf2.toarray(), &#39;\\n\\nTFIDF with L2 Norm (smooth):\\n&#39;,tfidf3.toarray()) ## Frequency Count: ## apple banana durian ## 0 5 1 0 ## 1 2 0 0 ## 2 3 1 0 ## 3 0 0 3 ## ## Vocabulary: {&#39;apple&#39;: 0, &#39;banana&#39;: 1, &#39;durian&#39;: 2} ## ## TFIDF Without Norm: ## [[6.43841036 1.69314718 0. ] ## [2.57536414 0. 0. ] ## [3.86304622 1.69314718 0. ] ## [0. 0. 7.15888308]] ## ## TFIDF with L2 Norm: ## [[0.96711783 0.25432874 0. ] ## [1. 0. 0. ] ## [0.91589033 0.40142857 0. ] ## [0. 0. 1. ]] ## ## TFIDF with L2 Norm (smooth): ## [[0.97081492 0.23982991 0. ] ## [1. 0. 0. ] ## [0.92468843 0.38072472 0. ] ## [0. 0. 1. ]] 18.10.3.3 TfidfVectorizer This vectorizer gives end to end processing from corpus into TFIDF vector matrix, including tokenization, stopwords. from sklearn.feature_extraction.text import TfidfVectorizer my_tokenizer = RegexpTokenizer(r&#39;[a-zA-Z0-9\\&#39;]+&#39;) ## Custom Tokenizer vec1 = TfidfVectorizer(tokenizer=my_tokenizer.tokenize, stop_words=&#39;english&#39;) #default smooth_idf=True, norm=&#39;l2&#39; vec2 = TfidfVectorizer(tokenizer=my_tokenizer.tokenize, stop_words=&#39;english&#39;,smooth_idf=False) vec3 = TfidfVectorizer(tokenizer=my_tokenizer.tokenize, stop_words=&#39;english&#39;, norm=None) X1 = vec1.fit_transform(corpus) # FIT the vectorizer, return fitted data X2 = vec2.fit_transform(corpus) # FIT the vectorizer, return fitted data X3 = vec3.fit_transform(corpus) # FIT the vectorizer, return fitted data print( &#39;TFIDF Features (Default with Smooth and L2 Norm):\\n&#39;, pd.DataFrame(X1.toarray().round(3), columns=vec1.get_feature_names()), &#39;\\n\\nTFIDF Features (without Smoothing):\\n&#39;, pd.DataFrame(X2.toarray().round(3), columns=vec2.get_feature_names()), &#39;\\n\\nTFIDF Features (without L2 Norm):\\n&#39;, pd.DataFrame(X3.toarray().round(3), columns=vec3.get_feature_names()) ) ## TFIDF Features (Default with Smooth and L2 Norm): ## apple banana durian ## 0 0.971 0.240 0.0 ## 1 1.000 0.000 0.0 ## 2 0.925 0.381 0.0 ## 3 0.000 0.000 1.0 ## ## TFIDF Features (without Smoothing): ## apple banana durian ## 0 0.967 0.254 0.0 ## 1 1.000 0.000 0.0 ## 2 0.916 0.401 0.0 ## 3 0.000 0.000 1.0 ## ## TFIDF Features (without L2 Norm): ## apple banana durian ## 0 6.116 1.511 0.000 ## 1 2.446 0.000 0.000 ## 2 3.669 1.511 0.000 ## 3 0.000 0.000 5.749 18.11 Appliction 18.11.1 Document Similarity Document1 and Document 2 are mutiplicate of Document0, therefore their consine similarity is the same. documents = ( &quot;apple apple banana&quot;, &quot;apple apple banana apple apple banana&quot;, &quot;apple apple banana apple apple banana apple apple banana&quot;) from sklearn.feature_extraction.text import TfidfVectorizer tfidf_vec = TfidfVectorizer() tfidf_matrix = tfidf_vec.fit_transform(documents) from sklearn.metrics.pairwise import cosine_similarity print(&#39;Cosine Similarity betwen doc0 and doc1:\\n&#39;,cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])) ## Cosine Similarity betwen doc0 and doc1: ## [[1.]] print(&#39;Cosine Similarity betwen doc1 and doc2:\\n&#39;,cosine_similarity(tfidf_matrix[1], tfidf_matrix[2])) ## Cosine Similarity betwen doc1 and doc2: ## [[1.]] print(&#39;Cosine Similarity betwen doc1 and doc2:\\n&#39;,cosine_similarity(tfidf_matrix[0], tfidf_matrix[2])) ## Cosine Similarity betwen doc1 and doc2: ## [[1.]] 18.12 Naive Bayes 18.12.1 Libraries from nlpia.data.loaders import get_data from nltk.tokenize.casual import casual_tokenize from collections import Counter 18.12.2 The Data movies = get_data(&#39;hutto_movies&#39;) # download data ## INFO:nlpia.futil:Reading CSV with `read_csv(*(&#39;C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\hutto_ICWSM_2014/movieReviewSnippets_GroundTruth.csv.gz&#39;,), **{&#39;nrows&#39;: None, &#39;low_memory&#39;: False})`... print(movies.head(), &#39;\\n\\n&#39;, movies.describe()) ## sentiment text ## id ## 1 2.266667 The Rock is destined to be the 21st Century&#39;s ... ## 2 3.533333 The gorgeously elaborate continuation of &#39;&#39;The... ## 3 -0.600000 Effective but too tepid biopic ## 4 1.466667 If you sometimes like to go to the movies to h... ## 5 1.733333 Emerges as something rare, an issue movie that... ## ## sentiment ## count 10605.000000 ## mean 0.004831 ## std 1.922050 ## min -3.875000 ## 25% -1.769231 ## 50% -0.080000 ## 75% 1.833333 ## max 3.941176 18.12.3 Bag of Words Tokenize each record, remove single character token, then convert into list of counters (words-frequency pair). Each item in the list is a counter, which represent word frequency within the record bag_of_words = [] for text in movies.text: tokens = casual_tokenize(text, reduce_len=True, strip_handles=True) # tokenize tokens = [x for x in tokens if len(x)&gt;1] ## remove single char token bag_of_words.append( Counter(tokens, strip_handles=True) ## add to our BoW ) unique_words = list( set([ y for x in bag_of_words for y in x.keys()]) ) print(&quot;Total Rows: &quot;, len(bag_of_words),&#39;\\n\\n&#39;, &#39;Row 1 BoW: &#39;,bag_of_words[:1],&#39;\\n\\n&#39;, # see the first two records &#39;Row 2 BoW: &#39;, bag_of_words[:2], &#39;\\n\\n&#39;, &#39;Total Unique Words: &#39;, len(unique_words)) ## Total Rows: 10605 ## ## Row 1 BoW: [Counter({&#39;to&#39;: 2, &#39;The&#39;: 1, &#39;Rock&#39;: 1, &#39;is&#39;: 1, &#39;destined&#39;: 1, &#39;be&#39;: 1, &#39;the&#39;: 1, &#39;21st&#39;: 1, &quot;Century&#39;s&quot;: 1, &#39;new&#39;: 1, &#39;Conan&#39;: 1, &#39;and&#39;: 1, &#39;that&#39;: 1, &quot;he&#39;s&quot;: 1, &#39;going&#39;: 1, &#39;make&#39;: 1, &#39;splash&#39;: 1, &#39;even&#39;: 1, &#39;greater&#39;: 1, &#39;than&#39;: 1, &#39;Arnold&#39;: 1, &#39;Schwarzenegger&#39;: 1, &#39;Jean&#39;: 1, &#39;Claud&#39;: 1, &#39;Van&#39;: 1, &#39;Damme&#39;: 1, &#39;or&#39;: 1, &#39;Steven&#39;: 1, &#39;Segal&#39;: 1, &#39;strip_handles&#39;: 1})] ## ## Row 2 BoW: [Counter({&#39;to&#39;: 2, &#39;The&#39;: 1, &#39;Rock&#39;: 1, &#39;is&#39;: 1, &#39;destined&#39;: 1, &#39;be&#39;: 1, &#39;the&#39;: 1, &#39;21st&#39;: 1, &quot;Century&#39;s&quot;: 1, &#39;new&#39;: 1, &#39;Conan&#39;: 1, &#39;and&#39;: 1, &#39;that&#39;: 1, &quot;he&#39;s&quot;: 1, &#39;going&#39;: 1, &#39;make&#39;: 1, &#39;splash&#39;: 1, &#39;even&#39;: 1, &#39;greater&#39;: 1, &#39;than&#39;: 1, &#39;Arnold&#39;: 1, &#39;Schwarzenegger&#39;: 1, &#39;Jean&#39;: 1, &#39;Claud&#39;: 1, &#39;Van&#39;: 1, &#39;Damme&#39;: 1, &#39;or&#39;: 1, &#39;Steven&#39;: 1, &#39;Segal&#39;: 1, &#39;strip_handles&#39;: 1}), Counter({&#39;of&#39;: 4, &#39;The&#39;: 2, &#39;gorgeously&#39;: 1, &#39;elaborate&#39;: 1, &#39;continuation&#39;: 1, &#39;Lord&#39;: 1, &#39;the&#39;: 1, &#39;Rings&#39;: 1, &#39;trilogy&#39;: 1, &#39;is&#39;: 1, &#39;so&#39;: 1, &#39;huge&#39;: 1, &#39;that&#39;: 1, &#39;column&#39;: 1, &#39;words&#39;: 1, &#39;cannot&#39;: 1, &#39;adequately&#39;: 1, &#39;describe&#39;: 1, &#39;co&#39;: 1, &#39;writer&#39;: 1, &#39;director&#39;: 1, &#39;Peter&#39;: 1, &quot;Jackson&#39;s&quot;: 1, &#39;expanded&#39;: 1, &#39;vision&#39;: 1, &quot;Tolkien&#39;s&quot;: 1, &#39;Middle&#39;: 1, &#39;earth&#39;: 1, &#39;strip_handles&#39;: 1})] ## ## Total Unique Words: 20686 Convert NaN into 0 then all features into integer bows_df = pd.DataFrame.from_records(bag_of_words) bows_df = bows_df.fillna(0).astype(int) # replace NaN with 0, change to integer bows_df.head() ## The Rock is destined to ... Bearable Staggeringly ve muttering dissing ## 0 1 1 1 1 2 ... 0 0 0 0 0 ## 1 2 0 1 0 0 ... 0 0 0 0 0 ## 2 0 0 0 0 0 ... 0 0 0 0 0 ## 3 0 0 1 0 4 ... 0 0 0 0 0 ## 4 0 0 0 0 0 ... 0 0 0 0 0 ## ## [5 rows x 20686 columns] 18.12.4 Build The Model from sklearn.naive_bayes import MultinomialNB train_y = movies.sentiment&gt;0 # label train_X = bows_df # features nb_model = MultinomialNB().fit( train_X, train_y) 18.12.5 Train Set Prediction First, make a prediction on training data, then compare to ground truth. train_predicted = nb_model.predict(bows_df) print(&quot;Accuracy: &quot;, np.mean(train_predicted==train_y).round(4)) ## Accuracy: 0.9357 "],
["web-scrapping.html", "19 Web Scrapping 19.1 requests 19.2 BeautifulSoup", " 19 Web Scrapping 19.1 requests 19.1.1 Creating A Session import requests from requests.adapters import HTTPAdapter from urllib3.util.retry import Retry import random _retries = Retry(connect=10,read=10,backoff_factor=1) # backoff is incremental interval in seconds between retries _timeout = (10,10) ## connect, read timeout in seconds rqs = requests.Session() rqs.mount( &#39;http://&#39; , HTTPAdapter(max_retries= _retries)) rqs.mount( &#39;https://&#39; , HTTPAdapter(max_retries= _retries)) link1 = &#39;https://www.yahoo.com&#39; link2 = &#39;http://mamamia777.com.au&#39; #user_agent = {&#39;User-Agent&#39;: random.choice(_USER_AGENTS)} #response1 = rqs.get(link1, timeout=_timeout) #response2 = rqs.get(link2, timeout=_timeout) print (page1.status_code) ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;page1&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; 19.1.2 Rotating Broswer _USER_AGENTS = [ #Chrome &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36&#39;, &#39;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36&#39;, &#39;Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36&#39;, &#39;Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36&#39;, &#39;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36&#39;, &#39;Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36&#39;, &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36&#39;, &#39;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36&#39;, &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36&#39;, &#39;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36&#39;, #Firefox &#39;Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)&#39;, &#39;Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko&#39;, &#39;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)&#39;, &#39;Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko&#39;, &#39;Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko&#39;, &#39;Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko&#39;, &#39;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)&#39;, &#39;Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko&#39;, &#39;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)&#39;, &#39;Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko&#39;, &#39;Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)&#39;, &#39;Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)&#39;, &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)&#39;] 19.2 BeautifulSoup 19.2.1 Module Import from bs4 import BeautifulSoup 19.2.2 HTML Tag Parsing 19.2.2.1 Sample Data my_html = &#39;&#39;&#39; &lt;div id=&quot;my-id1&quot; class=&#39;title&#39;&gt; &lt;p&gt;This Is My Title&lt;/p&gt; &lt;div id=&quot;my-id2&quot; class=&#39;subtitle&#39; custom_attr=&#39;funny&#39;&gt; &lt;p&gt;This is Subtitle&lt;/p&gt; &lt;/div&gt; &lt;div id=&quot;my-id3&quot; class=&#39;title&#39;, custom_attr=&#39;funny&#39;&gt; &lt;p&gt;This is paragraph1&lt;/p&gt; &lt;p&gt;This is paragraph2&lt;/p&gt; &lt;h3&gt;This is paragraph3&lt;/h3&gt; &lt;/div&gt; &lt;/div&gt; &#39;&#39;&#39; soup = BeautifulSoup(my_html) 19.2.2.2 First Match ID Selector Everthing under the selected tag will be returned. soup.find(id=&#39;my-id1&#39;) ## &lt;div class=&quot;title&quot; id=&quot;my-id1&quot;&gt; ## &lt;p&gt;This Is My Title&lt;/p&gt; ## &lt;div class=&quot;subtitle&quot; custom_attr=&quot;funny&quot; id=&quot;my-id2&quot;&gt; ## &lt;p&gt;This is Subtitle&lt;/p&gt; ## &lt;/div&gt; ## &lt;div class=&quot;title&quot; custom_attr=&quot;funny&quot; id=&quot;my-id3&quot;&gt; ## &lt;p&gt;This is paragraph1&lt;/p&gt; ## &lt;p&gt;This is paragraph2&lt;/p&gt; ## &lt;h3&gt;This is paragraph3&lt;/h3&gt; ## &lt;/div&gt; ## &lt;/div&gt; Class Selector soup.find(class_=&#39;subtitle&#39;) ## &lt;div class=&quot;subtitle&quot; custom_attr=&quot;funny&quot; id=&quot;my-id2&quot;&gt; ## &lt;p&gt;This is Subtitle&lt;/p&gt; ## &lt;/div&gt; Attribute Selector soup.find(custom_attr=&#39;funny&#39;) ## &lt;div class=&quot;subtitle&quot; custom_attr=&quot;funny&quot; id=&quot;my-id2&quot;&gt; ## &lt;p&gt;This is Subtitle&lt;/p&gt; ## &lt;/div&gt; soup.find( custom_attr=&#39;funny&#39;) ## &lt;div class=&quot;subtitle&quot; custom_attr=&quot;funny&quot; id=&quot;my-id2&quot;&gt; ## &lt;p&gt;This is Subtitle&lt;/p&gt; ## &lt;/div&gt; soup.find(&#39;div&#39;, custom_attr=&#39;funny&#39;) ## &lt;div class=&quot;subtitle&quot; custom_attr=&quot;funny&quot; id=&quot;my-id2&quot;&gt; ## &lt;p&gt;This is Subtitle&lt;/p&gt; ## &lt;/div&gt; 19.2.2.3 Find All Matches find_all soup = BeautifulSoup(my_html) multiple_result = soup.find_all(class_=&#39;title&#39;) print( &#39;Item 0: \\n&#39;, multiple_result[0], &#39;\\n\\nItem 1: \\n&#39;, multiple_result[1]) ## Item 0: ## &lt;div class=&quot;title&quot; id=&quot;my-id1&quot;&gt; ## &lt;p&gt;This Is My Title&lt;/p&gt; ## &lt;div class=&quot;subtitle&quot; custom_attr=&quot;funny&quot; id=&quot;my-id2&quot;&gt; ## &lt;p&gt;This is Subtitle&lt;/p&gt; ## &lt;/div&gt; ## &lt;div class=&quot;title&quot; custom_attr=&quot;funny&quot; id=&quot;my-id3&quot;&gt; ## &lt;p&gt;This is paragraph1&lt;/p&gt; ## &lt;p&gt;This is paragraph2&lt;/p&gt; ## &lt;h3&gt;This is paragraph3&lt;/h3&gt; ## &lt;/div&gt; ## &lt;/div&gt; ## ## Item 1: ## &lt;div class=&quot;title&quot; custom_attr=&quot;funny&quot; id=&quot;my-id3&quot;&gt; ## &lt;p&gt;This is paragraph1&lt;/p&gt; ## &lt;p&gt;This is paragraph2&lt;/p&gt; ## &lt;h3&gt;This is paragraph3&lt;/h3&gt; ## &lt;/div&gt; CSS Selector using select() Above can be achieved using css selector. It return an array of result (multiple matches). multiple_result = soup.select(&#39;.title&#39;) print( &#39;Item 0: \\n&#39;, multiple_result[0], &#39;\\n\\nItem 1: \\n&#39;, multiple_result[1]) ## Item 0: ## &lt;div class=&quot;title&quot; id=&quot;my-id1&quot;&gt; ## &lt;p&gt;This Is My Title&lt;/p&gt; ## &lt;div class=&quot;subtitle&quot; custom_attr=&quot;funny&quot; id=&quot;my-id2&quot;&gt; ## &lt;p&gt;This is Subtitle&lt;/p&gt; ## &lt;/div&gt; ## &lt;div class=&quot;title&quot; custom_attr=&quot;funny&quot; id=&quot;my-id3&quot;&gt; ## &lt;p&gt;This is paragraph1&lt;/p&gt; ## &lt;p&gt;This is paragraph2&lt;/p&gt; ## &lt;h3&gt;This is paragraph3&lt;/h3&gt; ## &lt;/div&gt; ## &lt;/div&gt; ## ## Item 1: ## &lt;div class=&quot;title&quot; custom_attr=&quot;funny&quot; id=&quot;my-id3&quot;&gt; ## &lt;p&gt;This is paragraph1&lt;/p&gt; ## &lt;p&gt;This is paragraph2&lt;/p&gt; ## &lt;h3&gt;This is paragraph3&lt;/h3&gt; ## &lt;/div&gt; More granular exmaple of css selector. soup.select(&#39;#my-id1 div.subtitle&#39;) ## [&lt;div class=&quot;subtitle&quot; custom_attr=&quot;funny&quot; id=&quot;my-id2&quot;&gt; ## &lt;p&gt;This is Subtitle&lt;/p&gt; ## &lt;/div&gt;] Using contains() soup.select(&quot;p:contains(&#39;This is paragraph&#39;)&quot;) ## [&lt;p&gt;This is paragraph1&lt;/p&gt;, &lt;p&gt;This is paragraph2&lt;/p&gt;] Combining ID, Class and Custom Attribute in the selector soup.select(&quot;div#my-id3.title[custom_attr=&#39;funny&#39;]:contains(&#39;This is paragraph&#39;)&quot;) ## [&lt;div class=&quot;title&quot; custom_attr=&quot;funny&quot; id=&quot;my-id3&quot;&gt; ## &lt;p&gt;This is paragraph1&lt;/p&gt; ## &lt;p&gt;This is paragraph2&lt;/p&gt; ## &lt;h3&gt;This is paragraph3&lt;/h3&gt; ## &lt;/div&gt;] 19.2.3 Meta Parsing my_meta = &#39;&#39;&#39; &lt;meta property=&quot;description&quot; content=&quot;KUALA LUMPUR: blah blah&quot; category=&quot;Malaysia&quot;&gt; &lt;meta property=&quot;publish-date&quot; content=&quot;2012-01-03&quot;&gt; &#39;&#39;&#39; soup = BeautifulSoup(my_meta) soup.find(&#39;meta&#39;, property=&#39;description&#39;)[&#39;content&#39;] ## &#39;KUALA LUMPUR: blah blah&#39; soup.find(&#39;meta&#39;, property=&#39;description&#39;)[&#39;category&#39;] ## &#39;Malaysia&#39; soup.find(&#39;meta&#39;, property=&#39;publish-date&#39;)[&#39;content&#39;] ## &#39;2012-01-03&#39; soup.find(&#39;meta&#39;, category=&#39;Malaysia&#39;)[&#39;property&#39;] ## &#39;description&#39; 19.2.4 Getting Content 19.2.4.1 Get Content get_text(strip=, separator=) Use strip=True to strip whitespace from the beginning and end of each bit of text Use `separator=‘’ to specify a string to be used to join the bits of text together It is recommended to use strip=True, separator='\\n' so that result from different operating system will be consistant soup = BeautifulSoup(my_html) elem = soup.find(id = &quot;my-id3&quot;) elem.get_text(strip=False) ## &#39;\\nThis is paragraph1\\nThis is paragraph2\\nThis is paragraph3\\n&#39; strip=True combine with separator will retain only the user readable text portion of each tag, with separator seperating them elem.get_text(strip=True, separator=&#39;\\n&#39;) ## &#39;This is paragraph1\\nThis is paragraph2\\nThis is paragraph3&#39; 19.2.4.2 Splitting Content It is useful to split using separator into list of string. elem = soup.find(id = &quot;my-id3&quot;) elem.get_text(strip=True, separator=&#39;\\n&#39;).split(&#39;\\n&#39;) ## [&#39;This is paragraph1&#39;, &#39;This is paragraph2&#39;, &#39;This is paragraph3&#39;] 19.2.5 Traversing 19.2.5.1 Get The Element elems = soup.select(&quot;div#my-id3.title[custom_attr=&#39;funny&#39;]:contains(&#39;This is paragraph&#39;)&quot;) elem = elems[0] elem ## &lt;div class=&quot;title&quot; custom_attr=&quot;funny&quot; id=&quot;my-id3&quot;&gt; ## &lt;p&gt;This is paragraph1&lt;/p&gt; ## &lt;p&gt;This is paragraph2&lt;/p&gt; ## &lt;h3&gt;This is paragraph3&lt;/h3&gt; ## &lt;/div&gt; 19.2.5.2 Traversing Children All Children In List findChildren() elem.findChildren() ## [&lt;p&gt;This is paragraph1&lt;/p&gt;, &lt;p&gt;This is paragraph2&lt;/p&gt;, &lt;h3&gt;This is paragraph3&lt;/h3&gt;] Next Children findNext() If the element has children, this will get the immediate child If the element has no children, this will find the next element in the hierechy first_child = elem.fin print( elem.findNext().get_text(strip=True), &#39;\\n&#39;, elem.findNext().findNext().get_text(strip=True), &#39;\\n&#39;) ## This is paragraph1 ## This is paragraph2 19.2.5.3 Traversing To Parent parent() elem_parent = elem.parent elem_parent.attrs ## {&#39;id&#39;: &#39;my-id1&#39;, &#39;class&#39;: [&#39;title&#39;]} 19.2.5.4 Get The Sibling findPreviousSibling() Sibling is element at the same level of hierachy elem_prev_sib = elem.findPreviousSibling() elem_prev_sib.attrs ## {&#39;id&#39;: &#39;my-id2&#39;, &#39;class&#39;: [&#39;subtitle&#39;], &#39;custom_attr&#39;: &#39;funny&#39;} "],
["finance.html", "20 Finance 20.1 Getting Data 20.2 Charting", " 20 Finance 20.1 Getting Data 20.1.1 pandas_datareder 20.1.1.1 OHLC EOD Pricing HLOC columns are adjusted with splits ‘Adj Close’ columns is adjusted with split and dividends ‘start’ and ‘end’ date must be string import pandas_datareader as pdr pdr.data.DataReader(&#39;PUBM.KL&#39;, start=&#39;2015-1-1&#39;, end=&#39;2019-12-31&#39;, data_source=&#39;yahoo&#39;) ## High Low Open Close Volume Adj Close ## Date ## 2015-01-02 18.280001 18.020000 18.260000 18.219999 1689000.0 15.345568 ## 2015-01-05 18.240000 17.760000 18.240000 17.820000 2667800.0 15.008673 ## 2015-01-06 17.799999 17.500000 17.780001 17.600000 5042600.0 14.823381 ## 2015-01-07 17.700001 17.580000 17.600000 17.580000 4913200.0 14.806539 ## 2015-01-08 17.680000 17.559999 17.580000 17.600000 4121100.0 14.823381 ## ... ... ... ... ... ... ... ## 2019-12-24 20.020000 19.719999 20.000000 19.820000 1405800.0 19.361732 ## 2019-12-26 19.860001 19.639999 19.820000 19.680000 600300.0 19.224972 ## 2019-12-27 19.940001 19.660000 19.680000 19.879999 1325700.0 19.420345 ## 2019-12-30 20.000000 19.780001 19.879999 19.980000 2180200.0 19.518034 ## 2019-12-31 19.900000 19.400000 19.799999 19.440001 3430600.0 18.990520 ## ## [1239 rows x 6 columns] 20.1.1.2 Splits and Dividends This method is similar to getting pricing data, except that different data_sources is used. pdr.DataReader(&#39;AAPL&#39;, data_source = &#39;yahoo-actions&#39;, start=&#39;2014-01-06&#39;, end=&#39;2015-06-15&#39; ) ## Error in py_call_impl(callable, dots$args, dots$keywords): SyntaxError: invalid syntax (&lt;string&gt;, line 1) ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 4, in &lt;module&gt; ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py&quot;, line 208, in wrapper ## return func(*args, **kwargs) ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_datareader\\data.py&quot;, line 545, in DataReader ## session=session, ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_datareader\\yahoo\\actions.py&quot;, line 15, in read ## data = super(YahooActionReader, self).read() ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_datareader\\base.py&quot;, line 251, in read ## df = self._read_one_data(self.url, params=self._get_params(self.symbols)) ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_datareader\\yahoo\\daily.py&quot;, line 206, in _read_one_data ## splits[&quot;SplitRatio&quot;] = splits.apply(split_ratio, axis=1) ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py&quot;, line 6928, in apply ## return op.get_result() ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py&quot;, line 186, in get_result ## return self.apply_standard() ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py&quot;, line 292, in apply_standard ## self.apply_series_generator() ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py&quot;, line 321, in apply_series_generator ## results[i] = self.f(v) ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_datareader\\yahoo\\daily.py&quot;, line 201, in split_ratio ## return eval(row[&quot;Splitratio&quot;]) 20.1.1.3 Merging OHLC and Splits/Dividends prices = pdr.DataReader(&#39;AAPL&#39;, data_source = &#39;yahoo&#39;, start=&#39;2014-06-06&#39;, end=&#39;2014-06-12&#39; ) actions = pdr.DataReader(&#39;AAPL&#39;, data_source = &#39;yahoo-actions&#39;, start=&#39;2014-06-06&#39;, end=&#39;2014-06-12&#39; ) ## Error in py_call_impl(callable, dots$args, dots$keywords): SyntaxError: invalid syntax (&lt;string&gt;, line 1) ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 4, in &lt;module&gt; ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py&quot;, line 208, in wrapper ## return func(*args, **kwargs) ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_datareader\\data.py&quot;, line 545, in DataReader ## session=session, ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_datareader\\yahoo\\actions.py&quot;, line 15, in read ## data = super(YahooActionReader, self).read() ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_datareader\\base.py&quot;, line 251, in read ## df = self._read_one_data(self.url, params=self._get_params(self.symbols)) ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_datareader\\yahoo\\daily.py&quot;, line 206, in _read_one_data ## splits[&quot;SplitRatio&quot;] = splits.apply(split_ratio, axis=1) ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py&quot;, line 6928, in apply ## return op.get_result() ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py&quot;, line 186, in get_result ## return self.apply_standard() ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py&quot;, line 292, in apply_standard ## self.apply_series_generator() ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py&quot;, line 321, in apply_series_generator ## results[i] = self.f(v) ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_datareader\\yahoo\\daily.py&quot;, line 201, in split_ratio ## return eval(row[&quot;Splitratio&quot;]) Use pandas.merge() function to combine both prices and splits dataframe in a new dataframe. Non matching line will have NaN. pd.merge(prices, actions, how=&#39;outer&#39;, left_index=True, right_index=True) \\ .loc[:,[&#39;High&#39;,&#39;Low&#39;,&#39;Open&#39;,&#39;Close&#39;,&#39;action&#39;,&#39;value&#39;]] ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;actions&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Alternatively, use pandas column assignment from the splits dataframe into price dataframe, it will automatically ‘merge’ based on the index. This approach reuse existing dataframe instead of creating new one. prices[&#39;action&#39;], prices[&#39;value&#39;] = actions.action, actions.value ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;actions&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; prices[[&#39;High&#39;,&#39;Low&#39;,&#39;Open&#39;,&#39;Close&#39;,&#39;action&#39;,&#39;value&#39;]] ## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: &quot;[&#39;action&#39;, &#39;value&#39;] not in index&quot; ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py&quot;, line 3001, in __getitem__ ## indexer = self.loc._convert_to_indexer(key, axis=1, raise_missing=True) ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py&quot;, line 1285, in _convert_to_indexer ## return self._get_listlike_indexer(obj, axis, **kwargs)[1] ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py&quot;, line 1092, in _get_listlike_indexer ## keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py&quot;, line 1185, in _validate_read_indexer ## raise KeyError(&quot;{} not in index&quot;.format(not_found)) 20.1.1.4 Query Multiple Stocks ** When multiple symbols are supplied to DataReader, dictionary containing multiple stock’s result are returned. stocks = [&#39;MLYBY&#39;, &#39;AAPL&#39;] my_dict = pdr.DataReader( stocks, data_source = &#39;yahoo-actions&#39;, start=&#39;2014-01-06&#39;, end=&#39;2015-06-15&#39; ) ## Error in py_call_impl(callable, dots$args, dots$keywords): SyntaxError: invalid syntax (&lt;string&gt;, line 1) ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 4, in &lt;module&gt; ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py&quot;, line 208, in wrapper ## return func(*args, **kwargs) ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_datareader\\data.py&quot;, line 545, in DataReader ## session=session, ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_datareader\\yahoo\\actions.py&quot;, line 15, in read ## data = super(YahooActionReader, self).read() ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_datareader\\base.py&quot;, line 256, in read ## df = self._dl_mult_symbols(self.symbols) ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_datareader\\base.py&quot;, line 266, in _dl_mult_symbols ## stocks[sym] = self._read_one_data(self.url, self._get_params(sym)) ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_datareader\\yahoo\\daily.py&quot;, line 206, in _read_one_data ## splits[&quot;SplitRatio&quot;] = splits.apply(split_ratio, axis=1) ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py&quot;, line 6928, in apply ## return op.get_result() ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py&quot;, line 186, in get_result ## return self.apply_standard() ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py&quot;, line 292, in apply_standard ## self.apply_series_generator() ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py&quot;, line 321, in apply_series_generator ## results[i] = self.f(v) ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_datareader\\yahoo\\daily.py&quot;, line 201, in split_ratio ## return eval(row[&quot;Splitratio&quot;]) print(my_dict.keys()) ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;my_dict&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Iterate through the dictionary to get the dataframe data for i in my_dict.items(): print(&#39;\\n\\nStock: &#39;, i[0], &#39;\\nDataFrame:&#39;, i[1]) ## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;my_dict&#39; is not defined ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; 20.1.2 yfinance yFinance Support Yahoo only, a better alternative This library has advantage of calculating adjsuted OHLC by split and dividends. Dividends and Splits are conveniently incorporated into pricing dataframe, so no manual merging necessary. Multiple symbols are represented in columns This library provides stock information (not all exchanges are supported though) 20.1.2.1 Stock Info There are plenty of infomration we can get form the dictionary of returend by info import yfinance as yf stock = yf.Ticker(&#39;AAPL&#39;) stock.info.keys() ## dict_keys([&#39;zip&#39;, &#39;sector&#39;, &#39;fullTimeEmployees&#39;, &#39;longBusinessSummary&#39;, &#39;city&#39;, &#39;phone&#39;, &#39;state&#39;, &#39;country&#39;, &#39;companyOfficers&#39;, &#39;website&#39;, &#39;maxAge&#39;, &#39;address1&#39;, &#39;industry&#39;, &#39;previousClose&#39;, &#39;regularMarketOpen&#39;, &#39;twoHundredDayAverage&#39;, &#39;trailingAnnualDividendYield&#39;, &#39;payoutRatio&#39;, &#39;volume24Hr&#39;, &#39;regularMarketDayHigh&#39;, &#39;navPrice&#39;, &#39;averageDailyVolume10Day&#39;, &#39;totalAssets&#39;, &#39;regularMarketPreviousClose&#39;, &#39;fiftyDayAverage&#39;, &#39;trailingAnnualDividendRate&#39;, &#39;open&#39;, &#39;averageVolume10days&#39;, &#39;expireDate&#39;, &#39;yield&#39;, &#39;algorithm&#39;, &#39;dividendRate&#39;, &#39;exDividendDate&#39;, &#39;beta&#39;, &#39;circulatingSupply&#39;, &#39;startDate&#39;, &#39;regularMarketDayLow&#39;, &#39;priceHint&#39;, &#39;currency&#39;, &#39;trailingPE&#39;, &#39;regularMarketVolume&#39;, &#39;lastMarket&#39;, &#39;maxSupply&#39;, &#39;openInterest&#39;, &#39;marketCap&#39;, &#39;volumeAllCurrencies&#39;, &#39;strikePrice&#39;, &#39;averageVolume&#39;, &#39;priceToSalesTrailing12Months&#39;, &#39;dayLow&#39;, &#39;ask&#39;, &#39;ytdReturn&#39;, &#39;askSize&#39;, &#39;volume&#39;, &#39;fiftyTwoWeekHigh&#39;, &#39;forwardPE&#39;, &#39;fromCurrency&#39;, &#39;fiveYearAvgDividendYield&#39;, &#39;fiftyTwoWeekLow&#39;, &#39;bid&#39;, &#39;tradeable&#39;, &#39;dividendYield&#39;, &#39;bidSize&#39;, &#39;dayHigh&#39;, &#39;exchange&#39;, &#39;shortName&#39;, &#39;longName&#39;, &#39;exchangeTimezoneName&#39;, &#39;exchangeTimezoneShortName&#39;, &#39;isEsgPopulated&#39;, &#39;gmtOffSetMilliseconds&#39;, &#39;quoteType&#39;, &#39;symbol&#39;, &#39;messageBoardId&#39;, &#39;market&#39;, &#39;annualHoldingsTurnover&#39;, &#39;enterpriseToRevenue&#39;, &#39;beta3Year&#39;, &#39;profitMargins&#39;, &#39;enterpriseToEbitda&#39;, &#39;52WeekChange&#39;, &#39;morningStarRiskRating&#39;, &#39;forwardEps&#39;, &#39;revenueQuarterlyGrowth&#39;, &#39;sharesOutstanding&#39;, &#39;fundInceptionDate&#39;, &#39;annualReportExpenseRatio&#39;, &#39;bookValue&#39;, &#39;sharesShort&#39;, &#39;sharesPercentSharesOut&#39;, &#39;fundFamily&#39;, &#39;lastFiscalYearEnd&#39;, &#39;heldPercentInstitutions&#39;, &#39;netIncomeToCommon&#39;, &#39;trailingEps&#39;, &#39;lastDividendValue&#39;, &#39;SandP52WeekChange&#39;, &#39;priceToBook&#39;, &#39;heldPercentInsiders&#39;, &#39;nextFiscalYearEnd&#39;, &#39;mostRecentQuarter&#39;, &#39;shortRatio&#39;, &#39;sharesShortPreviousMonthDate&#39;, &#39;floatShares&#39;, &#39;enterpriseValue&#39;, &#39;threeYearAverageReturn&#39;, &#39;lastSplitDate&#39;, &#39;lastSplitFactor&#39;, &#39;legalType&#39;, &#39;morningStarOverallRating&#39;, &#39;earningsQuarterlyGrowth&#39;, &#39;dateShortInterest&#39;, &#39;pegRatio&#39;, &#39;lastCapGain&#39;, &#39;shortPercentOfFloat&#39;, &#39;sharesShortPriorMonth&#39;, &#39;category&#39;, &#39;fiveYearAverageReturn&#39;, &#39;regularMarketPrice&#39;, &#39;logo_url&#39;]) print(stock.info[&#39;longName&#39;]) ## Apple Inc. 20.1.2.2 OHLC EOD Pricing Split Adjusted OHLC columns are adjusted with splits (when auto_adjust=False) ‘Adj Close’ columns is adjusted with split and dividends ‘start’ and ‘end’ date must be string stock = yf.Ticker(&#39;AAPL&#39;) stock.history( start=&#39;2014-06-06&#39;, end=&#39;2015-06-15&#39;, auto_adjust = False) ## Open High Low Close Adj Close Volume Dividends Stock Splits ## Date ## 2014-06-05 92.31 92.77 91.80 92.48 83.99 75951400 0.0 0.0 ## 2014-06-06 92.84 93.04 92.07 92.22 83.76 87484600 0.0 0.0 ## 2014-06-09 92.70 93.88 91.75 93.70 85.10 75415000 0.0 7.0 ## 2014-06-10 94.73 95.05 93.57 94.25 85.59 62777000 0.0 0.0 ## 2014-06-11 94.13 94.76 93.47 93.86 85.24 45681000 0.0 0.0 ## ... ... ... ... ... ... ... ... ... ## 2015-06-08 128.90 129.21 126.83 127.80 118.10 52674800 0.0 0.0 ## 2015-06-09 126.70 128.08 125.62 127.42 117.75 56075400 0.0 0.0 ## 2015-06-10 127.92 129.34 127.85 128.88 119.10 39087300 0.0 0.0 ## 2015-06-11 129.18 130.18 128.48 128.59 118.83 35390900 0.0 0.0 ## 2015-06-12 128.19 128.33 127.11 127.17 117.52 36886200 0.0 0.0 ## ## [258 rows x 8 columns] Split and Dividends Adjusted OHLC columns are adjusted with splits and dividends (when auto_adjust=True) Therefore, ‘Adj Close’ column is redundant, hence removed. import yfinance as yf stock = yf.Ticker(&#39;AAPL&#39;) stock.history( start=&#39;2014-06-06&#39;, end=&#39;2015-06-15&#39;, auto_adjust = True) ## Open High Low Close Volume Dividends Stock Splits ## Date ## 2014-06-05 83.84 84.25 83.37 83.99 75951400 0.0 0.0 ## 2014-06-06 84.32 84.49 83.61 83.76 87484600 0.0 0.0 ## 2014-06-09 84.19 85.26 83.32 85.10 75415000 0.0 7.0 ## 2014-06-10 86.03 86.32 84.98 85.59 62777000 0.0 0.0 ## 2014-06-11 85.49 86.06 84.89 85.24 45681000 0.0 0.0 ## ... ... ... ... ... ... ... ... ## 2015-06-08 119.12 119.40 117.20 118.10 52674800 0.0 0.0 ## 2015-06-09 117.08 118.36 116.09 117.75 56075400 0.0 0.0 ## 2015-06-10 118.21 119.52 118.15 119.10 39087300 0.0 0.0 ## 2015-06-11 119.38 120.30 118.73 118.83 35390900 0.0 0.0 ## 2015-06-12 118.46 118.59 117.46 117.52 36886200 0.0 0.0 ## ## [258 rows x 7 columns] 20.1.2.3 Splits and Dividends Getting both Splits and Dividends stock.actions ## Dividends Stock Splits ## Date ## 2014-06-09 0.00 7.0 ## 2014-08-07 0.47 0.0 ## 2014-11-06 0.47 0.0 ## 2015-02-05 0.47 0.0 ## 2015-05-07 0.52 0.0 Getting Dividends Only stock.dividends ## Date ## 2014-08-07 0.47 ## 2014-11-06 0.47 ## 2015-02-05 0.47 ## 2015-05-07 0.52 ## Name: Dividends, dtype: float64 Getting Splits Only stock.splits ## Date ## 2014-06-09 7.0 ## Name: Stock Splits, dtype: float64 20.1.2.4 Query Using Periods Available periods are: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max stock = yf.Ticker(&#39;AAPL&#39;) stock.history(periods=&#39;max&#39;) ## Open High Low Close Volume Dividends Stock Splits ## Date ## 2020-02-14 324.74 325.98 322.85 324.95 20028400 0 0 ## 2020-02-18 315.36 319.75 314.61 319.00 38132800 0 0 ## 2020-02-19 320.00 324.57 320.00 323.62 23496000 0 0 ## 2020-02-20 322.63 324.65 318.21 320.30 25141500 0 0 ## 2020-02-21 318.62 320.45 310.50 313.05 32388500 0 0 ## ... ... ... ... ... ... ... ... ## 2020-03-09 263.75 278.09 263.00 266.17 71686200 0 0 ## 2020-03-10 277.14 286.44 269.37 285.34 71322500 0 0 ## 2020-03-11 277.39 281.22 271.86 275.43 63899700 0 0 ## 2020-03-12 255.94 270.00 248.00 248.23 104618500 0 0 ## 2020-03-13 264.89 279.92 252.95 277.97 92481100 0 0 ## ## [20 rows x 7 columns] 20.1.2.5 Query Multiple Stocks **Use download() function to query multiple stocks. By default, it is grouped by column. Access data in result[‘Column’][‘Symbol’] To group by Symbol, use group_by parameter. With this, access data in result[‘Symbol’][‘Column’] By default, threads=True for parallel downloading. stocks = [&#39;MLYBY&#39;,&#39;AAPL&#39;] df1 = yf.download(stocks, start=&#39;2014-06-06&#39;, end=&#39;2014-06-15&#39;) ## [ 0% ] [*********************100%***********************] 2 of 2 completed df2 = yf.download(stocks, start=&#39;2014-06-06&#39;, end=&#39;2014-06-15&#39;, group_by=&#39;ticker&#39;) ## [ 0% ] [*********************100%***********************] 2 of 2 completed print(&#39;Group by Column Name:\\n&#39;, df1[&#39;Close&#39;][&#39;AAPL&#39;], &#39;\\n\\n&#39;, &#39;Group by Symbol: ]n&#39;, df2[&#39;AAPL&#39;][&#39;Close&#39;]) ## Group by Column Name: ## Date ## 2014-06-05 92.478569 ## 2014-06-06 92.224289 ## 2014-06-09 93.699997 ## 2014-06-10 94.250000 ## 2014-06-11 93.860001 ## 2014-06-12 92.290001 ## 2014-06-13 91.279999 ## Name: AAPL, dtype: float64 ## ## Group by Symbol: ]n Date ## 2014-06-05 92.478569 ## 2014-06-06 92.224289 ## 2014-06-09 93.699997 ## 2014-06-10 94.250000 ## 2014-06-11 93.860001 ## 2014-06-12 92.290001 ## 2014-06-13 91.279999 ## Name: Close, dtype: float64 20.1.3 world trading 20.1.3.1 OHLC EOD Pricing 20.2 Charting import cufflinks as cf # Cufflinks #cf.set_config_file(offline=True) # set the plotting mode to offline ## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;cufflinks&#39; ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; 20.2.1 Price Comparison stocks = [&#39;AAPL&#39;,&#39;MLYBY&#39;, &#39;PUBM.KL&#39;, &#39;HLFBF&#39;,&#39;1295.KL&#39;] df = yf.download(stocks, start=&#39;2019-01-01&#39;, end=&#39;2019-12-31&#39;) ## [ 0% ] [******************* 40% ] 2 of 5 completed [**********************60%**** ] 3 of 5 completed [**********************80%************* ] 4 of 5 completed [*********************100%***********************] 5 of 5 completed ## ## 1 Failed download: ## - 1295.KL: No data found for this date range, symbol may be delisted df[&#39;Close&#39;] ## 1295.KL AAPL HLFBF MLYBY PUBM.KL ## Date ## 2018-12-31 NaN 157.740005 4.50 4.66 NaN ## 2019-01-02 NaN 157.919998 4.50 4.13 24.660000 ## 2019-01-03 NaN 142.190002 4.50 4.13 24.600000 ## 2019-01-04 NaN 148.259995 4.50 4.49 24.660000 ## 2019-01-07 NaN 147.929993 4.50 4.68 24.700001 ## ... ... ... ... ... ... ## 2019-12-23 NaN 284.000000 3.95 4.26 20.040001 ## 2019-12-24 NaN 284.269989 3.95 4.26 19.820000 ## 2019-12-26 NaN 289.910004 3.95 4.26 19.680000 ## 2019-12-27 NaN 289.799988 3.95 4.30 19.879999 ## 2019-12-30 NaN 291.519989 3.95 4.15 19.980000 ## ## [257 rows x 5 columns] df.iplot() ## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: &#39;DataFrame&#39; object has no attribute &#39;iplot&#39; ## ## Detailed traceback: ## File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ## File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py&quot;, line 5179, in __getattr__ ## return object.__getattribute__(self, name) stock = yf.Ticker(&#39;PUBM.KL&#39;) #stock.history(periods=&#39;max&#39;) stock.history( start=&#39;2014-06-06&#39;, end=&#39;2015-06-15&#39;, auto_adjust = True) ## Open High Low Close Volume Dividends Stock Splits ## Date ## 2014-06-06 16.10 16.10 16.01 16.01 4871739 0.0 0 ## 2014-06-09 16.01 16.06 15.96 16.03 2288884 0.0 0 ## 2014-06-10 16.03 16.10 15.96 16.04 5620115 0.0 0 ## 2014-06-11 16.07 16.15 16.00 16.01 3582958 0.0 0 ## 2014-06-12 16.04 16.15 15.93 16.06 5126384 0.0 0 ## ... ... ... ... ... ... ... ... ## 2015-06-08 15.86 16.00 15.79 16.00 2270600 0.0 0 ## 2015-06-09 16.00 16.07 15.95 16.05 2411700 0.0 0 ## 2015-06-10 16.07 16.07 15.98 16.03 2299400 0.0 0 ## 2015-06-11 16.14 16.22 16.03 16.07 5276900 0.0 0 ## 2015-06-12 15.95 16.10 15.95 16.07 2767800 0.0 0 ## ## [251 rows x 7 columns] stock = yf.Ticker(&#39;1295.KL&#39;) stock.history( start=&#39;2014-06-06&#39;, end=&#39;2015-06-15&#39;, auto_adjust = True) ## Open High Low Close Volume Dividends Stock Splits ## Date ## 2014-06-19 18.25 18.40 18.16 18.40 1016300 0.0 0 ## 2014-06-20 18.40 18.40 18.20 18.29 7560000 0.0 0 ## 2014-06-23 18.29 18.31 18.16 18.24 5607900 0.0 0 ## 2014-06-24 18.29 18.29 18.11 18.20 6092700 0.0 0 ## 2014-06-25 18.13 18.18 18.07 18.09 7546900 0.0 0 ## ... ... ... ... ... ... ... ... ## 2015-06-05 17.26 17.33 17.18 17.28 1867200 0.0 0 ## 2015-06-08 17.22 17.37 17.15 17.37 2270600 0.0 0 ## 2015-06-09 17.37 17.44 17.31 17.42 2411700 0.0 0 ## 2015-06-10 17.44 17.44 17.35 17.41 2299400 0.0 0 ## 2015-06-11 17.52 17.61 17.41 17.44 5276900 0.0 0 ## ## [190 rows x 7 columns] stocks = [&#39;1295.KL&#39;] df = yf.download(&#39;1295.KL&#39;, start=&#39;2018-12-03&#39;, end=&#39;2019-03-21&#39;) ## [*********************100%***********************] 1 of 1 completed ## ## 1 Failed download: ## - 1295.KL: No data found for this date range, symbol may be delisted df ## Empty DataFrame ## Columns: [Open, High, Low, Close, Adj Close, Volume] ## Index: [] "]
]
