# NLP
Natural Language Processing

```{r include=FALSE}
library(reticulate)
use_condaenv('Anaconda3')   #conda_list() - to find out the name of conda environment
```

```{python include=FALSE, results='hide'}
import numpy as np
import pandas as pd
from IPython.core.display import display, HTML
display(HTML("<style>.container { width:75% !important; margin-left:350px; }</style>"))
pd.set_option( 'display.notebook_repr_html', False)  # render Series and DataFrame as text, not HTML
pd.set_option( 'display.max_column', 10)    # number of columns
pd.set_option( 'display.max_rows', 10)      # number of rows
pd.set_option( 'display.width', 90)         # number of characters per row
```

## Regular Expression
- Rgular expressions (called REs or regexes) is mandatory skill for NLP. The `re` is a **built-in* library  
- It is essentially a tiny, highly specialized programming language embedded inside Python and made available through the re module  
- Regular expression patterns are compiled into a series of bytecodes which are then executed by a matching engine written in C  

### Syntax
There are two methods to emply re. Below method compile a regex first, then apply it multiple times in subsequent code.

```{python}
import re
pattern = re.compile(r'put pattern here')
pattern.match('put text here')
```

Second method below employ compile and match in single line. The pattern cannot be reused, therefore good for onetime usage only.

```{python}
import re
pattern = (r'put pattern here')
re.match(pattern, r'put text here')  # compile and match in single line
```

### Finding

#### Find The First Match
There are two ways to find the first match:  
- **`re.search`** find first match anywhere in text, including multiline  
- **`re.match`** find first match at the BEGINNING of text, similar to `re.search`with `^`  
- Both returns first match, return  **MatchObject**  
- Both returns **None** if no match is found  

```{python}
pattern1 = re.compile('123') 
pattern2 = re.compile('123')
pattern3 = re.compile('^123')  # equivalent to above
text = 'abc123xyz'

## Single Line Text Example
print( 're.search found a match somewhere:\n',
       pattern1.search(text), '\n', ## found
       '\nre.match did not find anything at the beginning:\n',
       pattern2.match(text), '\n',
       '\nre.search did not find anything at beginning too:\n',
       pattern3.search(text))        ## None
```

Returned **MatchObject** provides useful information about the matched string.

```{python}
age_pattern = re.compile(r'\d+')
age_text    = 'Ali is my teacher. He is 109 years old. his kid is 40 years old.'
first_found = age_pattern.search(age_text)

print('Found Object:           ', first_found,
      '\nInput Text:             ', first_found.string,
      '\nInput Pattern:          ', first_found.re,
      '\nFirst Found string:     ', first_found.group(),
      '\nFound Start Position:   ', first_found.start(),
      '\nFound End Position:     ', first_found.end(),
      '\nFound Span:             ', first_found.span(),)
```

#### Find All Matches

**`findall()`** returns all matching string as **list**. If no matches found, it return an empty list.

```{python}
print(
  'Finding Two Digits:',
  re.findall(r'\d\d','abc123xyz456'), '\n',
  '\nFound Nothing:',
  re.findall(r'\d\d','abcxyz'))
```

### Matching Condition

#### Meta Characters

```
[]     match any single character within the bracket
[1234] is the same as [1-4]
[0-39] is the same as [01239]
[a-e]  is the same as [abcde]
[^abc] means any character except a,b,c
[^0-9] means any character except 0-9
a|b:   a or b
{n,m}  at least n repetition, but maximum m repetition
()     grouping
```


```{python}
pattern = re.compile(r'[a-z]+')
text1 = "tempo"
text2 = "tempo1"
text3 = "123 tempo1"
text4 = " tempo"
print(
  'Matching Text1:', pattern.match(text1),
  '\nMatching Text2:', pattern.match(text2),
  '\nMatching Text3:', pattern.match(text3),
  '\nMatching Text4:', pattern.match(text4))
```


#### Special Sequence

```
. : [^\n]
\d: [0-9]              \D: [^0-9]
\s: [ \t\n\r\f\v]      \S: [^ \t\n\r\f\v]
\w: [a-zA-Z0-9_]       \W: [^a-zA-Z0-9_]
\t: tab
\n: newline
\b: word boundry (delimited by space, \t, \n)
```

**Word Boundry Using `\b`**:  

- `\bABC` match if specified characters at the beginning of word (delimited by space, \t, \n), or beginning of newline  
- `ABC\b` match if specified characters at the end of word (delimited by space, \t, \n), or end of the line  

```{python}
text = "ABCD ABC XYZABC"
pattern1 = re.compile(r'\bABC')
pattern2 = re.compile(r'ABC\b')
pattern3 = re.compile(r'\bABC\b')

print('Match word that begins ABC:',
  pattern1.findall(text), '\n',
  'Match word that ends with ABC:',
  pattern2.findall(text),'\n',
  'Match isolated word with ABC:',
  pattern3.findall(text))
```

#### Repetition

When repetition is used, re will be **greedy**; it try to repeat as many times as possible. If **later portions of the pattern donâ€™t match**, the matching engine will then **back up and try again** with fewer repetitions.

```
?:    zero or 1 occurance
*:    zero or more occurance
+:    one  or more occurance
```

**`?` Zero or 1 Occurance**

```{python}
text = 'abcbcdd'
pattern = re.compile(r'a[bcd]?b')
pattern.findall(text)
```

**`+` At Least One Occurance**

```{python}
text = 'abcbcdd'
pattern = re.compile(r'a[bcd]+b')
pattern.findall(text)
```

**`*` Zero Or More Occurance Occurance**

```{python}
text = 'abcbcdd'
pattern = re.compile(r'a[bcd]*b')
pattern.findall(text)
```

#### Greedy vs Non-Greedy

- The `*`, `+`, and `?` qualifiers are all greedy; they match as much text as possible  
- If the `<.*>` is matched against `<a> b <c>`, it will match the entire string, and not just `<a>`  
- Adding **`?`** after the qualifier makes it perform the match in non-greedy; as few characters as possible will be matched. Using the RE <.*?> will match only '<a>'

```{python}
text = '<a> ali baba <c>'
greedy_pattern     = re.compile(r'<.*>')
non_greedy_pattern = re.compile(r'<.*?>')
print( 'Greedy:      ' ,        greedy_pattern.findall(text), '\n',
       'Non Greedy: ', non_greedy_pattern.findall(text) )
```

### Grouping

When `()` is used in the pattern, retrive the grouping components in MatchObject with `.groups()`. Result is in list. Example below extract hours, minutes and am/pm into a list.

#### Capturing Group

```{python}
text = 'Today at Wednesday, 10:50pm, we go for a walk'
pattern = re.compile(r'(\d\d):(\d\d)(am|pm)')
m = pattern.search(text)
print(
  'All Gropus: ', m.groups(), '\n',
  'Group 1: ', m.group(1), '\n',
  'Group 2: ', m.group(2), '\n',
  'Group 3: ', m.group(3) )
```

#### Non-Capturing Group

Having `(:? )` means don't capture this group

```{python}
text = 'Today at Wednesday, 10:50pm, we go for a walk'
pattern = re.compile(r'(:?\d\d):(?:\d\d)(am|pm)')
m = pattern.search(text)
print(
  'All Gropus: ', m.groups(), '\n',
  'Group 1: ', m.group(1), '\n',
  'Group 2: ', m.group(2) )
```

### Splittitng

Pattern is used to match **delimters**.

#### Use `re.split()`

```{python}
print( re.split('@',  "aa@bb @ cc "), '\n',
       re.split('\|', "aa|bb | cc "), '\n',
       re.split('\n', "sentence1\nsentence2\nsentence3") )
```

#### Use `re.compile().split()`

```{python}
pattern = re.compile(r"\|")
pattern.split("aa|bb | cc ")
```

### Substitution `re.sub()`


#### Found Match

Example below repalce anything within `{{.*}}`
```{python}
re.sub(r'({{.*}})', 'Durian', 'I like to eat {{Food}}.', flags=re.IGNORECASE)
```

Replace ` AND ` with ` & `. This does not require `()` grouping
```{python}
re.sub(r'\sAND\s', ' & ', 'Baked Beans And Spam', flags=re.IGNORECASE)
```

#### No Match

If not pattern not found, return the original text.
```{python}
re.sub(r'({{.*}})', 'Durian', 'I like to eat <Food>.', flags=re.IGNORECASE)
```
### Practical Examples

#### Extracting Float

```{python}
re_float = re.compile(r'\d+(\.\d+)?')
def extract_float(x):
    money = x.replace(',','')
    result = re_float.search(money)
    return float(result.group()) if result else float(0)

print( extract_float('123,456.78'), '\n',
       extract_float('rm 123.78 (30%)'), '\n',
       extract_float('rm 123,456.78 (30%)') )
```
## Word Tokenizer

### Custom Tokenizer

#### Split By Regex Pattern

Use **regex** to split words based on **specific punctuation as delimeter**.  
The rule is: split input text when any one or more continuous occurances of specified character.

```{python}
import re
pattern = re.compile(r"[-\s.,;!?]+")
pattern.split("hi @ali--baba, you are aweeeeeesome! isn't it. Believe it.:)")
```

#### Pick By Regex Pattern `nltk.tokenize.RegexpTokenizer`

Any sequence of chars fall within the bracket are considered tokens

```{python}
from nltk.tokenize import RegexpTokenizer
my_tokenizer = RegexpTokenizer(r'[a-zA-Z0-9\']+')
my_tokenizer.tokenize("hi @ali--baba, you are aweeeeeesome! isn't it. Believe it.:")
```

### `nltk.tokenize.word_tokenize()`

Words and punctuations are splitted into tokens

```{python}
from nltk.tokenize import word_tokenize
print( word_tokenize("hi @ali-baba, you are aweeeeeesome! isn't it. Believe it.:)") )
```

### `nltk.tokenize.casual.casual_tokenize()`

- Support emoji
- Support reduction of repetition chars
- Support removing userid (@someone)
- Good for social media text

```{python results='hold', collapse=FALSE}
from nltk.tokenize.casual     import casual_tokenize
## default
print( casual_tokenize("hi @ali-baba, you are aweeeeeesome! isn't it. Believe it. :)") )  
## shorten repeated chars
print( casual_tokenize("hi @ali-baba, you are aweeeeeesome! isn't it. Believe it.:)", 
          reduce_len=True))     
## shorten repeated chars, stirp usernames
print( casual_tokenize("hi @ali-baba, you are aweeeeeesome! isn't it. Believe it.:)", 
          reduce_len=True,      
          strip_handles=True))  
```


### `nltk.tokenize.treebank.TreebankWordTokenizer().tokenize()`

Treebank assume input text is **A sentence**, hence any period combined with word is treated as token.

```{python}
from nltk.tokenize.treebank   import TreebankWordTokenizer
TreebankWordTokenizer().tokenize("hi @ali-baba, you are aweeeeeesome! isn't it. Believe it.:)")
```

### Corpus Token Extractor

A corpus is a collection of documents (list of documents). 
A document is a text string containing one or many sentences.

```{python results='hide'}
from nltk.tokenize import word_tokenize
from nlpia.data.loaders import harry_docs as corpus
```

```{python}
## Tokenize each doc to list, then add to a bigger list
doc_tokens=[]
for doc in corpus:
  doc_tokens += [word_tokenize(doc.lower())]

print('Corpus (Contain 3 Documents):\n',corpus,'\n',
      '\nTokenized result for each document:','\n',doc_tokens)
```

Unpack list of token lists from above using sum. To get the **vocabulary** (unique tokens), **convert list to set**.

```{python}
## unpack list of list to list
vocab = sum(doc_tokens,[])
print('\nCorpus Vacabulary (Unique Tokens):\n',
       sorted(set(corpus_tokens)))
```

## Sentence Tokenizer

### `nltk.tokenize.sent_tokenize()`

```{python}
#nltk.download('punkt')
from nltk.tokenize import sent_tokenize
text="""\
Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.
The sky is pinkish-blue. You shouldn't eat cardboard
"""
tokenized_text=sent_tokenize(text)
for x in tokenized_text:
  print(x) 
```


## N-Gram

To create n-gram, first create 1-gram token

```{python}
from nltk.util import ngrams 
import re
sentence = "Thomas Jefferson began building the city, at the age of 25"
pattern = re.compile(r"[-\s.,;!?]+")
tokens = pattern.split(sentence)
print(tokens)
```

**ngrams()** is a generator, therefore, use **list()** to convert into full list

```{python}
ngrams(tokens,2)
```

Convert 1-gram to 2-Gram, wrap into list

```{python}
grammy = list( ngrams(tokens,2) )
print(grammy)
```

Combine each 2-gram into a string object

```{python}
[ " ".join(x) for x in grammy]
```

## Stopwords

### Custom Stop Words

Build the custom stop words dictionary.

```{python}
stop_words = ['a','an','the','on','of','off','this','is','at']
```

Tokenize text and remove stop words

```{python}
sentence = "The house is on fire"
tokens   = word_tokenize(sentence)
tokens_without_stopwords = [ x for x in tokens if x not in stop_words ]

print(' Original Tokens  : ', tokens, '\n',
      'Removed Stopwords: ',tokens_without_stopwords)
```

### NLTK Stop Words

Contain 179 words, in a list form

```{python}
import nltk
#nltk.download('stopwords')
nltk_stop_words = nltk.corpus.stopwords.words('english')
print('Total NLTK Stopwords: ', len(nltk_stop_words),'\n',
      nltk_stop_words)
```

### SKLearn Stop Words

Contain 318 stop words, in frozenset form

```{python}
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words
print(' Total Sklearn Stopwords: ', len(sklearn_stop_words),'\n\n',
       sklearn_stop_words)
```

### Combined NLTK and SKLearn Stop Words

```{python}
combined_stop_words = list( set(nltk_stop_words) | set(sklearn_stop_words) )
print('Total combined NLTK and SKLearn Stopwords:', len( combined_stop_words ),'\n'
      'Stopwords shared among NLTK and SKlearn  :', len( list( set(nltk_stop_words) & set(sklearn_stop_words)) ))
```

## Normalizing

Similar things are combined into single normalized form. This will reduced the vocabulary.

### Case Folding

If tokens aren't cap normalized, you will end up with large word list.
However, some information is often communicated by capitalization of word, such as name of places. If names are important, consider using proper noun.


```{python}
tokens = ['House','Visitor','Center']
[ x.lower() for x in tokens]
```


### Stemming

- Output of a stemmer is **not necessary a proper word**
- Automatically convert words to **lower cap**
- **Porter stemmer** is a lifetime refinement with 300 lines of python code  
- Stemming is faster then Lemmatization

```{python}
from nltk.stem.porter import PorterStemmer
stemmer = PorterStemmer()
tokens = ('house','Housing','hOuses', 'Malicious','goodness')
[stemmer.stem(x) for x in tokens ]
```

### Lemmatization

NLTK uses connections within **princeton WordNet** graph for word meanings.

```{python}
#nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

print( lemmatizer.lemmatize("better", pos ='a'), '\n',
       lemmatizer.lemmatize("better", pos ='n') )
```

```{python}
print( lemmatizer.lemmatize("good", pos ='a'), '\n',
       lemmatizer.lemmatize("good", pos ='n') )
```

### Comparing Stemming and Lemmatization

- Lemmatization is slower than stemming
= Lemmatization is better at retaining meanings
- Lemmatization produce valid english word
- Stemming not necessary produce valid english word
- Both reduce vocabulary size, but increase ambiguity
- For search engine application, stemming and lemmatization will improve recall as it associate more documents with the same query words, however with the cost of reducing precision and accuracy.

For search-based chatbot where accuracy is more important, it should first search with unnormalzied words.

## POS Tagging

The primary target of Part-of-Speech(POS) tagging is to identify the grammatical group of a given word. Whether it is a NOUN, PRONOUN, ADJECTIVE, VERB, ADVERBS, etc. based on the context. 

Tagging works sentence by sentence. 
- Document fist must be splitted into sentences
- Each sentence need to be tokenized into words

```{python}
#nltk.download('averaged_perceptron_tagger')
sentence = "Albert Einstein was born in Ulm, Germany in 1879."
tokens = nltk.word_tokenize(sentence)
tags   = nltk.pos_tag(tokens)
print('Tokens:\n', tokens, '\n\n', 'Tags:\n', tags)
```

## Sentiment

### Vader

- It is a rule based sentiment analyzer, contain 7503 lexicons
- It is good for social media because lexicon contain emoji and short form text
- Contain only 3 n-gram

#### Vader Lexicon

The lexicon is a dictionary. To make it iterable, need to convert into list:  
- Step 1: Convert `dict` to `dict_items`, which is a list containing items, each item is one dict  
- Step 2: Unpack `dict_items` to `list`

```{python}
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
vader_lex = SentimentIntensityAnalyzer().lexicon  # get the lexicon dictionary
vader_list = list(vader_lex.items())              # convert to items then list
print( 'Total Vader Lexicon:', len(vader_lex),'\n',
        vader_list[1:10], vader_list[220:240] )
```

**There is only four N-Gram in the lexicon**

```{python}
print('List of N-grams: ')
[ (tok,score) for tok, score in vader_list if " " in tok]
```

If stemming or lemmatization is used, stem/lemmatize the vader lexicon too

```{python}
[ (tok,score) for tok, score in vader_list if "lov" in tok]
```

#### Polarity Scoring

Scoring result is a dictionary of:

- neg
- neu
- pos
- compound
**neg, neu, pos adds up to 1.0**

Example below shows polarity for two sentences:

```{python}
corpus = ["Python is a very useful but hell difficult to learn",
        ":) :) :("]
for doc in corpus:
  print(doc, '-->', "\n:", SentimentIntensityAnalyzer().polarity_scores(doc) )
```

### SentiWordNet



## Feature Representation

### The Data

A corpus is a collection of multiple documents. In the below example, each document is represented by a sentence.

```{python}
corpus = [
   'This is the first document, :)',
   'This document is the second document.',
   'And this is a third one',
   'Is this the first document?',
]
```

### Frequency Count

Using purely frequency count as a feature will obviously bias on long document (which contain a lot of words, hence words within the document will have very high frequency).

#### + Tokenizer

**Default Tokenizer**  
By default, vectorizer apply tokenizer to select minimum **2-chars alphanumeric words**.  Below **train** the vectorizer using **`fit_transform()`**.

```{python}
from sklearn.feature_extraction.text import CountVectorizer
vec = CountVectorizer()          # initialize the vectorizer
X   = vec.fit_transform(corpus)  # FIT the vectorizer, return fitted data
print(pd.DataFrame(X.toarray(), columns=vec.get_feature_names()),'\n\n',
      'Vocabulary: ', vec.vocabulary_)
```

**Custom Tokenizer**  
You can use a custom tokenizer, which is a **function that return list of words**. Example below uses nltk RegexpTokenizer function, which retains one or more alphanumeric characters.

```{python}
my_tokenizer = RegexpTokenizer(r'[a-zA-Z0-9\']+')  ## Custom Tokenizer
vec2 = CountVectorizer(tokenizer=my_tokenizer.tokenize) ## custom tokenizer's function
X2   = vec2.fit_transform(corpus)  # FIT the vectorizer, return fitted data
print(pd.DataFrame(X2.toarray(), columns=vec2.get_feature_names()),'\n\n',
      'Vocabulary: ', vec.vocabulary_)
```

**1 and 2-Word-Gram Tokenizer**  
Use `ngram_range()` to specify range of grams needed.

```{python}
vec3 = CountVectorizer(ngram_range=(1,2))          # initialize the vectorizer
X3   = vec3.fit_transform(corpus)     # FIT the vectorizer, return fitted data
print(pd.DataFrame(X3.toarray(), columns=vec3.get_feature_names()),'\n\n',
      'Vocabulary: ', vec.vocabulary_)
```

**Apply Trained Vectorizer**
Once the vectorizer had been trained, you can apply them on new corpus. **Tokens not in the vectorizer vocubulary are ignored**.

```{python}
new_corpus = ["My Name is Charlie Angel", "I love to watch Star Wars"]
XX = vec.transform(new_corpus)
pd.DataFrame(XX.toarray(), columns=vec.get_feature_names())
```

#### + Stop Words

Vectorizer can optionally be use with stop words list. Use `stop_words=english` to apply filtering using sklearn built-in stop word.  You can replace `english` with other word **list object**.

```{python}
vec4 = CountVectorizer(stop_words='english') ## sklearn stopwords list
X4 = vec4.fit_transform(corpus)
pd.DataFrame(X4.toarray(), columns=vec4.get_feature_names())
```

### TFIDF

#### Equation

$$tf(t,d) = \text{occurances of term t in document t} \\
n     = \text{number of documents} \\
df(t) = \text{number of documents containing term t} \\
idf(t)  = log \frac{n}{df(t))} + 1 \\
idf(t)  = log \frac{1+n}{1+df(t))} + 1 \text{.... smoothing, prevent zero division} \\
tfidf(t) = tf(t) * idf(t,d)    \text{.... raw, no normalization on tf(t)} \\
tfidf(t) = \frac{tf(t,d)}{||V||_2} * idf(t)    \text{.... tf normalized with euclidean norm}$$

#### `TfidfTransformer`

To generate TFIDF vectors, first run `CountVectorizer` to get frequency vector matrix. Then take the output into this transformer.

```{python}
from sklearn.feature_extraction.text import TfidfTransformer

corpus = [
    "apple apple apple apple apple banana",
    "apple apple",
    "apple apple apple banana",
    "durian durian durian"]
    
count_vec = CountVectorizer()
X = count_vec.fit_transform(corpus)

transformer1 = TfidfTransformer(smooth_idf=False,norm=None)
transformer2 = TfidfTransformer(smooth_idf=False,norm='l2')
transformer3 = TfidfTransformer(smooth_idf=True,norm='l2')

tfidf1 = transformer1.fit_transform(X)
tfidf2 = transformer2.fit_transform(X)
tfidf3 = transformer3.fit_transform(X)

print(
  'Frequency Count: \n', pd.DataFrame(X.toarray(), columns=count_vec.get_feature_names()),
  '\n\nVocabulary: ', count_vec.vocabulary_,
  '\n\nTFIDF Without Norm:\n',tfidf1.toarray(), 
  '\n\nTFIDF with L2 Norm:\n',tfidf2.toarray(),  
  '\n\nTFIDF with L2 Norm (smooth):\n',tfidf3.toarray())
```

#### `TfidfVectorizer`

This vectorizer gives end to end processing from corpus into TFIDF vector matrix, including tokenization, stopwords.

```{python}
from sklearn.feature_extraction.text import TfidfVectorizer
my_tokenizer = RegexpTokenizer(r'[a-zA-Z0-9\']+')  ## Custom Tokenizer

vec1 = TfidfVectorizer(tokenizer=my_tokenizer.tokenize,  stop_words='english') #default smooth_idf=True, norm='l2'
vec2 = TfidfVectorizer(tokenizer=my_tokenizer.tokenize, stop_words='english',smooth_idf=False)
vec3 = TfidfVectorizer(tokenizer=my_tokenizer.tokenize, stop_words='english', norm=None)

X1   = vec1.fit_transform(corpus)  # FIT the vectorizer, return fitted data
X2   = vec2.fit_transform(corpus)  # FIT the vectorizer, return fitted data
X3   = vec3.fit_transform(corpus)  # FIT the vectorizer, return fitted data

print(
  'TFIDF Features (Default with Smooth and L2 Norm):\n',
  pd.DataFrame(X1.toarray().round(3), columns=vec1.get_feature_names()),
  '\n\nTFIDF Features (without Smoothing):\n',
  pd.DataFrame(X2.toarray().round(3), columns=vec2.get_feature_names()),
  '\n\nTFIDF Features (without L2 Norm):\n',
  pd.DataFrame(X3.toarray().round(3), columns=vec3.get_feature_names())
  )
```

## Appliction

### Document Similarity

Document1 and Document 2 are mutiplicate of Document0, therefore their consine similarity is the same.

```{python}
documents = (
    "apple apple banana",
    "apple apple banana apple apple banana",
    "apple apple banana apple apple banana apple apple banana")
    
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vec = TfidfVectorizer()
tfidf_matrix = tfidf_vec.fit_transform(documents)

from sklearn.metrics.pairwise import cosine_similarity
print('Cosine Similarity betwen doc0 and doc1:\n',cosine_similarity(tfidf_matrix[0], tfidf_matrix[1]))
print('Cosine Similarity betwen doc1 and doc2:\n',cosine_similarity(tfidf_matrix[1], tfidf_matrix[2]))
print('Cosine Similarity betwen doc1 and doc2:\n',cosine_similarity(tfidf_matrix[0], tfidf_matrix[2]))
```


## Naive Bayes

### Libraries

```{python results='hide'}
from nlpia.data.loaders import get_data
from nltk.tokenize.casual     import casual_tokenize
from collections import Counter
```

### The Data

```{python}
movies = get_data('hutto_movies')   # download data
print(movies.head(), '\n\n',
      movies.describe())
```

### Bag of Words

- Tokenize each record, remove single character token, then convert into list of counters (words-frequency pair).  
- Each item in the list is a counter, which represent word frequency within the record

```{python}
bag_of_words = []
for text in movies.text:
    tokens = casual_tokenize(text, reduce_len=True, strip_handles=True)  # tokenize
    tokens = [x for x in tokens if len(x)>1]                  ## remove single char token
    bag_of_words.append( Counter(tokens, strip_handles=True)  ## add to our BoW
    )

unique_words =  list( set([ y  for x in bag_of_words  for y in x.keys()]) )

print("Total Rows: ", len(bag_of_words),'\n\n',
      'Row 1 BoW: ',bag_of_words[:1],'\n\n',    # see the first two records
      'Row 2 BoW: ', bag_of_words[:2], '\n\n',
      'Total Unique Words: ', len(unique_words))
```

**Convert NaN into 0 then all features into integer**

```{python}
bows_df = pd.DataFrame.from_records(bag_of_words)
bows_df = bows_df.fillna(0).astype(int)  # replace NaN with 0, change to integer
bows_df.head()
```

### Build The Model

```{python}
from sklearn.naive_bayes import MultinomialNB
train_y  = movies.sentiment>0   # label
train_X  = bows_df              # features
nb_model = MultinomialNB().fit( train_X, train_y)
```

### Train Set Prediction

First, make a prediction on training data, then compare to ground truth. 

```{python}
train_predicted = nb_model.predict(bows_df)
print("Accuracy: ", np.mean(train_predicted==train_y).round(4))
```

